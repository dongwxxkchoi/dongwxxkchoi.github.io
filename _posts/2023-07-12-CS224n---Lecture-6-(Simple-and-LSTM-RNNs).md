---
layout: single
date: 2023-07-12
title: "CS224n - Lecture 6 (Simple and LSTM RNNs)"
use_math: true
author_profile: false
tags: [ê°•ì˜/ì±… ì •ë¦¬, ]
categories: [AI, ]
---


## RNN Language Model


![0](/assets/img/2023-07-12-CS224n---Lecture-6-(Simple-and-LSTM-RNNs).md/0.png)_ëŒ€ëµì ì¸ RNN ëª¨ë¸_


RNNì„ ë¶€ë¶„ë³„ë¡œ ë‚˜ëˆ ì„œ ë³´ìë©´, í¬ê²Œ

- **Input**
- **Embedding**
- **Hidden state**
- **Output**

ë¡œ ë³¼ ìˆ˜ ìˆìŒ


**<Input>**


![1](/assets/img/2023-07-12-CS224n---Lecture-6-(Simple-and-LSTM-RNNs).md/1.png)

- $x^{(t)} \in \mathbb{R}^{|V|}$  **(V: Vocab, t: Timestep)**
- **word vectors** $x_t$ëŠ” ê°ê° **Tê°œì˜ wordsì— ëŒ€ì‘**
- ê° $x^{(t)}$ì€ **ê° time stepì´ t**ì¼ ë•Œ **inputë˜ëŠ” word**ë¥¼ ì˜ë¯¸
- $x^{(t)}$ëŠ” **ëª¨ë¸ì´ ì•Œê³  ìˆëŠ” Vocabulary**ì— ì†í•¨

**<Embedding>**


$e^{(t)} = Ex^{(t)}$


inputìœ¼ë¡œ ë°›ì€ **xë¥¼ embedded vectorë¡œ embedding**í•´ì£¼ëŠ” ìˆ˜ì‹

- $E$: **Embedding Matrix**, $e^{(t)} : x^{(t)}$ì˜ **embedded vector**

<**hidden state>**


$h^{(t)} = \sigma(W_hh^{(t-1)}+W_ee^{(t)} + b_1)$


time-step **tì— hidden layer output featuresë¥¼ ê³„ì‚°**í•˜ëŠ” ìˆ˜ì‹

- $\sigma$: **non-linearity function** (ex. Sigmoid function)
- $W_h \in \mathbb{R}^{D_h\times D_h}$ : t-1 time-stepì˜ hidden stateì— ê³±í•´ì§€ëŠ” weight matrix
- $h^{(t-1)}$ : t-1 time-stepì˜ hidden state vector
- $W_e \in \mathbb{R}^{D_h\times d}$  : input embedded word vector $e^{(t)}$ì— ê³±í•´ì§€ëŠ” weight matrix
- $e^{(t)} \in \mathbb{R}^d$ (embedded vectorëŠ” dx1 vector)
- $b_1$ : hidden layerì˜ bias

<**Output>**


$\hat y^{(t)} = \textrm{softmax}(Uh^{(t)} + b_2) \in \mathbb{R}^{|V|}$


ì´ ëª¨ë¸ì˜ ê²½ìš°ëŠ”, **ì•ì˜ tê°œì˜ wordsë¥¼ í†µí•´, ì•ìœ¼ë¡œ ë‚˜ì˜¬ wordë¥¼ ì˜ˆì¸¡** 


ê·¸ **word**ëŠ” **Vì— ì†í•´ ìˆëŠ” word ì¤‘ í•˜ë‚˜**ì´ê³ , **softmaxë¥¼ í†µí•´ í™•ë¥  ë¶„í¬ ë°˜í™˜**

- $\hat y^{(t)}$ : ë‹¤ìŒ predicted word ($x^{(t)}, h_{t-1}$ë¥¼ í†µí•´ ì˜ˆì¸¡)
- $softmax$ : softmax function
- $U \in \mathbb{R}^{|V|\times D_h}$ :  hidden state vector $h^{(t)}$ë¥¼ |V|ë¡œ mapping í•´ì£¼ëŠ” weight vector (learned)
- $b_2$ : output layerì˜ bias

![2](/assets/img/2023-07-12-CS224n---Lecture-6-(Simple-and-LSTM-RNNs).md/2.png)



## Loss Function


**Loss Function**ìœ¼ë¡œëŠ” **Cross Entropy Loss**ë¥¼ ì¢…ì¢… ì‚¬ìš©í•¨


![3](/assets/img/2023-07-12-CS224n---Lecture-6-(Simple-and-LSTM-RNNs).md/3.png)

<details>
  <summary>cross entropy loss</summary>


[bookmark](https://velog.io/@rcchun/ë¨¸ì‹ ëŸ¬ë‹-í¬ë¡œìŠ¤-ì—”íŠ¸ë¡œí”¼cross-entropy)



  </details>
![4](/assets/img/2023-07-12-CS224n---Lecture-6-(Simple-and-LSTM-RNNs).md/4.png)


ì‹ì„ ì˜ ì‚´í´ë³´ë©´, **t ì‹œì ì˜ Loss Function**ì¸ë°, ê·¸ ê²°ê³¼ê°€ $x_{t+1}$**ê³¼ ê´€ë ¨**ì´ ìˆëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆìŒ


**why?**


cross entropy lossëŠ” í™•ë¥ ì´ ê°€ì¥ ë†’ì€ ê²ƒì„ 1ë¡œ ë§Œë“¤ê³  ë‚˜ë¨¸ì§€ëŠ” ëª¨ë‘ 0ìœ¼ë¡œ ë§Œë“¤ì–´ë²„ë¦¬ê¸° ë•Œë¬¸ì—,


í™•ë¥ ì´ ë†’ë‹¤ê³  íŒë‹¨ë˜ëŠ” $\hat y^{(t)}_{x_{t+1}}$ê³¼ ë™ì¼í•œ ê²ƒ


![5](/assets/img/2023-07-12-CS224n---Lecture-6-(Simple-and-LSTM-RNNs).md/5.png)_cross entropy error over a corpus of size T / eq(8)_


![6](/assets/img/2023-07-12-CS224n---Lecture-6-(Simple-and-LSTM-RNNs).md/6.png)



## Back Propagation for RNN 


![7](/assets/img/2023-07-12-CS224n---Lecture-6-(Simple-and-LSTM-RNNs).md/7.png)


![8](/assets/img/2023-07-12-CS224n---Lecture-6-(Simple-and-LSTM-RNNs).md/8.png)


![9](/assets/img/2023-07-12-CS224n---Lecture-6-(Simple-and-LSTM-RNNs).md/9.png)


![10](/assets/img/2023-07-12-CS224n---Lecture-6-(Simple-and-LSTM-RNNs).md/10.png)



## Perplexity

<details>
  <summary>**what is perplexity?**</summary>


[bookmark](https://wikidocs.net/21697)



  </details>
**perplexed : í—·ê°ˆë¦¬ëŠ”**


â†’ **Perplexity** : â€œ**í—·ê°ˆë¦¬ëŠ” ì •ë„**â€ ë¼ê³  ì´í•´í•˜ë©´ í¸í•¨


ê·¸ë ‡ê¸° ë•Œë¬¸ì—, Perplexity ì¦‰, **PPLì´ ë†’ì„ìˆ˜ë¡ ì–¸ì–´ ëª¨ë¸ì˜ ì„±ëŠ¥ì€ ë‚®ë‹¤.**


**(PPL ë‚®ì„ìˆ˜ë¡ ì–¸ì–´ ëª¨ë¸ ì„±ëŠ¥ ì¢‹ìŒ)**

<details>
  <summary>**In** **CS224n**</summary>


**standard í•œ evaluation metric**


$Perplexity = 2^J$ is called the perplexity relationship; it is basically 2 to the power of the negative log probability of the cross entropy error function shown in Equation 8. 


Perplexity is a measure of confusion where **lower values imply more confidence in predicting the next word in the sequence** (compared to the ground truth outcome).


![11](/assets/img/2023-07-12-CS224n---Lecture-6-(Simple-and-LSTM-RNNs).md/11.png)



  </details>
![12](/assets/img/2023-07-12-CS224n---Lecture-6-(Simple-and-LSTM-RNNs).md/12.png)

- **W: sentence**
- **len(W): N**

ì´ ë¬¸ì¥ì˜ í™•ë¥ ì— chain rule ì ìš©ì‹œ ë‹¤ìŒê³¼ ê°™ìŒ


![13](/assets/img/2023-07-12-CS224n---Lecture-6-(Simple-and-LSTM-RNNs).md/13.png)_n-gramì˜ ì˜ˆì‹œ_


![14](/assets/img/2023-07-12-CS224n---Lecture-6-(Simple-and-LSTM-RNNs).md/14.png)_bigramì˜ ì˜ˆ_

<details>
  <summary>**PPLì˜ ë¶„ê¸° ê³„ìˆ˜(Branching factor)ë¡œì¨ì˜ ì˜ë¯¸**</summary>


**PPL**ì€ **ì„ íƒí•  ìˆ˜ ìˆëŠ” ê°€ëŠ¥í•œ ê²½ìš°ì˜ ìˆ˜ë¥¼ ì˜ë¯¸**í•˜ëŠ” ë¶„ê¸°ê³„ìˆ˜


ì´ ì–¸ì–´ ëª¨ë¸ì´ **íŠ¹ì • ì‹œì ì—ì„œ í‰ê· ì ìœ¼ë¡œ ëª‡ ê°œì˜ ì„ íƒì§€ë¥¼ ê°€ì§€ê³  ê³ ë¯¼í•˜ê³  ìˆëŠ”ì§€**ë¥¼ ì˜ë¯¸


ì–¸ì–´ ëª¨ë¸ì— ì–´ë–¤ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì£¼ê³  ì¸¡ì •í–ˆë”ë‹ˆ PPLì´ 10ì´ ë‚˜ì™”ë‹¤


í•´ë‹¹ ì–¸ì–´ ëª¨ë¸ì€ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ëŒ€í•´ì„œ **ë‹¤ìŒ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ëŠ” time stepë§ˆë‹¤ í‰ê·  10ê°œì˜ ë‹¨ì–´ë¥¼ ê°€ì§€ê³  ì–´ë–¤ ê²ƒì´ ì •ë‹µì¸ì§€ ê³ ë¯¼**í•œë‹¤ëŠ” ëœ»


![15](/assets/img/2023-07-12-CS224n---Lecture-6-(Simple-and-LSTM-RNNs).md/15.png)


ê·¸ë ‡ê¸° ë•Œë¬¸ì—, PPLì´ ë‚®ì„ìˆ˜ë¡ **í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì—ì„œ ë†’ì€ ì •í™•ë„** (ì‹¤ì œì—” ì ìš© ì „)


ì‚¬ëŒì´ ëŠë¼ê¸°ì— ì¢‹ì€ ì–¸ì–´ ëª¨ë¸ì€ ì•„ë‹ˆë¼ëŠ” ì 


ì–¸ì–´ ëª¨ë¸ì˜ PPLì€ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ì˜ì¡´í•˜ë¯€ë¡œ ë‘ ê°œ ì´ìƒì˜ ì–¸ì–´ ëª¨ë¸ì„ ë¹„êµí•  ë•ŒëŠ” ì •ëŸ‰ì ìœ¼ë¡œ ì–‘ì´ ë§ê³ , ë˜í•œ ë„ë©”ì¸ì— ì•Œë§ì€ ë™ì¼í•œ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì‚¬ìš©í•´ì•¼ ì‹ ë¢°ë„ê°€ ë†’ë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤.



  </details>
![16](/assets/img/2023-07-12-CS224n---Lecture-6-(Simple-and-LSTM-RNNs).md/16.png)


**n-gram ëª¨ë¸ì— ë¹„í•´, RNN, LSTMìœ¼ë¡œ ë‚´ë ¤ê°ˆìˆ˜ë¡ ì„±ëŠ¥ ìƒìŠ¹ í™•ì¸**



## Advantages, Disadvantages and Applications of RNNs 



### Advantages & Disadvantages



#### **advantages**

1. They can process **input sequences of any length**
2. The **model size does not increase** for longer input sequence lengths
3. Computation for step t can (in theory) use information from many steps back.
4. The **same weights are applied** to every timestep of the input, so there is **symmetry** in how inputs are processed


#### **disadvantages**

1. **Computation is slow** - because it is **sequential**, it cannot be parallelized
2. In practice, it is difficult to access information from many steps back due to problems like **vanishing and exploding gradients**, which we discuss in the following subsection


### Vanishing Gradients and Exploding Gradients


**Example**


<Sentence 1>
"Jane walked into the room. John walked in too. Jane said hi to ___"
<Sentence 2>
"Jane walked into the room. John walked in too. It was late in the day, and everyone was walking home after a long day at work. Jane said hi to ___"


ì´ ë‘ ê²½ìš°ì—, **RNN ëª¨ë¸ì€ 2ë³´ë‹¤ëŠ” 1ì„ ë” ì˜ ì˜ˆì¸¡í•¨**


â†’ Vanishing Gradient

<details>
  <summary>**Vanishing Gradient**</summary>


[image](https://youtube.com/clip/UgkxZrFO6O6HKEbn_7Q-__yz8i2Ybhkutox1)



  </details><details>
  <summary>**Exploding Gradient**</summary>


[image](https://youtube.com/clip/UgkxXC7Ywe0jbrT0VTMhRT1aKylR7sPTE2Nb)



  </details>
**Solution**

- **fix vanishing gradient**

	**â†’ LSTM (seperate memory)**

- **fix exploding gradient**

	â†’ **Gradient clipping**


	**norm of the gradient is greater than some threshold, scale it down before applying SGD update**



### **Applications**



#### 1) sequence tagging


![17](/assets/img/2023-07-12-CS224n---Lecture-6-(Simple-and-LSTM-RNNs).md/17.png)


**ê° wordì— ëŒ€í•´ í’ˆì‚¬ íƒœê¹…**



#### 2) sentence classification


![18](/assets/img/2023-07-12-CS224n---Lecture-6-(Simple-and-LSTM-RNNs).md/18.png)_general_


![19](/assets/img/2023-07-12-CS224n---Lecture-6-(Simple-and-LSTM-RNNs).md/19.png)_general_


![20](/assets/img/2023-07-12-CS224n---Lecture-6-(Simple-and-LSTM-RNNs).md/20.png)_for sentence classification_



#### 3) language encoder module


![21](/assets/img/2023-07-12-CS224n---Lecture-6-(Simple-and-LSTM-RNNs).md/21.png)


**question answering, machine translation â€¦**


sequenceë¥¼ ë°›ì•„ í•œ ouptutì„ ë‚´ë†“ìœ¼ë‹ˆ, encoder?



#### 4) generate text (Decoding)


![22](/assets/img/2023-07-12-CS224n---Lecture-6-(Simple-and-LSTM-RNNs).md/22.png)


raw dataë¥¼ ë°›ì•„ ê¸°ê³„ì–´ë¥¼ ë‚´ë†“ìœ¼ë‹ˆ decoder?



#### 5) Generating text with a RNN Language Model


![23](/assets/img/2023-07-12-CS224n---Lecture-6-(Simple-and-LSTM-RNNs).md/23.png)


repeated sampling â†’ **outputì„ ì˜ˆì¸¡ sampleë¡œ ì‚¬ìš©**



#### 6) Translation


![24](/assets/img/2023-07-12-CS224n---Lecture-6-(Simple-and-LSTM-RNNs).md/24.png)

<details>
  <summary>**encoder & decoder**</summary>


![25](/assets/img/2023-07-12-CS224n---Lecture-6-(Simple-and-LSTM-RNNs).md/25.png)



  </details>

## Bidirectional RNNs



#### ë“±ì¥ ë°°ê²½


ì§€ê¸ˆê¹Œì§€ì˜ sequence data í•™ìŠµì—ì„œì˜ ëª©í‘œ


â†’ **ì§€ê¸ˆê¹Œì§€ ì£¼ì–´ì§„ ê²ƒì„ ë³´ê³  ë‹¤ìŒì„ ì˜ˆì¸¡**


í•˜ì§€ë§Œ, ë‹¤ìŒì˜ ê²½ìš°ëŠ”?


![26](/assets/img/2023-07-12-CS224n---Lecture-6-(Simple-and-LSTM-RNNs).md/26.png)


i)  happy, sad, angry ë“±ë“±


ii)  not, very ë“±ë“±


iii)  very ë“±


í•˜ì§€ë§Œ, ê¸°ì¡´ì˜ sequence ëª¨ë¸ì—ì„œëŠ” ë˜‘ê°™ì´ â€œ**I amâ€** ë§Œ í™•ì¸í•œë‹¤.


â†’ ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ì„œ **Bidirectional RNN ëª¨ë¸**ì´ ì œì•ˆë¨



#### êµ¬ì¡°


ìœ„ì™€ ê°™ì€ ë¬¸ì œì ì„ í•´ê²°í•˜ë ¤ë©´, forwardê°€ ì•„ë‹Œ backward ë°©ì‹ì´ í•„ìš”í•˜ë‹¤. ì¦‰, ë’¤ì—ì„œë¶€í„° sequenceë¥¼ ì½ëŠ” ì‘ì—…ì´ í•„ìš”í•œ ê²ƒ. 


â‡’ forward, backwardë¥¼ ìœ„í•œ RNN ëª¨ë¸ì´ ë™ì‹œì— ì¡´ì¬


![27](/assets/img/2023-07-12-CS224n---Lecture-6-(Simple-and-LSTM-RNNs).md/27.png)_Sentiment Classification_


ìœ„ì™€ ê°™ì€ ìƒí™©ì—ì„œ, **ë„¤ëª¨ì¹œ ë¶€ë¶„ì˜ hidden state(**$h$**)**ëŠ” **â€œthe movie was terriblyâ€**ê¹Œì§€ì— ëŒ€í•œ ì •ë³´ë§Œ í¬í•¨í•˜ê¸° ë•Œë¬¸ì—, hëŠ” negative ì •ë³´ë§Œ ë‹´ê²Œ ëœë‹¤.


í•˜ì§€ë§Œ, ì‹¤ì œë¡œëŠ” ë’¤ì— excitingì´ ë¶™ê¸° ë•Œë¬¸ì—, sentence ì „ì²´ì˜ ì˜ë¯¸ëŠ” positiveì—¬ì•¼ í•¨


![28](/assets/img/2023-07-12-CS224n---Lecture-6-(Simple-and-LSTM-RNNs).md/28.png)


ë”°ë¼ì„œ Bidirectional RNNì„ ì„¤ê³„í•´ì„œ, â€œthe movie was terribly exciting !â€ì„ ê°ì •ë¶„ì„í•  ë•Œ, ì™¼ìª½ì—ì„œë¶€í„°, ê·¸ë¦¬ê³  ì˜¤ë¥¸ìª½ì—ì„œë¶€í„° ëª¨ë‘ íƒìƒ‰í•´ì„œ, í•´ë‹¹ ë„¤ëª¨ì¹œ ë¶€ë¶„ì˜ **concatenated hidden states(**$h^\prime$**)**ì€ ë‘ ë¬¸ë§¥ìƒ ì˜ë¯¸ë¥¼ ëª¨ë‘ ë‹´ê²Œ ëœë‹¤.


ê°„ë‹¨í•˜ê²Œ ë„ì‹í™”í•˜ë©´ ì´ë ‡ê²Œ í‘œí˜„í•  ìˆ˜ ìˆë‹¤.


![29](/assets/img/2023-07-12-CS224n---Lecture-6-(Simple-and-LSTM-RNNs).md/29.png)


ìˆ˜ì‹ìœ¼ë¡œ í‘œí˜„í•˜ë©´ ì•„ë˜ì™€ ê°™ì´ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤.


![30](/assets/img/2023-07-12-CS224n---Lecture-6-(Simple-and-LSTM-RNNs).md/30.png)_ppt ì† ìˆ˜ì‹_


![31](/assets/img/2023-07-12-CS224n---Lecture-6-(Simple-and-LSTM-RNNs).md/31.png)_note ì† ìˆ˜ì‹ (more detailed)_



### Deep Bidirectional RNNs


![32](/assets/img/2023-07-12-CS224n---Lecture-6-(Simple-and-LSTM-RNNs).md/32.png)


![33](/assets/img/2023-07-12-CS224n---Lecture-6-(Simple-and-LSTM-RNNs).md/33.png)


![34](/assets/img/2023-07-12-CS224n---Lecture-6-(Simple-and-LSTM-RNNs).md/34.png)


ì´ë ‡ê²Œ ì—¬ëŸ¬ ê°œì˜ ì¸µì„ ìŒ“ìŒìœ¼ë¡œ, deep bidirectional RNNsì€ ë” ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ì§€ë§Œ, computational resourcesë¥¼ ë” ë§ì´ ìš”êµ¬í•˜ê³ , trainì´ ì–´ë µë‹¤ëŠ” ë‹¨ì ë„ ì¡´ì¬


(í™œìš©ë¶„ì•¼: nlp, speech recognition, computer vision, audio processingâ€¦)



## Gated Recurrent Units (GRU)


RNNsì€ **ë” ë³µì¡í•œ activation function**ì„ ì‚¬ìš©í–ˆì„ ë–„ ë” ì¢‹ì€ ì„±ëŠ¥ ë³´ì„


affine transformation(ê¸°í•˜í•™ì  ì„±ì§ˆ ë³´ì¡´ ë³€í™˜ë²•)ê³¼ point-wise nonlinearityë¥¼ ì‚¬ìš©í•œ hidden state ht-1ì—ì„œ htë¡œì˜ transitionì„ ê³ ì•ˆ


RNNì´ ë” long-term dependencyë¥¼ ê°€ì§ˆ ìˆ˜ ìˆë„ë¡ persistentí•œ memoryë¥¼ ê°–ì„ ìˆ˜ ìˆê²Œ í•¨


GRUê°€ ht-1ê³¼ xtì„ ì–´ë–»ê²Œ ì‚¬ìš©í•´ ë‹¤ìŒ hidden state htë¥¼ ë§Œë“œëŠ”ì§€ ì„¤ëª…


![35](/assets/img/2023-07-12-CS224n---Lecture-6-(Simple-and-LSTM-RNNs).md/35.png)


![36](/assets/img/2023-07-12-CS224n---Lecture-6-(Simple-and-LSTM-RNNs).md/36.png)







#### Reset Gate


![37](/assets/img/2023-07-12-CS224n---Lecture-6-(Simple-and-LSTM-RNNs).md/37.png)

- $r^{(t)}$ : **reset signal (ë¦¬ì…‹ ì‹ í˜¸)**

	**ì´ì „ time stepì˜ hidden state** $h^{(t-1)}$ ì´ **summarization (new memory)**ì¸ $\tilde h_t$**ì— ì–¼ë§ˆë‚˜ ì¤‘ìš”í•œì§€**ë¥¼ ê²°ì •í•©ë‹ˆë‹¤. reset gateì—ì„œ $h^{(t-1)}$ ì´ **ìƒˆë¡œìš´ ë©”ëª¨ë¦¬ ìƒì„±ì— ê´€ë ¨ì´ ì—†ë‹¤** íŒë‹¨í•˜ë©´**,** $h^{(t-1)}$**ì„ ë¬´ì‹œí•˜ë„ë¡ ì‹ í˜¸ë¥¼ ìƒì„±**í•©ë‹ˆë‹¤. 


	(0~1 ì‚¬ì´ì˜ ê°’, 0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ê´€ë ¨ X, 1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ê´€ë ¨ O)

- $U^{(r)}$ : $h^{(t-1)}$ ë¥¼ **ì²˜ë¦¬**í•˜ëŠ” **ê°€ì¤‘ì¹˜ í–‰ë ¬** (í•™ìŠµ O)
- $W^{(r)}$ : $x^{(t)}$ë¥¼ **ì²˜ë¦¬** í•˜ëŠ” **ê°€ì¤‘ì¹˜ í–‰ë ¬** (í•™ìŠµ O)


#### New memory generation


![38](/assets/img/2023-07-12-CS224n---Lecture-6-(Simple-and-LSTM-RNNs).md/38.png)

- $x^{(t)}$ : **ìƒˆë¡œìš´ input words**
- $h^{(t-1)}, h^{(t)}$ : **t-1, t timestepì˜ hidden state**
- $U$ : $h^{(t-1)}$ ë¥¼ **ì²˜ë¦¬**í•˜ëŠ” **ê°€ì¤‘ì¹˜ í–‰ë ¬** (í•™ìŠµ O) (resetì˜ Uì™€ ë‹¤ë¦„)
- $\tilde h^{(t)}$ **: ìƒˆë¡œìš´ memory**
- tanh : inputì„ **[-1,1] ì‚¬ì´ì˜ ê°’ìœ¼ë¡œ ë°˜í™˜**í•´ $\tilde h^{(t)}$ë¥¼ ìƒì„±

	(í‰ê· ì„ 0ìœ¼ë¡œ ë§ì¶”ê³ , hidden stateì˜ ë¶„ì‚°ì„ ì œì–´í•˜ê¸° ì¢‹ìŒ; ê²½í—˜ì  ì´ìœ ë„ ìˆìŒ)



#### Update Gate


![39](/assets/img/2023-07-12-CS224n---Lecture-6-(Simple-and-LSTM-RNNs).md/39.png)

- $z^{(t)}$ : **update signa**lë¡œ $h^{(t-1)}$ì´ **ë‹¤ìŒ stateë¡œ** **ì–¼ë§ˆë‚˜ ë§ì´ ì „ë‹¬ë˜ì–´ì•¼ í•˜ëŠ”ì§€**ë¥¼ ê²°ì •í•¨

	ì˜ˆë¥¼ ë“¤ì–´, $z^{(t)} $**ì´ 1ì— ê°€ê¹ë‹¤ë©´,** $h^{(t-1)}$**ì€ ê±°ì˜ ëª¨ë‘** $h^{(t)}$**ë¡œ ë³µì‚¬**ë¨, **0ì— ê°€ê¹ë‹¤**ë©´ ëŒ€ë¶€ë¶„ì˜ **ìƒˆë¡œìš´ ë©”ëª¨ë¦¬** $\tilde h^{(t)}$**ê°€ ë‹¤ìŒ hidden stateë¡œ ì „ë‹¬**ë¨

- $U^{(z)}$ : $h^{(t-1)}$ ë¥¼ **ì²˜ë¦¬**í•˜ëŠ” **ê°€ì¤‘ì¹˜ í–‰ë ¬** (í•™ìŠµ O)
- $W^{(r)}$ : $x^{(t)}$ë¥¼ **ì²˜ë¦¬í•˜ëŠ” ê°€ì¤‘ì¹˜ í–‰ë ¬** (í•™ìŠµ O)


#### Hidden state


![40](/assets/img/2023-07-12-CS224n---Lecture-6-(Simple-and-LSTM-RNNs).md/40.png)





- $h^{(t)}$ : ì´ì „ hidden state $h^{(t-1)}$ ê³¼, update gateì˜ ì˜í–¥ì„ ë°›ì€ ìƒˆë¡œìš´ memory $\tilde h^{(t)}$ë¥¼ ë°›ì•„ ë”í•´ì„œ ìƒì„±ë¨

**ì¤‘ìš”í•œ ì **


updateí•´ì•¼ í•  parameterê°€ ë§ìŒ


â†’ $W, U, W^{(r)}, U^{(r)},W^{(z)},U^{(z)}$



## Long Short-Term Memory RNNs (LSTMs)


> ğŸ’¡ **hidden state â†’ hidden state +** **cell state** **(read, erase, write)**


![41](/assets/img/2023-07-12-CS224n---Lecture-6-(Simple-and-LSTM-RNNs).md/41.png)_LSTM_


![42](/assets/img/2023-07-12-CS224n---Lecture-6-(Simple-and-LSTM-RNNs).md/42.png)_RNN_


**ì°¨ì´ì **


â†’ **cell state (with many gates)**


![43](/assets/img/2023-07-12-CS224n---Lecture-6-(Simple-and-LSTM-RNNs).md/43.png)


![44](/assets/img/2023-07-12-CS224n---Lecture-6-(Simple-and-LSTM-RNNs).md/44.png)



#### Forget gate


![45](/assets/img/2023-07-12-CS224n---Lecture-6-(Simple-and-LSTM-RNNs).md/45.png)

- $f^{(t)}$ : **ê³¼ê±° ë©”ëª¨ë¦¬ ì…€** $c^{(t-1)}$ ì´ **í˜„ì¬ ë©”ëª¨ë¦¬ ì…€** $c^{(t)} $ì˜ ê³„ì‚°ì— ìœ ìš©í•œì§€ ì—¬ë¶€ë¥¼ í‰ê°€. $h^{(t-1)}$ì™€ $x^{(t)}$ë¥¼ ê°ê° ì²˜ë¦¬í•´ dot product í•´ì¤€ í›„, **sigmoid**ë¥¼ í†µí•´ 0~1 ì‚¬ì´ì˜ **í™•ë¥ ê°’(forgetí•  ì •ë„)**($f^{(t)}$**)ìœ¼ë¡œ ë°˜í™˜**í•´ $c^{(t-1)}$ ì™€ element-wise product

![46](/assets/img/2023-07-12-CS224n---Lecture-6-(Simple-and-LSTM-RNNs).md/46.png)



#### Input gate


![47](/assets/img/2023-07-12-CS224n---Lecture-6-(Simple-and-LSTM-RNNs).md/47.png)

- $i^{(t)}$ : **reset signal (ë¦¬ì…‹ ì‹ í˜¸)**

	$x^{(t)}$ì™€ $h^{(t-1)}$ì„ í†µí•´ $x^{(t)}$ì„ **preserveí•  ê°€ì¹˜ê°€ ìˆëŠ”ì§€ ì—¬ë¶€**ë¥¼ ê²°ì •.


	ì´ **ì •ë³´ì˜ ì§€í‘œ(**$i^{(t)}$**) (ê¸°ì–µí•  í™•ë¥  ë°˜í™˜)**


![48](/assets/img/2023-07-12-CS224n---Lecture-6-(Simple-and-LSTM-RNNs).md/48.png)



#### New memory generation


![49](/assets/img/2023-07-12-CS224n---Lecture-6-(Simple-and-LSTM-RNNs).md/49.png)

- $\tilde c^{(t)}$**:** $x^{(t)}$ì™€ $h^{(t-1)}$ë¥¼ tanhë¥¼ í†µí•´ ê³„ì‚°í•œ ê°’ì— preserveí•  ì •ë„ì¸ $i^{(t)}$ë¥¼ element-wise productí•´ **ìƒˆë¡œìš´ ë©”ëª¨ë¦¬** $\tilde c^{(t)}$ë¥¼ **ìƒì„±**

![50](/assets/img/2023-07-12-CS224n---Lecture-6-(Simple-and-LSTM-RNNs).md/50.png)



#### Final memory generation


![51](/assets/img/2023-07-12-CS224n---Lecture-6-(Simple-and-LSTM-RNNs).md/51.png)

- $c^{(t)}$: $i^{(t)}$**ë¥¼ ë°˜ì˜í•œ** $\tilde c^{(t)}$**ì™€** $f^{(t)}$**ë¥¼ ë°˜ì˜í•œ** $c^{(t-1)}$**ì„ ë”í•´ì„œ ìµœì¢… ë©”ëª¨ë¦¬ ì…€ì¸** $c^{(t)}$**ë¥¼ ìƒì„±**

![52](/assets/img/2023-07-12-CS224n---Lecture-6-(Simple-and-LSTM-RNNs).md/52.png)



#### Output/Exposure Gate


![53](/assets/img/2023-07-12-CS224n---Lecture-6-(Simple-and-LSTM-RNNs).md/53.png)

- $o^{(t)}$: final memory cellì¸ $c^{(t)}$ëŠ” hidden state $h^{(t)}$ì— ì €ì¥í•  í•„ìš”ê°€ ì—†ëŠ” ë§ì€ ì •ë³´ë¥¼ í¬í•¨. $c^{(t)}$ ì¤‘ ì–´ë–¤ ë¶€ë¶„ì´ hidden state $h^{(t)}$ì—ì„œ exposed ë˜ì–´ì•¼ í•˜ëŠ”ì§€ì˜ í™•ë¥ ($o^{(t)}$)ì— ëŒ€í•œ í‰ê°€ë¥¼ ìˆ˜í–‰. tanhë¥¼ í†µê³¼í•œ $c^{(t)}$ì™€ element-wise productë˜ì–´ ìµœì¢…ì ìœ¼ë¡œ $h^{(t)}$ë¥¼ ë°˜í™˜
- $h^{(t)}$: final hidden stateë¥¼ ì˜ë¯¸

![54](/assets/img/2023-07-12-CS224n---Lecture-6-(Simple-and-LSTM-RNNs).md/54.png)

- **ì–¸ì œ tanh, sigmoid?**
	- tanh : inputì„ **[-1,1] ì‚¬ì´ì˜ ê°’ìœ¼ë¡œ ë°˜í™˜**í•´ í‰ê· ì„ 0ìœ¼ë¡œ ë§ì¶”ê³ , hidden stateì˜ ë¶„ì‚°ì„ ì œì–´í•œ í›„ sigmoidë¥¼ í†µí•´ ë‚˜ì˜¨ í™•ë¥ ê³¼ element wiseë˜ì–´ ê²°ê´ê°’ì„ ìƒì„±í•  ë•Œ ì‚¬ìš©

		(í‰ê· ì„ 0ìœ¼ë¡œ ë§ì¶”ê³ , hidden stateì˜ ë¶„ì‚°ì„ ì œì–´í•˜ê¸° ì¢‹ìŒ; ê²½í—˜ì  ì´ìœ ë„ ìˆìŒ)

	- sigmoid : LSTMì— íŠ¹íˆ, forgetí•  ì •ë„, inputí•  ì •ë„ ë“±ì˜ í™•ë¥  ê°’ì´ ë§ì´ í•„ìš”í•œë°, ì´ ë•Œ sigmoidë¥¼ í†µí•´ í™•ë¥ ì„ ë°˜í™˜
- **ì–´ë–»ê²Œ vanishing gradientsë¥¼ í•´ê²°?**

	â†’ **preserve information** over many timesteps **(by memory cell)**


		(ë§Œì•½, $f^{(t)}=1, i^{(t)}= 0$ ì´ë©´ ì´ì „ ì •ë³´ê°€ ì™„ë²½ preserved)


	â†’ ì•„ì˜ˆ vanishing/exploding gradientê°€ ì™„ë²½í•˜ê²Œ ì—†ë‹¤ê³ ëŠ” í•  ìˆ˜ ì—†ì§€ë§Œ, **long-distance dependencyë¥¼ ì‰½ê²Œ í•™ìŠµí•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” ëª¨ë¸**

- **real-word success**

	**2013-2015 dominant approach**


	(í˜„ì¬ëŠ” transformers ë“±ì´ dominant)

- **vanishing/exploding gradient**ê°€ RNN ë§Œì˜ ë¬¸ì œ?

	**ì•„ë‹˜**   


	â†’ feed-forward, convolutional ë“±ë“± ë§ì€ **deep neural networkì˜ ë¬¸ì œ**   


	**solution?**


	â†’ add more direct connections 


	**ex) Residual connections â†’ ResNet**


	![55](/assets/img/2023-07-12-CS224n---Lecture-6-(Simple-and-LSTM-RNNs).md/55.png)


	**ë ˆì´ì–´ê°€ ì¼ë¶€ ì •ë³´ë¥¼ ì¶”ê°€í•˜ê±°ë‚˜ ìˆ˜ì •í•˜ë”ë¼ë„ ì´ì „ ì •ë³´ë¥¼ ë³´ì¡´í•˜ëŠ” ë°©ì‹**


	â†’ ì´ë¥¼ í†µí•´ ì¸µì´ ê¹Šì–´ì§€ë”ë¼ë„ ê·¸ë˜ë””ì–¸íŠ¸ê°€ ì‚¬ë¼ì§€ì§€ ì•Šê³  ì „ë‹¬ë˜ì–´ í•™ìŠµì´ ì˜ ë˜ë„ë¡ ë„ì›€

	- **ìŠ¤í‚µ ì—°ê²° skip connection**: ë ˆì´ì–´ì˜ ì¶œë ¥ê°’ì„ ë‹¤ìŒ ë ˆì´ì–´ë¡œ ë°”ë¡œ ì „ë‹¬í•˜ëŠ” ì—°ê²°ë¡œ, ì¶œë ¥ê°’ê³¼ ì…ë ¥ê°’ì„ ë”í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ì´ë£¨ì–´ì§‘ë‹ˆë‹¤. ì´ì „ ë ˆì´ì–´ì˜ ì¶œë ¥ê°’ì„ í˜„ì¬ ë ˆì´ì–´ì˜ ì…ë ¥ê°’ì— ë”í•¨ìœ¼ë¡œì¨, ì´ì „ ë ˆì´ì–´ì˜ ì •ë³´ê°€ ê·¸ëŒ€ë¡œ ë³´ì¡´ë˜ë©° ê·¸ë˜ë””ì–¸íŠ¸ê°€ ì˜ ì „ë‹¬ë˜ê²Œ ë©ë‹ˆë‹¤.

	**ex) Dense connections â†’ DenseNet**


	![56](/assets/img/2023-07-12-CS224n---Lecture-6-(Simple-and-LSTM-RNNs).md/56.png)


	**ì§ì ‘ì ìœ¼ë¡œ ëª¨ë“  layerë¥¼ future layersì™€ ì—°ê²°**


	**ex) Highway connections â†’ HighwayNet**


	![57](/assets/img/2023-07-12-CS224n---Lecture-6-(Simple-and-LSTM-RNNs).md/57.png)


	resNetê³¼ ìœ ì‚¬í•˜ì§€ë§Œ, transformation layerê°€ dynamic gateì— ì˜í•´ ì¡°ì ˆ


	LSTMsì— ì˜ê°ì„ ë°›ì€ ë°©ì‹


RNNs ë“±ì€ ê³„ì† ê°™ì€ weight matrixê°€ ë°˜ë³µì ìœ¼ë¡œ ê³±í•´ì§€ê¸° ë•Œë¬¸ì—, ë°˜ë³µ í•™ìŠµì´ ì—†ë‹¤ëŠ” ì¥ì ë„ ìˆì§€ë§Œ, back-propagationì—ì„œì˜ vanishing gradientëŠ” ì–´ì©” ìˆ˜ ì—†ìŒ

