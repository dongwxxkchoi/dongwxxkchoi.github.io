---
layout: single
date: 2023-07-07
title: "CS224n - Lecture 5 (Recurrent Neural Networks (RNNs))"
use_math: true
tags: [ê°•ì˜/ì±… ì •ë¦¬, ]
categories: [AI, ]
---

> â˜‘ï¸ **Lecture 5  
>   
> 1. Neural dependency parsing  
> 2. A bit more about neural networks  
> 3. Language modeling + RNNs**



### Dependency parser


![0](/assets/img/2023-07-07-CS224n---Lecture-5-(Recurrent-Neural-Networks-(RNNs)).md/0.png)


**conventional transition based dependency parser**

- worked with indicator features**(= boolean features)**

	â†’ represent the presence or absence


	â†’ specifying some condition if itâ€™s true of a configuration

	- word on top of the stack is good
	- part of speech is adjective
	- next word coming up is a personal pronoun

ì´ëŸ° êµ¬ì„± ìš”ê±´ í•˜ë‚˜í•˜ë‚˜ê°€ boolean featuresë¡œ ì €ì¥ 


ê·¸ë ‡ê¸° ë•Œë¬¸ì— **ë‹¨ì ì´ ì¡´ì¬**


ë‹¨ì  1 - features: **sparse**  


ë‹¨ì  2 - features: **incomplete** 


ë‹¨ì  3 - **expensive computation**


	â†’ 95%ì˜ parsing timeì´ feature computationì— ì“°ì„


ê·¸ë˜ì„œ ë“±ì¥í•œ ê²ƒì´ **Neural Dependency Parser**


(ì¢€ ë” denseí•˜ê³  compactí•œ feature representation ë³´ì—¬ì¤Œ)



#### Neural Dependency Parser


![1](/assets/img/2023-07-07-CS224n---Lecture-5-(Recurrent-Neural-Networks-(RNNs)).md/1.png)

- use **Stack** and **Buffer**
- **dimensionality** : several million â†’ 1,000

![2](/assets/img/2023-07-07-CS224n---Lecture-5-(Recurrent-Neural-Networks-(RNNs)).md/2.png)


ë‘ ê°€ì§€ typeì˜ parser 

- **Transition based parser**
	- ì¢…ë¥˜
		- MaltParser
		- C&M 2014
	- íŠ¹ì§•
		- ë¹ ë¦„ - ëª¨ë“  feature computation ìˆ˜í–‰í•˜ì§€ ì•Šì•„ì„œ
- **Symbolic Graph based parser**
	- ì¢…ë¥˜
		- MSTParser
		- TurboParser
	- íŠ¹ì§•
		- ì •í™•í•¨
		- transition based parserì— ë¹„í•´ ëŠë¦¼

**â‡’** **C&M Parser â†’ best**



#### Distributed Representations


ê° wordë¥¼ word embeddingí•´ì„œ ë‚˜íƒ€ëƒ„


â†’ Similar wordsëŠ” close vectors


![3](/assets/img/2023-07-07-CS224n---Lecture-5-(Recurrent-Neural-Networks-(RNNs)).md/3.png)


**part-of-speech tags (POS)**ì™€ **dependency labels** ë˜í•œ d-dimensional vectorsë¡œ í‘œí˜„ ê°€ëŠ¥


ë” ì‘ì€ discrete setsê°€ ë§ì€ semantical similaritiesë¥¼ ë“œëŸ¬ë‚¼ ìˆ˜ ìˆìŒ


![4](/assets/img/2023-07-07-CS224n---Lecture-5-(Recurrent-Neural-Networks-(RNNs)).md/4.png)


---


> ğŸ’¡ **What is Part-of-speech (POS) tags (í’ˆì‚¬ íƒœê¹…)?**  
>   
> syntactic and grammatical context(ë¬¸ë§¥)ì— ê¸°ë°˜í•´ ë¬¸ì¥ì—ì„œ ê° ë‹¨ì–´ì˜ í’ˆì‚¬ë¥¼ í• ë‹¹í•˜ëŠ” ê³¼ì •  
>   
> - **ë‹¨ì–´ì˜ ì¹´í…Œê³ ë¦¬**  
> - **ë¬¸ì¥ì—ì„œì˜ êµ¬ë¬¸ ê¸°ëŠ¥**  
> - **ë‹¤ë¥¸ ë‹¨ì–´ë“¤ê³¼ì˜ ê´€ê³„**ë¥¼ ë‚˜íƒ€ëƒ„  
>   
> ì¼ë°˜ì ì¸ pos:   
> - nouns (ëª…ì‚¬)  
> - verbs (ë™ì‚¬)  
> - adjectives (í˜•ìš©ì‚¬)  
> - adverbs (ë¶€ì‚¬)  
> - pronouns (ëŒ€ëª…ì‚¬)  
> - prepositions (ì „ì¹˜ì‚¬)  
> - conjunctions (ì ‘ì†ì‚¬)  
> - interjections (ê°íƒ„ì‚¬)  
>   
> **The cat sat on the mat**  
> - â€œTheâ€: DT(determiner)  
> - "cat": NN(noun)  
> - "sat": VBD(verb; past tense)  
> - "on": IN(preposition or subordinating conjuction)  
> - "the": DT(determiner)  
> - â€œmatâ€: NN(noun)  
>   
> ê°œì²´ëª… ì¸ì‹, ê°ì • ë¶„ì„, í…ìŠ¤íŠ¸ ë¶„ë¥˜ ë° ê¸°ê³„ ë²ˆì—­ê³¼ ê°™ì€ ë§ì€ ìì—°ì–´ ì²˜ë¦¬ ì‘ì—…ì— ì¤‘ìš”í•œ ì „ì²˜ë¦¬ ë‹¨ê³„


[bookmark](https://stackoverflow.com/questions/29332851/what-does-nn-vbd-in-dt-nns-rb-means-in-nltk)


---


**Example - POS tagging & dependency labels**


![5](/assets/img/2023-07-07-CS224n---Lecture-5-(Recurrent-Neural-Networks-(RNNs)).md/5.png)



### Extracting Tokens & vector representations from configuration


stack, bufferì„ ì´ìš©í•´, set of tokensë¥¼ ì¶”ì¶œí•˜ëŠ” ë°©ë²•


![6](/assets/img/2023-07-07-CS224n---Lecture-5-(Recurrent-Neural-Networks-(RNNs)).md/6.png)


**â†’ C&M Parser ì½ëŠ” ë²•?**


[bookmark](https://velog.io/@tobigs-text1415/Lecture-5-Linguistic-Structure-Dependency-Parsing)



#### Softmax Classifier


deep learningì—ì„œ ìì£¼ ì‚¬ìš©í•˜ëŠ” simpleí•œ classifier


![7](/assets/img/2023-07-07-CS224n---Lecture-5-(Recurrent-Neural-Networks-(RNNs)).md/7.png)

- **inputs** : $x \in \mathbb{R}^d$
- **classes** : $y \in C$
- **Weight** **matrix** : $W \in \mathbb{R}^{C\times d}$

d dimensional vectorsì¸ **xë¥¼ ì…ë ¥**ìœ¼ë¡œ ë°›ì•„ **Cì— ì†í•œ í´ë˜ìŠ¤ yì— ëŒ€í•œ í™•ë¥ ì„ ë°˜í™˜**


$$
p(y|x) = \frac{exp(W_y \cdot x)}{\Sigma^C_{c=1}exp(W_c\cdot x)}
$$

- **softmax**ë¥¼ í†µí•´, negative log lossì¸ $\Sigma_i-log~p(y_i|x_i)$ ë¥¼ ìµœì†Œí™”í•¨ **(= cross entropy loss)**
- Traditional ML classifiersì— ë¹„í•´ ì„±ëŠ¥ ì¢‹ìŒ **(non-linear classification > linear classification)**

	![8](/assets/img/2023-07-07-CS224n---Lecture-5-(Recurrent-Neural-Networks-(RNNs)).md/8.png)


í•˜ì§€ë§Œ softmax ìì²´ê°€ non linearí•œ classificationì„ ì œê³µí•˜ëŠ” ê²ƒì€ ì•„ë‹˜


softmax ì•„ë˜ì— ìŒ“ì—¬ ìˆëŠ” **neural networkë¡œ ë¶€í„° non linearí•œ í‘œí˜„ ê³µê°„**ì„ ì œê³µ ë°›ìŒ


(what neural net can do is warp the space around and move the representation of data points)


softmaxëŠ” ê·¸ ë¹„ì„ í˜•ì  í‘œí˜„ ê³µê°„ì—ì„œ **linearí•œ classification ìˆ˜í–‰**


**â‡’ simple feed forward neural network multi-class classifier**


![9](/assets/img/2023-07-07-CS224n---Lecture-5-(Recurrent-Neural-Networks-(RNNs)).md/9.png)


![10](/assets/img/2023-07-07-CS224n---Lecture-5-(Recurrent-Neural-Networks-(RNNs)).md/10.png)


![11](/assets/img/2023-07-07-CS224n---Lecture-5-(Recurrent-Neural-Networks-(RNNs)).md/11.png)


![12](/assets/img/2023-07-07-CS224n---Lecture-5-(Recurrent-Neural-Networks-(RNNs)).md/12.png)


ì´ë ‡ê²Œ Neural Networkê°€ êµ¬ì„±ë˜ëŠ”ë°, Dependency parser


![13](/assets/img/2023-07-07-CS224n---Lecture-5-(Recurrent-Neural-Networks-(RNNs)).md/13.png)


![14](/assets/img/2023-07-07-CS224n---Lecture-5-(Recurrent-Neural-Networks-(RNNs)).md/14.png)


Graph-based dependency parsers



#### Multiclass Classifier using softmax


Dependency parser model


graph-based dependency parser



#### Regularization



#### L2 Regularization


![15](/assets/img/2023-07-07-CS224n---Lecture-5-(Recurrent-Neural-Networks-(RNNs)).md/15.png)

- big neural nets, so many parameters â†’ **overfit ê°€ëŠ¥ì„± ë†’ìŒ**
- $\lambda\Sigma_k\theta^2_k$ì—ì„œì˜ $\lambda$**ëŠ” strength of regularizationì„ ë‚˜íƒ€ëƒ„**
	- $\lambda$ê°€ í¬ë©´ í´ìˆ˜ë¡, regularization ê°•ë„ ì„¸ì§

![16](/assets/img/2023-07-07-CS224n---Lecture-5-(Recurrent-Neural-Networks-(RNNs)).md/16.png)



#### Dropout

- Train ê³¼ì •ì—ì„œ, **ëœë¤í•˜ê²Œ ë‰´ëŸ°ì˜ ì¼ë¶€ë¶„ì„ 0ìœ¼ë¡œ ë§Œë“¤ì–´** í•™ìŠµë˜ì§€ ì•Šë„ë¡ í•˜ëŠ” ê²ƒ
- prevent **feature co-adaptation**
undefined<details>
  <summary>**feature co-adaptation**</summary>


ìƒí˜¸ì ì‘ ë¬¸ì œëŠ”,Â ì‹ ê²½ë§ì˜ í•™ìŠµ ì¤‘,Â **ì–´ëŠ ì‹œì ì—ì„œ ê°™ì€ ì¸µì˜ ë‘ ê°œ ì´ìƒì˜ ë…¸ë“œì˜ ì…ë ¥ ë° ì¶œë ¥ ì—°ê²°ê°•ë„ê°€ ê°™ì•„ì§€ë©´,Â ì•„ë¬´ë¦¬ í•™ìŠµì´ ì§„í–‰ë˜ì–´ë„ ê·¸ ë…¸ë“œë“¤ì€ ê°™ì€ ì¼ì„ ìˆ˜í–‰í•˜ê²Œ ë˜ì–´ ë¶ˆí•„ìš”í•œ ì¤‘ë³µì´ ìƒê¸°ëŠ” ë¬¸ì œ**ë¥¼ ë§í•œë‹¤.Â ì¦‰ ì—°ê²°ê°•ë„ë“¤ì´ í•™ìŠµì„ í†µí•´ ì—…ë°ì´íŠ¸ ë˜ë”ë¼ë„ ì´ë“¤ì€ ê³„ì†í•´ì„œ ì„œë¡œ ê°™ì€ ì…ì¶œë ¥ ì—°ê²° ê°•ë„ë“¤ì„ ìœ ì§€í•˜ê²Œ ë˜ê³  ì´ëŠ” ê²°êµ­ í•˜ë‚˜ì˜ ë…¸ë“œë¡œ ì‘ë™í•˜ëŠ” ê²ƒìœ¼ë¡œì¨,Â ì´í›„ ì–´ë– í•œ í•™ìŠµì„ í†µí•´ì„œë„ ì´ë“¤ì€ ë‹¤ë¥¸ ê°’ìœ¼ë¡œ ë‚˜ëˆ ì§ˆ ìˆ˜ ì—†ê³  ìƒí˜¸ ì ì‘í•˜ëŠ” ë…¸ë“œë“¤ì—ëŠ” ë‚­ë¹„ê°€ ë°œìƒí•˜ëŠ” ê²ƒì´ë‹¤.Â ê²°êµ­ ì´ê²ƒì€ ì»´í“¨íŒ… íŒŒì›Œì™€ ë©”ëª¨ë¦¬ì˜ ë‚­ë¹„ë¡œ ì´ì–´ì§„ë‹¤.


ë“œëì•„ì›ƒì€ ì´ëŸ¬í•œ ìƒí˜¸ì ì‘ ë¬¸ì œë¥¼ í•´ì†Œí•œë‹¤.Â ì¦‰,Â ë“œëì•„ì›ƒì´ ì„ì˜ë¡œ ë…¸ë“œë“¤ì„ ìƒëµí•  ë•Œ ì´ëŸ¬í•œ ìƒí˜¸ ì ì‘ ì¤‘ì¸ ë…¸ë“œë“¤ ì¤‘ ì¼ë¶€ëŠ” ìƒëµí•˜ê³  ì¼ë¶€ëŠ” ìƒëµí•˜ì§€ ì•Šê²Œ ë˜ë¯€ë¡œ í•™ìŠµ ì¤‘ ìƒí˜¸ ì ì‘ì´ ë°œìƒí•œ ë…¸ë“œë“¤ì´ ë¶„ë¦¬ë  ìˆ˜ ìˆì–´ì„œ ìƒí˜¸ ì ì‘ ë¬¸ì œë¥¼ íšŒí”¼í•  ìˆ˜ ìˆê²Œ ëœë‹¤.



  </details>- Naive Bayes ëª¨ë¸ê³¼ Logistic Regression Modelì˜ ì¤‘ê°„ ëŠë‚Œì„ ì œê³µ

	(Naive BayesëŠ” weightsê°€ ëª¨ë‘ ë…ë¦½ì ì¸ ë°˜ë©´, logistic regression modelì—ì„  ëª¨ë“  weightsê°€ ì—°ê´€ì´ ìˆìŒ)



#### Vectorization


**Vector loop** v.s. **concatenated vector matrix**


![17](/assets/img/2023-07-07-CS224n---Lecture-5-(Recurrent-Neural-Networks-(RNNs)).md/17.png)


![18](/assets/img/2023-07-07-CS224n---Lecture-5-(Recurrent-Neural-Networks-(RNNs)).md/18.png)



#### Activation Function â†’ Non-linearities


Multi-Layer Neural Networkë¥¼ ì„¤ê³„í•˜ë©´ì„œ, **non-linearityë¥¼ ê°–ëŠ” ê²ƒì´ ì¤‘ìš”**í•˜ë‹¤ê³  ìœ„ì—ì„œ ì–¸ê¸‰


Layer ì‚¬ì´ì—ì„œ ê·¸ ì—­í• ì„ í•´ì£¼ëŠ” ê²ƒì´ ë°”ë¡œ activation function


![19](/assets/img/2023-07-07-CS224n---Lecture-5-(Recurrent-Neural-Networks-(RNNs)).md/19.png)

- **Sigmoid**

	â†’ real numberë¥¼ ë°›ì•„ [0, 1]ë¡œ ë³€í™˜


	â†’ í•­ìƒ positive spaceì¸ ê²ƒì´ ë‹¨ì 

- **tanh**

	â†’ rescaled and shifted version of sigmoid


	â†’ [-1, 1] ì‚¬ì´ì—ì„œ ëŒ€ì¹­


	â†’ ëŠë¦¬ê³ , ê³„ì‚°í•˜ê¸° expensive

- **hard tanh**

	â†’ fast, less expensive


	â†’ flat linesì„ ì‚´ë¦° tanh

- **ReLU (Rectified Linear Unit)**

	â†’ ê°€ì¥ ë§ì´ ì‚¬ìš©í•˜ëŠ” ë°©ì‹ 


	â†’ ê°€ì¥ simple, negativeëŠ” 0, positiveëŠ” y=x


	â†’ train very quickly, straightforward gradient back flow

- **Leaky ReLU**

	â†’ ReLUì˜ ë³€í˜•


	â†’ negative ë¶€ë¶„ì— slope

- **Parametric ReLU**

	â†’ slopeë¥¼ parameterí™” í•´ì„œ ì¡°ì ˆí•  ìˆ˜ ìˆìŒ

- **Swish**

	â†’ 0 ë¶€ê·¼ì—ì„œ gradientë¥¼ ê°€ì§


ì´ ì¤‘ ì–´ë–¤ ê²ƒë„ superiorí•œ ê²ƒì€ ì—†ìŒ



#### Parameter Initialization

- Uniform distribution [-r, r]
- zero initialization
- **Xavier Initialization**

	sigmoid ê³„ì—´ì˜ activation functionì„ ì‚¬ìš©í•  ë•Œ, ê°€ì¤‘ì¹˜ë¥¼ ì´ˆê¸°í™”í•˜ëŠ” ë°©ë²•


	ì…ë ¥ ë°ì´í„°ì˜ ë¶„ì‚°ì´ ì¶œë ¥ ë°ì´í„°ì—ì„œ ìœ ì§€ë˜ë„ë¡ ê°€ì¤‘ì¹˜ë¥¼ ì´ˆê¸°í™”


	$$
	Var(W_i) = \frac{2}{n_{in}+n_{out}}
	$$



#### Optimizer

- **SGD**
- Adagrad
- RMSprop
- **Adam â†’ please Use!**
- SparseAdam


#### Learning Rates


modelì— ë”°ë¼ ì„¤ì •í•´ì•¼ í•˜ëŠ” ì •ë„ê°€ ë‹¤ë¦„


$10^{-3} \sim 10^{-4}$ ì •ë„ë©´ ì¢‹ìŒ


â†’ **halve the learning rate after every k epochs**



#### Language Modeling


**Language Modeling**


: the task of predicting what word comes next


![20](/assets/img/2023-07-07-CS224n---Lecture-5-(Recurrent-Neural-Networks-(RNNs)).md/20.png)


![21](/assets/img/2023-07-07-CS224n---Lecture-5-(Recurrent-Neural-Networks-(RNNs)).md/21.png)


![22](/assets/img/2023-07-07-CS224n---Lecture-5-(Recurrent-Neural-Networks-(RNNs)).md/22.png)


ì¦‰, $x^{(1)},x^{(2)},...,x^{(t)}$ì˜ sequenceê°€ ì•ì— ì£¼ì–´ì¡Œì„ ë•Œ, 


ë‹¤ìŒ wordì¸ $x^{(t+1)}$ì˜ probability distributionì„ ê³„ì‚°í•˜ëŠ” ê²ƒì´ë¼ê³  í•  ìˆ˜ ìˆë‹¤.


(ë‹¨, $x^{(t+1)}$ì€ vocab ëª¨ìŒì¸ $V = \{w_1, ..., w_{|V|}\}$ì— ì†í•˜ëŠ” word)


(ì¦‰, ìš°ë¦¬ê°€ ì´ë¯¸ ì•Œê³  ìˆëŠ” vocab ì¤‘ì— ì˜ˆì¸¡ì„ ì§„í–‰)


![23](/assets/img/2023-07-07-CS224n---Lecture-5-(Recurrent-Neural-Networks-(RNNs)).md/23.png)


â†’ _**Language Modelë¡œ ë‹¤ìŒì„ ê³„ì‚°í•  ìˆ˜ ìˆìŒ**_



#### n-gram Language Models


neural network ë°©ì‹ ì „ 2~30ë…„ ë™ì•ˆ ì§€ë°°ì ì¸ ë°©ì‹


**n-gram**


: chunk of _**n**_ **consecutive(ì—°ì´ì€) words**

- unigrams - 1ê°œ word
- bigrams - 2ê°œ words
- trigrams - 3ê°œ words
- 4-grams - 4ê°œ words

**Markov Assumption**


â†’ $x^{(t+1)}$**depends only on the preceding** _**n**_**-1 words**


![24](/assets/img/2023-07-07-CS224n---Lecture-5-(Recurrent-Neural-Networks-(RNNs)).md/24.png)_ì¦‰, ì•ì˜ n-1 wordsì—ë§Œ ì˜í–¥ì„ ë°›ëŠ”ë‹¤ëŠ” ê°€ì •_


![25](/assets/img/2023-07-07-CS224n---Lecture-5-(Recurrent-Neural-Networks-(RNNs)).md/25.png)


ì¡°ê±´ë¶€ í™•ë¥ ì„ êµ¬í•˜ë ¤ë©´, ê°ê° n, n-1 gramì˜ í™•ë¥ ì„ ë‹¤ êµ¬í•´ì•¼ í•¨    **how?**


![26](/assets/img/2023-07-07-CS224n---Lecture-5-(Recurrent-Neural-Networks-(RNNs)).md/26.png)


â†’ like í°ìˆ˜ì˜ ë²•ì¹™


![27](/assets/img/2023-07-07-CS224n---Lecture-5-(Recurrent-Neural-Networks-(RNNs)).md/27.png)


**n-gram** ëª¨ë¸ì„ ì‚¬ìš© â†’ $x^{t+1}$ì˜ ì˜ˆì¸¡ì„ ìœ„í•´ ê·¸ **ì•ì˜ nê°œì˜ wordsë§Œ ê³ ë ¤**í•˜ê² ë‹¤. (ê·¸ ì „ì€ discard)


![28](/assets/img/2023-07-07-CS224n---Lecture-5-(Recurrent-Neural-Networks-(RNNs)).md/28.png)


í•˜ì§€ë§Œ ì´ë ‡ê²Œ ëœë‹¤ë©´, ì•ì˜ proctorì— ëŒ€í•œ ë‚´ìš©ì€ ì‚¬ë¼ì§€ê²Œ ë˜ì–´, 


ë¬¸ë§¥ì— ë” ë§ëŠ” wordsì¸ **exams**ê°€ ì•„ë‹ˆë¼ **books**ë¡œ ì˜ˆì¸¡í•  í™•ë¥ ì´ ë†’ìŒ


**â‡’ Problem 1**


(Countingì„ ì´ìš©í•œë‹¤ëŠ” ì ì—ì„œ Naive Bayes Modelsì™€ ë¹„ìŠ·, but unigramì´ê¸° ë•Œë¬¸ì— neighborsë¥¼ ê³ ë ¤í•˜ì§€ ì•ŠëŠ” ë‹¤ëŠ” ì ì€ ë‹¤ë¦„)



#### Sparsity Problem


![29](/assets/img/2023-07-07-CS224n---Lecture-5-(Recurrent-Neural-Networks-(RNNs)).md/29.png)


**íŠ¹ì • N-gramì˜ corpus ë‚´ ì¶œí˜„ ë¹ˆë„ê°€ ë‚®ì„ ë•Œ ë°œìƒ** 


**Problem 1) â€œ**students opened their wâ€ê°€ ì´ì „ì— ë“±ì¥í•˜ì§€ ì•ŠëŠ”ë‹¤ë©´?


**â†’ smoothing** (ì‘ì€ ê°’ì„ ë”í•´ 0ì´ ë˜ì§€ ì•Šë„ë¡ í•˜ëŠ” ê²ƒ)


**Problem 2) â€œ**students opened theirâ€ ê°€ ì´ì „ì— ë“±ì¥í•˜ì§€ ì•ŠëŠ”ë‹¤ë©´?


**â†’ backoff** (use N-1 gram)


**â‡’ nì„ ì¦ê°€ì‹œí‚¤ëŠ” ê²ƒì€ ì‹¬ê°í•œ Sparsity Problem ì´ˆë˜**



#### Storage Problems


![30](/assets/img/2023-07-07-CS224n---Lecture-5-(Recurrent-Neural-Networks-(RNNs)).md/30.png)


**N-gramì˜ ëª¨ë“  ì •ë³´ë¥¼ ì €ì¥í•˜ë ¤ë©´, modelì˜ í¬ê¸°ê°€ ì§€ë‚˜ì¹˜ê²Œ ì»¤ì§**


---



#### **Example - Generating Text by predicted words**


![31](/assets/img/2023-07-07-CS224n---Lecture-5-(Recurrent-Neural-Networks-(RNNs)).md/31.png)


![32](/assets/img/2023-07-07-CS224n---Lecture-5-(Recurrent-Neural-Networks-(RNNs)).md/32.png)


![33](/assets/img/2023-07-07-CS224n---Lecture-5-(Recurrent-Neural-Networks-(RNNs)).md/33.png)


![34](/assets/img/2023-07-07-CS224n---Lecture-5-(Recurrent-Neural-Networks-(RNNs)).md/34.png)


![35](/assets/img/2023-07-07-CS224n---Lecture-5-(Recurrent-Neural-Networks-(RNNs)).md/35.png)


![36](/assets/img/2023-07-07-CS224n---Lecture-5-(Recurrent-Neural-Networks-(RNNs)).md/36.png)


**grammatical but not incoherent**


**trigramì˜ ì˜ˆì‹œì¸ë°, nì„ ë” ëŠ˜ë¦¬ëŠ” ê²ƒì€ ë¬¸ì œê°€ ìˆìœ¼ë¯€ë¡œ ì¢‹ì§€ ëª»í•œ ë°©ë²•**


---



### Neural Language Model


windowë¥¼ center word ì£¼ìœ„ê°€ ì•„ë‹Œ, center word ì§ì „ìœ¼ë¡œ ìƒì„± (fixed window)


![37](/assets/img/2023-07-07-CS224n---Lecture-5-(Recurrent-Neural-Networks-(RNNs)).md/37.png)


ê·¸ê±¸ ë°”íƒ•ìœ¼ë¡œ í•´ì„œ neural network ì„¤ê³„


![38](/assets/img/2023-07-07-CS224n---Lecture-5-(Recurrent-Neural-Networks-(RNNs)).md/38.png)


â†’ softmaxë¥¼ í†µí•´, vocab ì† wordë“¤ì— ëŒ€í•œ í™•ë¥  ë°˜í™˜


â†’ í•™ìŠµì— negative samplingë„ ì‚¬ìš© ê°€ëŠ¥


![39](/assets/img/2023-07-07-CS224n---Lecture-5-(Recurrent-Neural-Networks-(RNNs)).md/39.png)

- **ì¥ì **
1. Softmax â†’ **Sparsity ë¬¸ì œ í•´ì†Œ**
2. Counting ê°’ì„ ì €ì¥í•  í•„ìš”ê°€ ì—†ìŒ â†’ **Storage ë¬¸ì œ í•´ì†Œ**
- **ë‹¨ì **
1. N-gramê³¼ ê°™ì´ ë¬¸ë§¥ì„ ë°˜ì˜í•˜ì§€ ëª»í•¨ (small window size)
2. ë‹¨ì–´ì˜ ìœ„ì¹˜ì— ë”°ë¼ ê³±í•´ì§€ëŠ” ê°€ì¤‘ì¹˜ê°€ ë‹¤ë¥´ê¸° ë•Œë¬¸ì— **Neural Modelì´ ë¹„ìŠ·í•œ ë‚´ìš©ì„ ì—¬ëŸ¬ ë²ˆ í•™ìŠµí•˜ëŠ” ë¹„íš¨ìœ¨ì„±**ì„ ê°€ì§


### RNN(Recurrent Neural Network) Language Model


**ìˆœì°¨ì ì¸ (Recurrent) ì •ë³´ë¥¼ ì²˜ë¦¬í•˜ëŠ” ê²ƒì´ ê¸°ë³¸ ì•„ì´ë””ì–´**


**1) ë™ì¼í•œ íƒœìŠ¤í¬ë¥¼ í•œ ì‹œí€€ìŠ¤ì˜ ìš”ì†Œë§ˆë‹¤ ì ìš©**


**2)** **ì¶œë ¥ ê²°ê³¼ê°€ ì´ì „ì˜ ê³„ì‚° ê²°ê³¼ì— ì˜í–¥ ë°›ìŒ**


![40](/assets/img/2023-07-07-CS224n---Lecture-5-(Recurrent-Neural-Networks-(RNNs)).md/40.png)


![41](/assets/img/2023-07-07-CS224n---Lecture-5-(Recurrent-Neural-Networks-(RNNs)).md/41.png)

