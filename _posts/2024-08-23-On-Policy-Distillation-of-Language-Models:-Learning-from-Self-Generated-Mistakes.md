---
layout: single
date: 2024-08-23
title: "On-Policy Distillation of Language Models: Learning from Self-Generated Mistakes"
use_math: true
author_profile: false
tags: [ë…¼ë¬¸ ì •ë¦¬, ]
categories: [AI, ]
---


## ë…¼ë¬¸ ì†Œê°œ 

1. ì œëª© : On-Policy Distillation of Language Models: Learning from Self-Generated Mistakes
2. keyword : Knowledge Distillation, Auto-regressive model, On-policy generated data, Imitation Learning, RLHF/RLAIF
3. í•™íšŒ/ì €ë„ : ICLR
4. ì¶œíŒë…„ë„ : 2024
5. ë…¼ë¬¸

[bookmark](https://arxiv.org/abs/2306.13649)



## ë³¸ë¬¸ ì†Œê°œ



### ë…¼ë¬¸ ì„ ì • ì´ìœ 


LLMì˜ Knowledge Distillation ê´€ë ¨ ìµœì‹  ë…¼ë¬¸. On-policy generated dataë¥¼ ì´ìš©í•œë‹¤ëŠ” ì ê³¼, RLHFì™€ì˜ ì‰¬ìš´ í†µí•©ì´ ê°€ëŠ¥í•˜ë‹¤ëŠ” ê²ƒì´ ì¸ìƒ ê¹Šì–´ì„œ ì„ íƒí–ˆìŠµë‹ˆë‹¤.



### ë…¼ë¬¸ 3ë¬¸ì¥ ìš”ì•½

- Issue: teacher-studentì˜ distribution mismatch
- Limitation of previous works

	1. Fixed datasetì˜ ì´ìš© 


	2. KLì˜ ì„ íƒ

- Contribution
	1. on-policy student generated outputs ì´ìš© distillation ê³ ì•ˆ
	2. RLHF/RLAIF ë“±ê³¼ ì—°ê²°ë  ìˆ˜ ìˆëŠ” GKD ì œì‹œ
	3. Distillationì—ì„œì˜ on-policy student generated output sequenceì˜ ì¤‘ìš”ì„± í™•ì¸
	4. student-teacher ê°„ì˜ ìµœì ì˜ divergenceê°€ task-dependent í•˜ë‹¤ëŠ” í†µì°° ì œê³µ


### 3. Introduction


LLMê³¼ ê°™ì€ auto-regressive sequence modelsëŠ” parameter ìˆ˜ë¥¼ ì¦ê°€ ì‹œí‚´ìœ¼ë¡œì¨, ì„±ëŠ¥ì„ ì¦ëŒ€ì‹œì¼°ê³ , ì´ëŠ” ë°°í¬ì˜ ì–´ë ¤ì›€ì„ ì´ˆë˜í•œë‹¤. ë”°ë¼ì„œ, ì´ëŸ° **í° ëª¨ë¸ë“¤ì˜ ì„±ëŠ¥ì„ ìœ ì§€í•˜ë©´ì„œ parameter ìˆ˜ë¥¼ ì¤„ì´ëŠ” ê²ƒì´ ëª©í‘œ**ê°€ ë˜ê³  ìˆë‹¤.


Model Compression ê¸°ë²• ì¤‘ í•˜ë‚˜ê°€ **knowledge distillation [1]**ì´ë‹¤. í•˜ì§€ë§Œ, auto-regressive sequence modelsì—ì„œ teacher modelì˜ output sequence ë˜ëŠ” token-levelì˜ **fixed setì„ ë§Œë“¤ì–´ ì‚¬ìš©í•˜ëŠ” ë°©ì‹**ì€ **1) expensive**í•˜ê³ , 2) **teacher-student ê°„ outputì˜ distribution mismatch**(Imitation Learningì—ì„œì˜ ë¬¸ì œì ê³¼ ë™ì¼. DAgger[2] ë“±ì¥)ë¡œ ì´ëŒ ìˆ˜ ìˆë‹¤. **forward KL divergenceë¥¼ ìµœì†Œí™”í•˜ëŠ” í˜„ì¬ ë°©ì‹**ì€, studentì˜ expression ability ë¶€ì¡±ìœ¼ë¡œ **ì„±ëŠ¥ì´ ì¢‹ì§€ ëª»í•˜ë‹¤**.


ì´ ë…¼ë¬¸ì—ì„œëŠ”, **Generalized KD (GKD)**ë¥¼ í†µí•´ ë¬¸ì œë¥¼ í•´ê²°í–ˆë‹¤. **auto-regressive sequence modelsì— ëŒ€í•œ KDë¥¼ Imitation learningìœ¼ë¡œ ê°„ì£¼**í–ˆê³ , **studentì˜ self-generated sequenceë¥¼ í†µí•´ í•™ìŠµ**í•  ìˆ˜ ìˆë„ë¡ í–ˆë‹¤. ë˜í•œ, **reverse KLê³¼ generalized JSD** ë“±ì„ í†µí•´ studentì˜ expression ability ë¶€ì¡± ë¬¸ì œë¥¼ í•´ê²°í–ˆë‹¤.


GKDëŠ” autoregressive LMsì— ëŒ€í•œ KD methodsë¥¼ í†µí•©í–ˆë‹¤. ì„œë¡œ ë‹¤ë¥¸ í¬ê¸°ì˜ T5 ëª¨ë¸ì— on-policy methodsë¥¼ ë„ì…í•´, ì„±ëŠ¥ í–¥ìƒì„ ì´ëŒì—ˆë‹¤. 


**Key contributions**ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.

1. on-policy student-generated ouputsë¥¼ í™œìš©í•˜ëŠ” GKDë¥¼ í†µí•´, auto-regressive LMsì˜ **training-inference ì‹œì˜ mismatchë¥¼ í•´ê²°**í–ˆë‹¤.
2. **RLAIFê°™ì€ LMì—ì„œì˜ RL fine-tuningê³¼ on-policy GKDë¥¼ í˜¼í•©**í•  ìˆ˜ ìˆìŒì„ ì¦ëª…í–ˆë‹¤.
3. Distillation ë„ì¤‘ **student-generated on-policy output sequencesì„ í™œìš©í•˜ëŠ” ê²ƒì˜ ì¤‘ìš”ì„±**ê³¼ ì‘ì—… íŠ¹ì„±ì— ë”°ë¼, **teacher-student ê°„ ìµœì ì˜ Divergenceê°€ ë‹¬ë¼ì ¸ì•¼ í•œë‹¤ëŠ” ì ì— ëŒ€í•œ insightë¥¼ ì œê³µ**í–ˆë‹¤.


### 4. Related works



#### **Knowledge Distillation**

- **supervisedKD,  seqKD[3]**

	ê¸°ì¡´ì˜ KD ë°©ì‹ë“¤ì„. 

- **Hidden states / Attention scores**

	Jiao et al. (2020)[4], Wang et al. (2020)[5]ì˜ ì—°êµ¬ì—ì„œ teacher modelì˜ hidden states / attention scoresë¥¼ ëª¨ë°©í•˜ë„ë¡ teacher modelì„ í›ˆë ¨ì‹œì¼°ìŒ. 


	í•˜ì§€ë§Œ, KD - imitation learning ê°„ì˜ ì—°ê²°ì„±ì„ ì¶©ë¶„íˆ íƒêµ¬í•˜ì§€ ëª»í–ˆìœ¼ë©°, distribution mismatchëŠ” í•´ê²°í•˜ì§€ ëª»í–ˆë‹¤.

- **ImitKD**

	ImitKd(Lin et al., 2020)[6]ì€ Imitation Learning ê°œë…ì„ ë„ì…í–ˆìŒ.


	í•˜ì§€ë§Œ, on-policy data ìˆ˜ì§‘ê³¼ RL fine-tuning ê³¼ì˜ í†µí•©ì— ëŒ€í•œ ê³ ë ¤ëŠ” ì—†ì—ˆìŒ.

- **F-distill**

	F-distill(Wen et al., 2023)[7]ì€ ë‹¤ì–‘í•œ divergenceë¥¼ ë„ì…í–ˆìŒ.


	í•˜ì§€ë§Œ, GKD ë³´ë‹¤ ì„±ëŠ¥ì´ ë‚®ì•˜ë‹¤.

- **MiniLLM**

	MiniLLM(Gu et al., 2023)[8]ì€ Imitation Learning ê°œë…ì„ ë„ì…í–ˆê³ , KD ìì²´ë¥¼ RL ë¬¸ì œë¡œ framing í–ˆìŒ. policy gradientë¥¼ í†µí•´ optimizationì„ ìˆ˜í–‰í–ˆë‹¤.


	í•˜ì§€ë§Œ, êµ‰ì¥íˆ ë³µì¡í–ˆê³ , GKDì— ë¹„í•´ ì„±ëŠ¥ì´ ë‚®ì•˜ë‹¤.



#### **Divergence**


![0](/assets/img/2024-08-23-On-Policy-Distillation-of-Language-Models:-Learning-from-Self-Generated-Mistakes.md/0.png)

- forward KL

	![1](/assets/img/2024-08-23-On-Policy-Distillation-of-Language-Models:-Learning-from-Self-Generated-Mistakes.md/1.png)


	forward KL ì‚¬ìš© ì‹œ, Pê°€ 0ì´ ì•„ë‹Œ ê³³ì—ì„œ, Që„ 0ì´ ì•„ë‹Œ ê°’ì„ ê°€ì§€ë„ë¡ í•™ìŠµëœë‹¤. ë”°ë¼ì„œ, ì—¬ëŸ¬ ëª¨ë“œë“¤ì„ ì „ì²´ì ìœ¼ë¡œ ë„“ê²Œ ì»¤ë²„ë§í•˜ë„ë¡ í•™ìŠµëœë‹¤.

- reverse KL

	![2](/assets/img/2024-08-23-On-Policy-Distillation-of-Language-Models:-Learning-from-Self-Generated-Mistakes.md/2.png)


	reverse KL ì‚¬ìš© ì‹œ, Pê°€ 0ì¸ ê³³ì—ì„œ, Qê°€ 0ì´ ë˜ë„ë¡ í•™ìŠµëœë‹¤. ì£¼ë¡œ Pì˜ ëª¨ë“œ ì¤‘ í•˜ë‚˜ì— ì§‘ì¤‘ëœë‹¤.

- JSD

	![3](/assets/img/2024-08-23-On-Policy-Distillation-of-Language-Models:-Learning-from-Self-Generated-Mistakes.md/3.png)


	$\beta=\frac{1}{2}$ì„ ì‚¬ìš©ì‹œ, symmetricí•˜ë©° boundedí•œ íŠ¹ì„±ì„ ê°–ëŠ”ë‹¤. ë˜í•œ, $\beta$ë¥¼ í†µí•´ forward, reverse KL ì¤‘ ì–´ë””ë¥¼ ë” ë§ì´ ë°˜ì˜í•  ì§€ë¥¼ ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.


**Speculate Decoding**


LLMì—ì„œ í…ìŠ¤íŠ¸ ìƒì„± ì‹œì˜ inference ì†ë„ë¥¼ í–¥ìƒì‹œí‚¤ëŠ” ê¸°ìˆ ë¡œ, small draft modelê³¼ big target modelì„ ì´ìš©í•´ ê³¼ì •ì„ ìµœì í™”í•œë‹¤. ìµœê·¼ì˜ ë‘ ì—°êµ¬ Zhou et al. (2023)[9]ê³¼ Liu et al. (2023)[10]ì—ì„œ GKDë¥¼ ì´ìš©í•´ ë‘ ëª¨ë¸ ê°„ì˜ alignmentë¥¼ ê°œì„ í–ˆë‹¤.



### 5. Method


**Problem Setup**


ë‘ ê°œì˜ auto-regressive sequence models,  student $p_S$, teacher $p_T$


studentê°€ learnable parameters $\theta$ë¥¼ ê°–ê³ , $p_s^{\theta}$ê°€ $\theta$ì— ëŒ€í•´ ë¯¸ë¶„ê°€ëŠ¥í•˜ë‹¤.


inputs $X$ê°€ ì£¼ì–´ì¡Œê³ , ê·¸ì— ëŒ€í•œ datasetìœ¼ë¡œ input-output sequence pairs $(X,Y)$ê°€ ìˆë‹¤.


(ë˜ëŠ” Teacher modelì˜ generated sequenceë¥¼ $Y$ë¡œ í™œìš©í•  ìˆ˜ ìˆë‹¤.)


divergence $\mathcal{D}$ë¡œ $p_T$ì™€ $p_S$ì˜ token-level distributions ì‚¬ì´ì˜ ì°¨ì´ë¥¼ ì •ì˜í•œë‹¤.


![4](/assets/img/2024-08-23-On-Policy-Distillation-of-Language-Models:-Learning-from-Self-Generated-Mistakes.md/4.png)


**Supervised FT**


fixed-datasetì´ ìˆê³  teacherì˜ feedbackì´ ì—†ë‹¤ê³  í•  ë•Œ, ê°€ì¥ ê°„ë‹¨í•œ ë°©ì‹ì€ student policyì—ì„œ negative log-likelihoodë¥¼ ìµœì†Œí™”í•˜ëŠ” ê²ƒì´ë‹¤.


![5](/assets/img/2024-08-23-On-Policy-Distillation-of-Language-Models:-Learning-from-Self-Generated-Mistakes.md/5.png)


**Sequence-Level KD**


SeqKD(Kim & Rush, 2016)ì€ teacherì— ì˜í•´ ìƒì„±ëœ high probability sequencesì˜ likelihoodì„ maximizeí•˜ëŠ” ë°©ë²•ìœ¼ë¡œ ì´ë¤„ì§„ë‹¤. ì´ëŠ” teacher-generated outputsì— ëŒ€í•œ supervised FTë¡œ ë³¼ ìˆ˜ ìˆë‹¤.


**Supervised KD**


Studentê°€ teacherì˜ token-level probability distributionsì„ ëª¨ë°©í•˜ëŠ” ë°©ë²•ìœ¼ë¡œ ì´ë¤„ì§„ë‹¤. $p_S$ëŠ” supervised objective $L_{SD}$ë¡œ í›ˆë ¨ëœë‹¤.


![6](/assets/img/2024-08-23-On-Policy-Distillation-of-Language-Models:-Learning-from-Self-Generated-Mistakes.md/6.png)



#### 5.1 Generalized Knowledge Distillation (GKD)


Imitation Learning(IL) ì¤‘ on-policy imitation approaches(DAgger)ì„ í†µí•´ **student policyë¥¼ í†µí•´ sequenceë¥¼ ë°˜ë³µì ìœ¼ë¡œ ìˆ˜ì§‘**í•˜ê³ , í•´ë‹¹ **sequenceì— ëŒ€í•œ expert labels**ì„ ì–»ê³ , **í•´ë‹¹ datasetì—ì„œ studentë¥¼ train**ì‹œí‚¤ëŠ” ë°©ë²•ì„ ì œì•ˆí•œë‹¤.


**Studentì˜** **self-generated output sequencesì— ëŒ€í•œ erroneous tokens**ì— ëŒ€í•´ **teacherì˜ logitìœ¼ë¡œ ë¶€í„° token-specific feedbackì„ ë°›ëŠ” ì´ ë°©ë²•**ì„ **on-policy KD** ë¼ê³  í•œë‹¤. studentëŠ” $y_{<n}$ **ìƒíƒœì—ì„œ, teacherì˜ token-level distributionsì¸**  $p_T(y_n\|x)$**ì„ ëª¨ë°©**í•œë‹¤. on-policy lossì¸ $\mathcal{L}_{OD}$ëŠ” ì•„ë˜ì™€ ê°™ì´ ì •ì˜ëœë‹¤.


![7](/assets/img/2024-08-23-On-Policy-Distillation-of-Language-Models:-Learning-from-Self-Generated-Mistakes.md/7.png)


í•´ë‹¹ ë°©ë²• ìˆ˜í–‰ ì‹œ, teacherì˜ feedbackì„ ë°›ê¸° ë•Œë¬¸ì—, studentâ€™s sampling distributionì¸ $p_S(\cdot\|x)$ì—ì„œì˜ backpropagationì´ ì¼ì–´ë‚˜ì§€ ì•ŠëŠ”ë‹¤. ì´ëŠ” trainingì„ ì•ˆì •ì ìœ¼ë¡œ ë§Œë“¤ê³ , computationally efficientí•˜ë‹¤. trainingì€ temperature $\gamma=1$ë¡œ ë‹¤ì–‘í•œ sequenceë¥¼ generateí•˜ë„ë¡ í•œë‹¤. ë˜í•œ, studentë¥¼ ì´ìš©í•´ sequenceë¥¼ generateí•˜ëŠ” ê²ƒì€ teacherë¥¼ ì´ìš©í•˜ëŠ” ê²ƒì— ë¹„í•´ costë„ ì ê²Œ ë“ ë‹¤.


on-policy KDì— ë”í•´ supervised approachì™€ on-policy approachë¥¼ í†µí•©í–ˆê³ , ì´ë¥¼ Generalized KD (GKD)ë¼ê³  í•œë‹¤. ë”°ë¼ì„œ, GKDëŠ” output-sequenceë¡œ fixed datasetê³¼ on-policy student-generated sequencesë¥¼ ë‘˜ ë‹¤ ì‚¬ìš©í•œë‹¤. ë”°ë¼ì„œ, GKDëŠ” ë‹¤ìŒ objectiveë¥¼ ìµœì†Œí™”í•œë‹¤.


![8](/assets/img/2024-08-23-On-Policy-Distillation-of-Language-Models:-Learning-from-Self-Generated-Mistakes.md/8.png)


$\mathcal{D}(p_T,p_S)(y\|x)$ì€ teacher-student distributionsê°„ì˜ divergenceì´ê³ , $\lambda\in[0,1]$ì€ student data fractionì„ ì¡°ì ˆí•˜ëŠ” hyper-parameterì´ë‹¤. $\lambda$ ê°’ì— ë”°ë¥¸ GKDì˜ ê²°ê³¼ëŠ” ì•ìœ¼ë¡œ í™•ì¸í•´ ë³¼ ê²ƒì´ë‹¤.


![9](/assets/img/2024-08-23-On-Policy-Distillation-of-Language-Models:-Learning-from-Self-Generated-Mistakes.md/9.png)


**Remark**


**í•´ë‹¹ ê³¼ì •ì€ RLHFì™€ ìƒë‹¹íˆ ìœ ì‚¬í•˜ë‹¤. íŠ¹íˆ,** Student modelì´ Supervised Fine-Tuningì„ í†µí•´ ì–´ëŠì •ë„ í•™ìŠµëœ ìƒíƒœ ê°€ì •í•œë‹¤ë©´, **1) SFT**ë¡œ ì‹œì‘í•´ì„œ, **2) Expertì˜ feedback**ì„ ë°›ëŠ” ì¼ë ¨ì˜ ê³¼ì •ì´ RLHFì™€ ìœ ì‚¬í•˜ë‹¤. GKDëŠ” RLHFì—ì„œì˜ hyperparameter tuning insightë¥¼ í™œìš©í•´, ì¶”ê°€ì ì¸ hyperparameter ì—†ì´ ì ì€ overhead ë§Œìœ¼ë¡œ **ì‰½ê²Œ RLHFì™€ í†µí•©**ë  ìˆ˜ ìˆë‹¤.



#### 5.2 RL Fine-tuning + On-policy GKD


distillationì€ ì£¼ìš” objectiveë¥¼ ì§ì ‘ì ìœ¼ë¡œ ìµœì í™”í•˜ëŠ” ê²ƒì´ ì•„ë‹Œ, ëŒ€ë¦¬ì ì¸ ë°©ë²•ìœ¼ë¡œ ì´ìš©ë  ìˆ˜ ìˆìœ¼ë©°, ë¯¸ë¶„ë¶ˆê°€ëŠ¥í•œ ê²½ìš°ë„ ìˆë‹¤. ìš°ë¦¬ëŠ” ì´ objectiveë¥¼ RLì„ í†µí•´ optimizeí•  ìˆ˜ ìˆë‹¤. íŠ¹íˆ, on-policy KDì˜ ê²½ìš°ëŠ” studentì˜ outputë§Œì„ ìš”êµ¬í•˜ë¯€ë¡œ, RLHF ë“±ê³¼ ì‰½ê²Œ ê²°í•©ë  ìˆ˜ ìˆë‹¤. student policyì— ëŒ€í•œ reward $r$ì„ teacher policyì— ê°€ê¹ë„ë¡ ìœ ì§€í•˜ê²Œ optimize í•˜ë©´ ëœë‹¤. 


![10](/assets/img/2024-08-23-On-Policy-Distillation-of-Language-Models:-Learning-from-Self-Generated-Mistakes.md/10.png)


$\alpha\in[0,1]$ì„ í†µí•´ RL objectiveì— ëŒ€í•œ distillation ê°•ë„ë¥¼ ì¡°ì ˆí•  ìˆ˜ ìˆë‹¤. ì´ë¥¼ í†µí•´, human preferenceì— model alignmentí•˜ëŠ” alignment taxë¥¼ ì¤„ì¼ ìˆ˜ ìˆë‹¤. ì‹¤í—˜ì„ í†µí•´ RLAIFì™€ì˜ í†µí•©ìœ¼ë¡œ hallucinationì„ ì¤„ì¼ ìˆ˜ ìˆì—ˆê³ , ë™ì‹œì— distillationì„ í†µí•œ downstream performanceë¥¼ ì¦ëŒ€í•  ìˆ˜ ìˆì—ˆë‹¤.


![11](/assets/img/2024-08-23-On-Policy-Distillation-of-Language-Models:-Learning-from-Self-Generated-Mistakes.md/11.png)


**Remark**


RLHF[11], RLAIF[12]ì„ ì‚¬ìš©í•  ë•Œ, reverseKLì„ ì´ìš©í•´ ì •ì±…ì´ ë„ˆë¬´ ë³€í™”í•˜ì§€ ì•Šë„ë¡ ì œì•½ì„ ê±¸ì–´ì£¼ëŠ” ì—­í• ì„ í•œë‹¤. ReverseKL, JSD(0.9)ì„ ì‚¬ìš©í•´ ì„¸ë°€í•œ ì¡°ì •ì„ í•  ìˆ˜ ìˆë‹¤.



#### 6. Result


**Setup**

- Teacher Models: T5-XL (~ 3B)
- Student Models: T5-small (77M), T5-base (250M), T5-large (800M)
- Divergence: forward KL, reverse KL, JSD(0.1), JSD(0.5), JSD(0.9)
- Data fraction ğœ†: ğœ†=1 (On-Policy), ğœ†=0.5 (Mixed), ğœ†=0 (Supervised)
- Baseline models: SeqKD, SupervisedKD, ImitKD, f-distill

**Abstractive Summarization**


Dataset: Xsum dataset
Metric: ROUGE-2, ROUGE-L, ROUGE-1


![12](/assets/img/2024-08-23-On-Policy-Distillation-of-Language-Models:-Learning-from-Self-Generated-Mistakes.md/12.png)


GKDê°€ ê¸°ì¡´ baseline ë³´ë‹¤ ì„±ëŠ¥ ì¦ëŒ€í–ˆìœ¼ë©°, ì ì€ í•™ìŠµ ë°ì´í„°ì…‹ì—ì„œë„ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì˜€ê³ , ë°ì´í„° ì¦ê°€ì— ë”°ë¥¸ ì„±ëŠ¥ì˜ í–¥ìƒë„ ì¢‹ì•˜ìŒ.


Dataset: Xsum dataset
Metric: Self-BLEU


![13](/assets/img/2024-08-23-On-Policy-Distillation-of-Language-Models:-Learning-from-Self-Generated-Mistakes.md/13.png)


JSD(0.9), ReverseKLì˜ ê²½ìš°ëŠ” íŠ¹íˆ, Self-BLEUê°€ ë‹¤ë¥¸ divergenceì— ë¹„í•´ ë†’ì•„, diversityê°€ ê°ì†Œí•˜ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆìŒ.


Dataset: Xsum dataset
Metric: ROUGE-2, Entailment


![14](/assets/img/2024-08-23-On-Policy-Distillation-of-Language-Models:-Learning-from-Self-Generated-Mistakes.md/14.png)


GKD + RLHF ì—ì„œ Î±ê°€ ì»¤ì§ˆìˆ˜ë¡, **ìš”ì•½ì˜ í’ˆì§ˆ(ROUGE-2)ì€ ì˜¬ë¼ê°€ë‚˜**, **ì‚¬ì‹¤ì  ì¼ê´€ì„±(Entailment)ì€ ê°ì†Œ**


**Machine Translation**


Dataset: WMT14 en-de dataset 
Metric: BLEU
Method: Beam searchë¥¼ í†µí•´ ì–»ì€ 3ê°œì˜ ê²°ê³¼ë¥¼ í‰ê· ëƒ„


![15](/assets/img/2024-08-23-On-Policy-Distillation-of-Language-Models:-Learning-from-Self-Generated-Mistakes.md/15.png)


![16](/assets/img/2024-08-23-On-Policy-Distillation-of-Language-Models:-Learning-from-Self-Generated-Mistakes.md/16.png)


GKD (On-policy) ë°©ë²•ì˜ ì„±ëŠ¥ì´ ì œì¼ ì¢‹ì•˜ë‹¤.


Dataset: WMT14 en-de dataset
Metric: BLEU


![17](/assets/img/2024-08-23-On-Policy-Distillation-of-Language-Models:-Learning-from-Self-Generated-Mistakes.md/17.png)


Machine translationì˜ ê²½ìš°ëŠ” Forward KL, Reverse KLì— ë¹„í•´ **JSDì˜ ê²½ìš°ê°€ ì„±ëŠ¥ í–¥ìƒ í­ì´ ì»¸ìŒ**


**Arithmetic Reasoning**


Dataset: GSM8K dataset
Metric: Accuracy
Setup: Wei et al. (2022)ì—ì„œì˜ CoT ì˜ˆì œ 4ê°œ ì¶”ê°€í•´, few-shot promptingìœ¼ë¡œ ë¬¸ì œë¥¼ í’€ë„ë¡ í•¨
Teacher: Flan T5-XL   Student: Flan T5-Base, Small


![18](/assets/img/2024-08-23-On-Policy-Distillation-of-Language-Models:-Learning-from-Self-Generated-Mistakes.md/18.png)


Arithmetic Reasoningì˜ ê²½ìš°ëŠ” **Forward KL, Reverse KLì´** JSDì— ë¹„í•´ **ì„±ëŠ¥ì´ ì¢‹ì•˜ìŒ**


**Task Agnostic Distillation**


Train Dataset: FLAN2021    
Test Dataset: MMLU, BBHMetric: Few-Shot Prompted Accuracy
Teacher: Flan T5-XL   
Student: FLAN T5-Base
Train: Instruction â€“ Answer pairì„ ì´ìš©í•´ í•™ìŠµ ìˆ˜í–‰


![19](/assets/img/2024-08-23-On-Policy-Distillation-of-Language-Models:-Learning-from-Self-Generated-Mistakes.md/19.png)


|            | **MMLU** | **BBH** |
| ---------- | -------- | ------- |
| FLAN-T5 XL | 52.4%    | 41%     |
| T5-Large   | 35.6%    | 31.25%  |

undefined
On Policy with Reverse KLì´ ê°€ì¥ ì„±ëŠ¥ì´ ì¢‹ì•˜ìŒ



#### 8. Conclusion


Auto-regressive modelì—ì„œì˜ train-test distribution mismatchë¥¼ ìœ„í•´ on-policy GKDì„ ê³ ì•ˆí–ˆë‹¤.


1) teacherì˜ token-level guideë¥¼ ì´ìš©í•œ on-policy student-generated outputs ì´ìš© distillation ê³ ì•ˆí–ˆê³ , 2) LMì˜ RL í•™ìŠµ(ex. RLAIF)ê³¼ ì—°ê²°ë  ìˆ˜ ìˆëŠ” GKDë¥¼ ì œì‹œí–ˆìœ¼ë©°, 3) Distillationì—ì„œ student generated on-policy output sequenceì˜ ì¤‘ìš”ì„±ì„ í™•ì¸í–ˆìœ¼ë©°, 4) student-teacher ê°„ì˜ ìµœì ì˜ divergenceê°€ task-dependent í•˜ë‹¤ëŠ” í†µì°°ì„ ì œê³µí–ˆë‹¤. 


ë…¼ë¬¸ì—ì„  ë‹¤ë¥¸ auto-regressive models (audio, video, text-to-image)ë¡œì˜ ì—°êµ¬ì— ì ìš©í•´ë³´ëŠ” ê²ƒì„ ì œì•ˆí–ˆë‹¤. 



#### 9. Reference


[1] _Distilling the Knowledge in a Neural Network_ (Hinton et al., 2014)


[2] _A reduction of imitation learning and structured prediction to no-regret online learning_ (Ross et al., 2011)


[3] _Sequence-level knowledge distillation_ (Kim & Rush, 2016)


[4] _Tinybert: Distilling bert for natural language understanding_ (Jiao et al., 2020)


[5] _Minillm: Deep self-attention distillation for task-agnostic compression of pre-trained transformers_ (Wang et al., 2020)


[6] _Autoregressive knowledge distillation through imitation learning_ (Lin et al., 2020)


[7] _f-divergence minimization for sequence-level knowledge distillation._(Wen et al., 2023)


[8] _Knowledge distillation of large language models_ (Gu et al., 2023)


[9] _Distillspec: Improving speculative decoding via knowledge distillation_ (Zhou et al., 2023)


[10] _Online speculative decoding_ (Liu et al., 2023)


[11] _Training language models to follow instructions with human feedback_ (Ouyang et al., 2022)


[12] _RLAIF: Scaling Reinforcement Learning from Human Feedback with AI Feedback_ (Lee et al., 2023)

