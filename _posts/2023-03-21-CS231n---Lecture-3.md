---
layout: single
date: 2023-03-21
title: "CS231n - Lecture 3"
use_math: true
tags: [강의/책 정리, ]
categories: [AI, ]
---


### Linear Classifier


![0](/assets/img/2023-03-21-CS231n---Lecture-3.md/0.png)

- 가중치(Weight) W를 입력으로 받아 각 스코어를 확인하고 이 W가 얼마나 안 좋은지 정량적으로 말해주는 손실함수(Loss Function)을 정의해야 한다
- 손실함수(Loss Function)를 최소화하는 파라미터를 효율적으로 찾는 방법을 제시해야 한다
- 가중치(Weight) W 값을 랜덤으로 설정한 뒤 고양이, 자동차, 개구리 이미지에서 얻은 결과이다
- 이중에 가장 큰 값으로 예측하므로 예측 결과는 다음과 같습니다.
	1. 고양이 사진 = 개(dog: 8.02) : `Wrong`
	2. 자동차 사진 = 자동차(automobile: 6.04) : `Correct`
	3. 개구리 사진 = 트럭(truck: 6.14) : `Wrong`


### Loss Function


![1](/assets/img/2023-03-21-CS231n---Lecture-3.md/1.png)

- 현재 사용하고 있는 이미지 분류기가 얼마 만큼의 손실을 가지고 있는지를 나타내는 함수이다
- Train 이미지 데이터 `X`와 Target `y`가 있고, 어떤 Loss Function을 $L_i$라고 하자
- 최종 Loss인 $L$은 데이터 셋에서 각 N개의 샘플들의 Loss 평균이 된다


### Multiclass SVM Loss


![2](/assets/img/2023-03-21-CS231n---Lecture-3.md/2.png)


![3](/assets/img/2023-03-21-CS231n---Lecture-3.md/3.png)


![4](/assets/img/2023-03-21-CS231n---Lecture-3.md/4.png)

- Linear Classification인 $f
(x_i, W)$에서 나온 결과를 $s$라고 하자
- 정답인 클래스의  $s$ 값을 $s_{y_{i}}$라고 하고 나머지 클래스의 값과 비교하자
- 정답인 클래스와 나머지를 비교했을때, 정답보다 다른 클래스의 점수가 더 높다면 이 차이 만큼이 Loss 라고 정한다
- 이 Loss 에서 **safety margin** 이라는 값을 추가합시다. 이는 정답 클래스가 적어도 다른 클래스보다 **safety margin** 값 만큼은 커야 한다는 이야기이며, 예시에서는 `safety margin = 1` 이다
- 이 Loss 값이 0보다 작은 음수 값인 경우에는 포함하지 않는다
- **Hinge Loss**라고 불리는 그래프의 가로축은 $s_{y_{i}}$값, 세로축은  $L_i$의 값인 Loss 값을 표현하였습니다.


{% raw %}
```python
def L_i_vectorized(x, y, W):
	scores = W.dot(x)
	margins = np.maximum(0,scores - scores[y] + 1)
	margins[y] = 0
	loss_i = np.sum(margins)
	return loss_i
```
{% endraw %}




#### Example 1.


![5](/assets/img/2023-03-21-CS231n---Lecture-3.md/5.png)

- Loss(`cat`)
	- Loss(class: `cat`) = **0**
	- Loss(class: `car`) = max(0, 5.1 - 3.2 + 1) = **2.9**
	- Loss(class: `frog`) = max(0, -1.7 - 3.2 + 1) = **0**
	- 고양이 사진의 전체 **Loss = 0 + 2.9 + 0 = 2.9**

		![6](/assets/img/2023-03-21-CS231n---Lecture-3.md/6.png)

- Loss(`car`)
	- Loss(class: `cat`) = max(0, 1.3 - 4.9 + 1) = **0**
	- Loss(class: `car`) = **0**
	- Loss(class: `frog`) = max(0, 2.0 - 4.9 + 1) = **0**
	- 고양이 사진의 전체 **Loss = 0 + 0 + 0 = 0**

		![7](/assets/img/2023-03-21-CS231n---Lecture-3.md/7.png)

- Loss(`frog`)
	- Loss(class: `cat`) = max(0, 2.2 - (-3.1) + 1) = **6.3**
	- Loss(class: `car`) = max(0, 2.5 - (-3.1) + 1) = **6.6**
	- Loss(class: `frog`) = **0**
	- 고양이 사진의 전체 **Loss = 6.3 + 6.6 + 0 = 0**

		![8](/assets/img/2023-03-21-CS231n---Lecture-3.md/8.png)

- Loss
	- Loss(`cat`) = **2.9**
	- Loss(`car`) = **0**
	- Loss(`frog`) = **12.9**

	![9](/assets/img/2023-03-21-CS231n---Lecture-3.md/9.png)



#### Multiclass SVM Loss의 성질

- 정답 스코어가 `safety margin = 1`값을 만족하는 범위라면 loss값에 변화를 주지 않는다
- Loss의 **최솟값은 0** 이며, **최대값은** $\infty$이다
- 모든 S 값이 0이라고 하면 `loss = #Class - 1`값을 가집니다. 이러한 특성은 loss function의 작동을 **Debug Strategy**로 적절하다
- Loss에 자기 클래스까지 더해주면 Loss의 최소값이 1이 되며, 가중치(Weight) W를 최적화하는데 상관은 없지만 관례상 적절하진 않다
- 클래스 수는 정해져있고, 평균을 취한다는건 그저 Loss를 리스케일하는 것이다
- Loss Function으로 $L_{i}=\sum{max(0,\  s_j -s_{y_{i}}+margin)^2}$을 사용하면 Loss의 값도 바뀌고, 실제로 **Squared Hinge Loss**라고 자주 사용한다. 이것은 ‘조금 잘못된 것’과 ‘많이 잘못된 것’ 사이에 가중치를 줄 수 있는 방법이다
- 만약 Loss 값이 0이라고 할때, `W: Weight Parameter` 값에 임의에 상수를 곱해도 Loss값은 0이다

---



### Regularization


![10](/assets/img/2023-03-21-CS231n---Lecture-3.md/10.png)

- Loss가 줄어든다는 것은 `Train` 데이터에 대해서 좋은 성능을 가진다는 것이지, `Test` 데이터에 대해서 좋은 성능을 가질지는 알 수 없다
- `train` 데이터에 대해서는 좋은 성능을 가지도록 학습되지만, `test`데이터에 대해서는 오히려 성능이 떨어지는 현상인 **Overfitting(과적합)**이라 한다
- Loss Function에 **Regularization** 수식을 추가하여 가중치(Weight) W 값에 제약을 걸어서 더 단순한 W 를 선택하도록 돕고, 그에 따라 Overfitting(과적합)을 해결할 수 있다
- **Occam’s Razor(오컴의 면도날)** : 어떤 현상을 설명 가능한 다양한 가설이 있다면, `‘더 단순한 것’`을 선호해야 한다


#### Regularization 종류

- **L2 Regularization**

	$R(W) = \sum_k\sum_lW_{k,l}^2$

- L1 Regularization

	$R(W) = \sum_k\sum_l|W_{k,l}|$

- Elastic Net (L1 + L2)

	$R(W) = \sum_k\sum_l\beta W_{k,l}^2+|W_{k,l}|$

- Max norm Regularization
- Dropout


#### Example 1.  


$x=[1, 1, 1, 1]$,  $w_1=[1, 0, 0, 0]$,  $w_2=[0.25, 0.25, 0.25, 0.25]$

- `L1 regularization`
	- weight 값이 0으로 수렴하는 것이 많은 형태이다. 이를 Sparse matrix(희소 행렬)이라 부른다
	- 위에서 0의 값이 많다는 이야기는 어떤 특징들은 무시하겠다는 이야기로 볼 수 있다
	- $w_1^T \cdot x=1$, $w_2^T \cdot x=1$로 같은 Loss 값을 가진다
- `L2 regularization`
	- weight의 값이 큰 값은 점점 줄이며 대부분의 값들이 0의 가까운 값을 가지는 가우시안 분포를 가진다
	- weight이 0이 아니라는 점에서 모든 특징들을 무시하지 않고 조금씩은 참고 하겠다라고 볼 수 있다

		![11](/assets/img/2023-03-21-CS231n---Lecture-3.md/11.png)


		⇒ 다른 Loss 값을 가진다


---



### Softmax Classifier (Multinomial Logistic Regression)


![12](/assets/img/2023-03-21-CS231n---Lecture-3.md/12.png)

- Multiclass SVM Loss와는 다르게 **스코어 자체에 추가적인 의미**를 부여하고  Loss 값을 결정한다
1. Score 값에 `Softmax (Exponetial)`를 취한다
2. 이 값의 `확률 분포 값`을 구한다
3. 앞서 구한 값에 `-log` 를 취하여 Target Class에 대해서 더해준다

	→ log값을 취하면 점수가 높을수록 작은 값을 가진다. 하지만 Loss는 예측값이 얼마나 잘못되었늕지는 확인하는 작업으로 -log를 사용한다


![13](/assets/img/2023-03-21-CS231n---Lecture-3.md/13.png)



#### Softmax Classifier의 성질

- Loss의 **최솟값은** $-log(1)=0$ 이며, **최대값은** $-log(0)=\infty$이다
- 모든 S 값이 0이라고 하면 `loss = log(C)`값을 가집니다. 이러한 특성은 loss function의 작동을 **Debug Strategy**로 적절하다


### SVM vs Softmax


![14](/assets/img/2023-03-21-CS231n---Lecture-3.md/14.png)

- SVM
	- 일정 Margins를 넘기만 하면 더 이상 성능 개선을 하지 않는다

	$L_i=\sum_{j\neq y_i}max(0, s_j-s_{y_i}+1)$

- Softmax
	- Normalize를 하여 계속 성능을 개선하려고 한다

	$L_i=-log \frac {e^{s_{y_i}}} {\sum_j e^{s_j}} $



### Optimization

- 가중치(Weight) W 값이 얼마나 잘못되어 있는지에 대해서 SVM과 Softmax와 같은 Loss Funtion으로 확인하면 된다
- 가중치(Weight) W를 더 좋은 방향으로 가도록 하는 과정이 Optimization이다


#### Strategy #1 : Random Search



{% raw %}
```python
# assume X_train is the data where each column is an example (e.g. 3,073 x 50,000)
# assume Y_train are the labels (e.g. 1D array of 50,000)
# assume the function L evaluates the loss function

bestloss = float("inf") # Python assigns the highest possible float value
for num in xrange(1000):
	W = np.random.randn(10, 3073) * 0.0001 # generate random parameters
	loss = L(X_train, Y_train, W) # get the loss over the entire training set
	if loss < bestloss: # keep track of the best solution
		bestloss = loss
		bestW = W
	print 'in attempt %d the loss was %f, best %f' % (num, loss, bestloss)

# prints:
# in attempt 0 the loss was 9.401632, best 9.401632
# in attempt 1 the loss was 8.959668, best 8.959668
# in attempt 2 the loss was 9.044034, best 8.959668
# in attempt 3 the loss was 9.278948, best 8.959668
# in attempt 4 the loss was 8.605604, best 8.605604
# ... (trunctated : continues for 1000 lines)
```
{% endraw %}




{% raw %}
```python
# assume X_test is [3073 x 10000], Y_test[10000 x 1]
scores = Wbest.dot(Xte_cols) # 10 x 10000, the class scores for all test examples
Yte_predict = np.argmax(scores, axis = 0)
# and calculate accuracy (fraction of predictions that are correct)
np.mean(Yte_predict == Yte)
# returns 0.1555
```
{% endraw %}


- 아무런 기준없이 W 값을 변경해보며 가장 좋은 성능을 가지는 Weight를 찾는 것이다
- CIFAR-10에서 클래스가 10개니까 임의 확률은 10%이고, Random Search를 하면 약 15.5%의 성능이 나온다
- 현대 최고 성능인 SOTA가 95%인 것과 비교하면 좋은 방법이 아니다


#### Strategy #2 : Follow the slope


$$
\frac{df(x)}{dx}= \lim_{h \to 0} \frac{f(x+h)-f(x)}{h} 
$$

- 지금 위치에서 주변의 좀 더 낮은 곳으로 움직이고, 그 위치에서 다시 한번 주변의 좀 더 낮은 곳으로 움직이는 것을 반복하여 좋은 성능을 가지는 Weight를 찾는 것이다
- Example 1.

	![15](/assets/img/2023-03-21-CS231n---Lecture-3.md/15.png)

	- $f(x+h)$를 구하기 위해서 `h의 값을 0.0001`이라고 가정하자
	- 이렇게 구한 Loss 값의 차이를 h로 나눈 값이 `gradiend dW`이다
	- `Numerical Gradient` 방식으로 모든 W에 대해서 이를 반복하면 모든 gradient dW 값을 구할 수 있다
	- 하지만 모든 W 성분에 대해서 계산되어야 하기 때문에 굉장히 느리다는 단점이 있다
- Example 2.

	![16](/assets/img/2023-03-21-CS231n---Lecture-3.md/16.png)


	$$
	\begin{align} &L= \frac1N \sum_{i=1}^{N}L_i+\sum_k W_k^2 \\ &L_i=\sum_{j \neq y_i}max(0-, s_j - s_{y_i}+1) \\ &s =f(x;W)=Wx   \end{align} \tag{}
	$$

	- 뉴턴과 라이프니치가 만든 수학적으로 미분을 간단하게 할 수 있는 `Analytic Gradient` 방식을 이용하자
- `Numerical gradient`
	- 근사값을 구함, 느리다, 코드짜기 쉽다.
- `Anaytic gradient`
	- 정확한 값을 구함, 빠르다, 코드짜기 어렵다.


### Gradient Descent



{% raw %}
```python
# Vanilla Gradient Descent

while True:
	weights_grad = evaluate_gradient(loss_fun, data, weights)
	weights += - step_size * weights_grad # perform parameter update
```
{% endraw %}


- `Gradient`가 올라가는 방향이 아니라 내려가는 방향을 찾아야 하기 때문에 `-` 값을 곱하고, 여기서 `step_size(learning rate)`라는 새로운 `Hyperparameter`를 사용한다.
- `step_size(learning rate)`란 산을 내려가는 사람으로 비유하자면 `걸음의 폭`이라고 생각하시면 된다. 방향을 정하고 1m를 갈지, 10m를 갈지를 정하는 것이라고 생각하시면 된다
- 이 값을 너무 작게 주면 학습이 너무 느리게 된다는 단점이 있으며, 너무 큰 값을 주게 되면 제대로 수렴하지 못하게 됩니다.
- 이러한 점에서 이 `step_size(learning rate)`값을 잘 정해주어야 합니다.


### Stochastic Gradient Descent (SGD)



{% raw %}
```python
# Vanilla Minibatch Gradient Descent

while True:
	data_batch = sample_training_data(data, 256) # sample 256 examples
	weights_grad = evaluate_gradient(loss_fun, data_batch, weights)
	weights += - step_size * weights_grad # perform parameter update
```
{% endraw %}


- `Gradient Descent`를 구하기 위해서는 전체 Train 데이터셋 Loss의 평균을 Loss Function으로 이용하는데, 실제로 N이 엄청 커진다면 시간이 매우 오래 걸린다
- `Stochastic Gradient Descent (SGD)`는 전체 데이터 셋의 Gradient를 구하는 것이 아니라 Minibatch라는 작은 트레이닝 Sample로 나눠서 학습하는 방식이다.
- CPU와 GPU의 메모리가 2의 승수로 이뤄져 있어 보통 2의 승수로 정하며 32, 64, 128로 쓰는 편이다.

[bookmark](http://vision.stanford.edu/teaching/cs231n-demos/linear-classify/)



### Image Feature

- Linear Classification은 이미지에서는 좋은 방법이 아니라 말 머리가 2개 나오는 등 성능이 좋지 않다
- DNN이 유행하기 전까지는 Linear Classification을 이용하기 위해서 Two-Stage Approach를 사용했다

![17](/assets/img/2023-03-21-CS231n---Lecture-3.md/17.png)

- 이미지의 모양, 컬러 히스토그램, Edge 형태와 같은 특징 표현을 연결한 특징 벡터를 계산한다

![18](/assets/img/2023-03-21-CS231n---Lecture-3.md/18.png)

- 이 특징 벡터를  Linear Classifier의 입력값으로 사용한다


#### Color Histogram


![19](/assets/img/2023-03-21-CS231n---Lecture-3.md/19.png)

- Color의 Hue 값만 뽑아서 모든 픽셀을 양동이에 넣고 각 양동이에 담긴 픽셀의 갯수를 세서 특징을 추출하는 것이다
- 개구리의 경우 초록색이 많은 것을 알 수 있고, 이것을 간단한 특징 벡터라고 한다


#### Histogram of Oriented Gradient (HoG)


![20](/assets/img/2023-03-21-CS231n---Lecture-3.md/20.png)

- 이미지를 `8*8`픽셀로 나눠서 각 픽셀의 지배적인 edge 방향을 계산하고 각 edge들에 대해서 양동이에 넣는 것이
- 잎의 경우 대각 Edge가 있다는 것을 알 수 있고, 이것을 간단한 특징 벡터라고 한


#### Bag of Words


![21](/assets/img/2023-03-21-CS231n---Lecture-3.md/21.png)

- 이 방법은 NLP에서 영감을 받은 방식으로, 어떤 문장에서 `여러 단어의 발생 빈도`를 세서 양동이에 넣는 것이다.
- 하지만 이 방식을 그대로 이미지에 적용하는 것은 쉬운 일이 아니었고, `Visual Words` 라는 것을 만들었다
- 이미지들을 임의대로 조각내고, 그 조각들을  K-means와 같은 알고리즘으로 군집화한다
- 이미지 내에서 다양하게 구성된 각 군집들은 다양한 색과 다양한 방향에 대한 Edge도 포착할 수 있다


### ConvNets


![22](/assets/img/2023-03-21-CS231n---Lecture-3.md/22.png)

- 이제는 특징을 뽑아내서 사용하는 것이 아니라 입력된 이미지에서 스스로 특징을 뽑아내도록 사용하는 CNN을 사용한다
