---
layout: single
date: 2023-07-03
title: "CS224n - Lecture 3 (Backprop and Neural Networks)"
use_math: true
tags: [ê°•ì˜/ì±… ì •ë¦¬, ]
categories: [AI, ]
---


### Named Entity Recognition (NER)


í•œê¸€ë¡œ **ê°œì²´ëª… ì¸ì‹**ì´ë¼ê³  í•©ë‹ˆë‹¤.


ì–´ë–¤ ì´ë¦„ì„ ì˜ë¯¸í•˜ëŠ” ë‹¨ì–´ë¥¼ ë³´ê³ ëŠ” ê·¸ **ë‹¨ì–´ê°€ ì–´ë–¤ ìœ í˜•ì¸ì§€ë¥¼ ì¸ì‹**í•˜ëŠ” ê²ƒ


(neural networkì˜ ì˜ˆì‹œë¥¼ ìœ„í•´ ë‚˜ì˜¨ taskë¡œ ë³´ì„)


![0](/assets/img/2023-07-03-CS224n---Lecture-3-(Backprop-and-Neural-Networks).md/0.png)_ë¶„í™ìƒ‰ ê¸€ì”¨ë¡œ ì“°ì¸ ê²ƒì€ ê·¸ ë‹¨ì–´ì˜ ìœ í˜•ì„ ë‚˜íƒ€ë‚¸ ê²ƒ_


PER ì‚¬ëŒì´ë¦„, LOC ìœ„ì¹˜, DATE ì‹œê°„ ë“±ìœ¼ë¡œ ë¶„ë¥˜ê°€ ë˜ì–´ ìˆëŠ”ë°,


ëˆˆì— ë„ëŠ” ê²ƒì€ **Paris**ì…ë‹ˆë‹¤. 


ìœ„ì˜ ì˜ˆì‹œì—ì„œ ParisëŠ” ë‘ ê°€ì§€ë¡œ ë¶„ë¥˜ë˜ëŠ”ë°,


Paris Hiltonì˜ **Paris(PER)**ì™€, Hilton Hotelì´ ìˆëŠ” **Paris(LOC)**ì…ë‹ˆë‹¤.


â†’ ì´ë ‡ê²Œ ê°™ì€ ì´ë¦„ì„ ë‹¤ë¥¸ ìœ í˜•ìœ¼ë¡œ ì¸ì‹í•˜ê¸° ìœ„í•´ì„  **contextë¥¼ ì´ìš©**í•´ì•¼ í•©ë‹ˆë‹¤.


ì´ ì‘ì—…ì„ ìœ„í•´ ë‹¤ìŒ ê³¼ì •ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.


> ğŸ§® **ê³¼ì •**  
> 1. word vectorë¡œ ì´ë¤„ì§„ **context window** ë§Œë“¤ê¸°  
>   
> 2. **neural network layer**ì— íˆ¬ì…  
>   
> 3. **logistic classifier**ë¡œ **ë¶„ë¥˜** (ì¼ë‹¨ì€ simple í•˜ê²Œ)

	1. word vectorë¡œ ì´ë¤„ì§„ **context window** ë§Œë“¤ê¸°
	2. **neural network layer**ì— íˆ¬ì…
	3. **logistic classifier**ë¡œ **ë¶„ë¥˜** (ì¼ë‹¨ì€ simple í•˜ê²Œ)

ìš°ë¦¬ëŠ” ë¬¸ë§¥ì†ì— ë“±ì¥í•˜ëŠ” **Paris**ë¼ëŠ” ë‹¨ì–´ê°€ **LOC**ì¸ì§€ ì•Œì•„ë³¼ ê²ƒì…ë‹ˆë‹¤. 


**ì´ ì£¼ì œë¥¼ í†µí•´ ì „ë°˜ì ì¸ neural network ê³¼ì •ì„ ì‚´í´ë³¼ ê²ƒì…ë‹ˆë‹¤.**



#### A. context window


$x_{Paris}$**ì™€ location ê³¼ì˜ ì—°ê´€ì„±**ì„ ì°¾ì•„ë³´ê¸° ìœ„í•´, $x_{Paris}$ ì£¼ìœ„ì˜ **context window**ë¥¼ êµ¬ì„±


![1](/assets/img/2023-07-03-CS224n---Lecture-3-(Backprop-and-Neural-Networks).md/1.png)


x í•˜ë‚˜ í•˜ë‚˜ëŠ” word imbeddingì„ í†µí•´ êµ¬ì„±ëœ **word vectorë¥¼ ì˜ë¯¸**í•˜ê³ 


ê·¸ ê²°ê³¼, $X_{window}$ëŠ” **ê¸¸ê²Œ ëŠ˜ì—¬ì§„ vectorì˜ í˜•íƒœ**ê°€ ë©ë‹ˆë‹¤.


![2](/assets/img/2023-07-03-CS224n---Lecture-3-(Backprop-and-Neural-Networks).md/2.png)_ì˜ˆì‹œëŠ” 4-dimensional word imbedding ì‹œì˜ window vector_



#### B. **neural network layer**


context windowë¥¼ í†µí•´ vectorê°€ êµ¬ì„±ëœ í›„, neural network layerë“¤ë¡œ íˆ¬ì…ë˜ê²Œ ë©ë‹ˆë‹¤.


![3](/assets/img/2023-07-03-CS224n---Lecture-3-(Backprop-and-Neural-Networks).md/3.png)



#### C. logic classifier


neural network layerë“¤ì„ ì§€ë‚˜, logic classifierì— í™œìš©í•˜ê¸° ìœ„í•´ í•œ probability valueë¥¼ outputí•¨


($s = u^Th$**; uëŠ” (mx1)ì˜ weight vector, ìœ„ì˜ ì˜ˆì‹œì—ì„  (8x1)**)


**së¥¼ í™•ë¥ ë¡œ ë³€í™˜**í•˜ê¸° ìœ„í•´ **sigmoid** í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•´, ìµœì¢… outputì¸ $J_t(\theta) = \sigma(s) = \frac{1}{1+e^{-s}}$ë¥¼ ë°˜í™˜í•¨


ì´ë ‡ê²Œ outputì„ ì‚°ì¶œí•´ loss functionì„ í†µí•´ ì°¨ì´ë¥¼ ì¤„ì—¬ë‚˜ê°€ì•¼ í•©ë‹ˆë‹¤.


neural network ë°©ì‹ì—ì„œ ê°€ì¥ ë§ì´ ì‚¬ìš©í•˜ëŠ” ë°©ì‹ì´ **Gradient Descent ë°©ì‹**ì´ê³ , ì´ ë°©ì‹ì„ ì‚¬ìš©í•˜ê¸° ìœ„í•´ì„  **parameterì˜ ì§€ì†ì ì¸ update**ê°€ í•„ìš”í•©ë‹ˆë‹¤. costë¡œ ì¸í•´ Stochastic Gradient Descent ë°©ì‹ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.


$$
\theta^{new} = \theta^{old}-\alpha\triangledown_\theta J(\theta)
$$


ê° parameter ë‹¨ìœ„ë¡œ ë³¸ë‹¤ë©´ $\theta_j^{new} = \theta_j^{old}-\alpha \frac{\partial J(\theta)}{\partial~\theta_j^{old}}$ë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.


**ì´ Aâ†’Bâ†’Cì˜ ê³¼ì •ì„ í†µí•´ ì§„í–‰ë¨**



### Gradient Descent

1. **n inputs and 1 output**

$f(x) = f(x_1, x_2, ..., x_n)$ 


$\frac{\partial f}{\partial x} = [\frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2}, ..., \frac{\partial f}{\partial x_n}]$ 

1. **n inputs and m outputs   (MxN)**

 $f(x) = [f_1(x_1, x_2, ..., x_n),...,f_m(x_1, x_2, ..., x_n)]$ 


![4](/assets/img/2023-07-03-CS224n---Lecture-3-(Backprop-and-Neural-Networks).md/4.png)


---



#### **What is Jacobian?**


ìì½”ë¹„ì•ˆ(Jacobian)ì€ **ë²¡í„° í•¨ìˆ˜(vector-valued function)ì˜ ê° ì›ì†Œì— ëŒ€í•œ ëª¨ë“  ê°€ëŠ¥í•œ ì¼ì°¨ í¸ë¯¸ë¶„(First-order partial derivatives)ì„ êµ¬ì„±í•œ í–‰ë ¬**


ìì½”ë¹„ì•ˆ í–‰ë ¬ì€ **ë²¡í„° í•¨ìˆ˜ì˜ ë¯¸ë¶„ì— ê´€í•œ ì •ë³´**ë¥¼ ë‹´ê³  ìˆìœ¼ë©°, ë§¤ìš° ì¤‘ìš”í•œ ì„ í˜• ëŒ€ìˆ˜í•™ì  ë„êµ¬ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ìì½”ë¹„ì•ˆ í–‰ë ¬ì€ í•¨ìˆ˜ì˜ ê¸°ìš¸ê¸° ë²¡í„°(gradient vector)ë¥¼ ê³„ì‚°í•˜ê³ , í•¨ìˆ˜ì˜ í…Œì¼ëŸ¬ ì „ê°œ(Taylor expansion)ë¥¼ ìœ„í•œ ì„ í˜• ê·¼ì‚¬(linear approximation)ë¥¼ ìˆ˜í–‰í•˜ëŠ” ë“±ì˜ ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ í™œìš©ë©ë‹ˆë‹¤.


ë§ë¡œ ë“¤ìœ¼ë‹ˆ ë­” ì†Œë¦°ì§€ ëª¨ë¥´ê² ë„¤ìš”â€¦


ìˆ˜ì‹ì„ í†µí•´ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤.


![5](/assets/img/2023-07-03-CS224n---Lecture-3-(Backprop-and-Neural-Networks).md/5.png)


ë‹¤ìŒê³¼ ê°™ì´, nê°œì˜ inputì„ ë°›ëŠ” mê°œì˜ í•¨ìˆ˜ê°€ ìˆë‹¤ê³  í•©ì‹œë‹¤.


ì´ë“¤ì„ ëª¨ë‘ í¸ë¯¸ë¶„ í•˜ë ¤ë©´ ë‹¤ìŒê³¼ ê°™ì€ ë²¡í„°/í–‰ë ¬ë¡œ í‘œì‹œí•©ë‹ˆë‹¤.


![6](/assets/img/2023-07-03-CS224n---Lecture-3-(Backprop-and-Neural-Networks).md/6.png)_Jacobian Matrix_


ì´ë ‡ê²Œ nê°œì˜ ë³€ìˆ˜ë¥¼ ë°›ëŠ” mê°œì˜ í•¨ìˆ˜ê°€ ìˆì„ ë•Œ, ì´ í•¨ìˆ˜ë“¤ì˜ í¸ë¯¸ë¶„ì„ êµ¬í•´ì•¼ í•˜ëŠ” ìƒí™©ì…ë‹ˆë‹¤.


ì´ ë•Œ, ì´ **ì „ì²´ í¸ë¯¸ë¶„**ì„ ìš°ë¦¬ê°€ ë‹¨ìˆœíˆ **ê³±í•´ì„œ ë”í•˜ëŠ” í¼ìœ¼ë¡œ ë§Œë“¤ì–´ ë†“ì„ ìˆ˜ ìˆëŠ” ê²ƒ**ì„ **Jacobian Matrix**ë¼ê³  í•©ë‹ˆë‹¤.


ì˜ˆì‹œë¥¼ í†µí•´ ì•Œì•„ë³´ì£ 


![7](/assets/img/2023-07-03-CS224n---Lecture-3-(Backprop-and-Neural-Networks).md/7.png)


$F_1(x, y) = x^2y,~~~F_2(x,y)=5x+sin(y)$ ì¼ë•Œ,


Jacobian Matrix  $J_F(x, y)$ ëŠ” ë‹¤ìŒì²˜ëŸ¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.


![8](/assets/img/2023-07-03-CS224n---Lecture-3-(Backprop-and-Neural-Networks).md/8.png)


ë§Œì•½, 2ê°œì˜ ë³€ìˆ˜ì— 3ê°œì˜ í•¨ìˆ˜ë¼ë©´?


**â†’ 3x2 matrixê°€ ìƒì„±ë¨**


**ì´ì²˜ëŸ¼ Jacobian matrixì˜ í˜•íƒœëŠ” mxn matrixì…ë‹ˆë‹¤. (m: í•¨ìˆ˜ ê°œìˆ˜, n: input ê°œìˆ˜)**



#### **ë„í•¨ìˆ˜ëŠ” ì–´ë–»ê²Œ êµ¬í•  ìˆ˜ ìˆì„ê¹Œ? - Chain Rule**


ìš°ë¦¬ëŠ” **chain ruleì„ ì´ìš©**í•´, ë„í•¨ìˆ˜ë¥¼ êµ¬í•œë‹¤.


![9](/assets/img/2023-07-03-CS224n---Lecture-3-(Backprop-and-Neural-Networks).md/9.png)


Jacobianì˜ ê²½ìš°ì—ë„ ë¹„ìŠ·í•˜ê²Œ ìˆ˜í–‰ ê°€ëŠ¥


(ì•„ë˜ì˜ z, W, b ë“±ì€ ëª¨ë‘ í–‰ë ¬ê°’)


![10](/assets/img/2023-07-03-CS224n---Lecture-3-(Backprop-and-Neural-Networks).md/10.png)


($h=f(z)$ë¥¼ ë¯¸ë¶„í•´ $\frac{\partial h}{\partial z}$, $z = Wx + b$ë¥¼ ë¯¸ë¶„í•´ $\frac{\partial z}{\partial x}$ë¥¼ êµ¬í•¨)


(1) $x$  (input)    (Mx1)


![11](/assets/img/2023-07-03-CS224n---Lecture-3-(Backprop-and-Neural-Networks).md/11.png)


(2) $z = Wx +b$    (W: NxM,   b: Nx1)


![12](/assets/img/2023-07-03-CS224n---Lecture-3-(Backprop-and-Neural-Networks).md/12.png)


![13](/assets/img/2023-07-03-CS224n---Lecture-3-(Backprop-and-Neural-Networks).md/13.png)


(3) $h = f(z)$    (h: Nx1,    z: Nx1)  


![14](/assets/img/2023-07-03-CS224n---Lecture-3-(Backprop-and-Neural-Networks).md/14.png)


(4) $s = u^Th$    (s: scalar,    u: Nx1) 


![15](/assets/img/2023-07-03-CS224n---Lecture-3-(Backprop-and-Neural-Networks).md/15.png)


**(1) â†’ (4)ì˜ ë°©í–¥ì„ í†µí•´ forward propagation**ì´ ì´ë¤„ì§€ê³ 


![16](/assets/img/2023-07-03-CS224n---Lecture-3-(Backprop-and-Neural-Networks).md/16.png)


**gradient descent ë°©ì‹ì„ í†µí•´ (4) â†’ (1)ì˜ ë°©í–¥ìœ¼ë¡œ backpropagation**ì´ ì´ë¤„ì§„ë‹¤.


![17](/assets/img/2023-07-03-CS224n---Lecture-3-(Backprop-and-Neural-Networks).md/17.png)


![18](/assets/img/2023-07-03-CS224n---Lecture-3-(Backprop-and-Neural-Networks).md/18.png)_ìƒì„¸ ê³¼ì • 1_


![19](/assets/img/2023-07-03-CS224n---Lecture-3-(Backprop-and-Neural-Networks).md/19.png)_ìƒì„¸ ê³¼ì • 2_


ì´ ê³¼ì •ì—ì„œ $\frac{\partial s}{\partial b}, \frac{\partial s}{\partial z} $ ë“±ì€ ë°”ë¡œ ì•Œ ìˆ˜ ì—†ë‹¤. ì´ë¥¼ ê°„í¸í•˜ê²Œ í•´ì£¼ëŠ” ë°©ì‹ì´ ë°”ë¡œ **chain rule**ì´ë‹¤.


ê·¸ë ‡ë‹¤ë©´, ê° (1)~(4)ì—ì„œì˜ í¸ë¯¸ë¶„ê°’ì„ ê³„ì‚°í•˜ë ¤ë©´ ì–´ë–»ê²Œ chain ruleì„ ì ìš©í•´ì•¼ í•˜ëŠ”ì§€ ì•Œì•„ë³´ì.



#### (4) $s = u^Th$    (s: scalar,    u: Mx1) 


ìš°ì„ , (4)ëŠ” së¥¼ êµ¬í•˜ëŠ” ì‹ì´ê¸° ë•Œë¬¸ì—, backpropagationì˜ ì²˜ìŒ ë‹¨ê³„ì´ë‹¤.


![20](/assets/img/2023-07-03-CS224n---Lecture-3-(Backprop-and-Neural-Networks).md/20.png)


**scalarë¥¼ vectorë¡œ í¸ë¯¸ë¶„**í•˜ëŠ” ê²½ìš°ì— í•´ë‹¹í•œë‹¤.


---


ìŠ¤ì¹¼ë¼ í•¨ìˆ˜ë¥¼ ë²¡í„°ë¡œ ë¯¸ë¶„(Gradient) (yê°€ scalar, xê°€ vector)


![21](/assets/img/2023-07-03-CS224n---Lecture-3-(Backprop-and-Neural-Networks).md/21.png)


---


![22](/assets/img/2023-07-03-CS224n---Lecture-3-(Backprop-and-Neural-Networks).md/22.png)


> ğŸ’¡ ì™œ $u^T$**ì¸ê°€ìš”?**  
> â†’ $s = u^Th$ ì—ì„œ sëŠ” $u^Th$ ë¥¼ í†µí•´ì„œë§Œ hì— ì—°ê´€ë˜ê¸° ë•Œë¬¸  
> â†’ dot productì˜ derivateëŠ” ê³„ì‚°ì—ì„œì˜ ë‹¤ë¥¸ vectorì˜ transposeë¡œ ì£¼ì–´ì§„ë‹¤


$$
\frac{\partial s}{\partial h} = u^T
$$



#### (3) $h = f(z)$    (h: Nx1,    z: Nx1)  


![23](/assets/img/2023-07-03-CS224n---Lecture-3-(Backprop-and-Neural-Networks).md/23.png)


**vectorë¥¼ vectorë¡œ í¸ë¯¸ë¶„í•˜ëŠ” ê²½ìš°**ì— í•´ë‹¹


---


ë²¡í„°ë¥¼ ë²¡í„°ë¡œ ë¯¸ë¶„


![24](/assets/img/2023-07-03-CS224n---Lecture-3-(Backprop-and-Neural-Networks).md/24.png)


---


![25](/assets/img/2023-07-03-CS224n---Lecture-3-(Backprop-and-Neural-Networks).md/25.png)


> ğŸ’¡ **ì™œ diagonal matrixê°€ ë‚˜ì˜¤ë‚˜ìš”?**  
> $h_i = f(z_i)$ ì²˜ëŸ¼, indexê°€ ê°™ì€ ë³€ìˆ˜ë¼ë¦¬ë§Œ ì˜í–¥ì„ ë°›ìœ¼ë¯€ë¡œ, indexê°€ ê°™ì§€ ì•Šì€ ê²½ìš°ëŠ” partial derivativeê°€ 0ì´ ë©ë‹ˆë‹¤.


$$
\frac{\partial s}{\partial z} = \frac{\partial s}{\partial h}\frac{\partial h}{\partial z} = u^Tdiag(f\prime(z))
$$



#### (2) $z = Wx +b$    (W: NxM,   b: Nx1)


![26](/assets/img/2023-07-03-CS224n---Lecture-3-(Backprop-and-Neural-Networks).md/26.png)


![27](/assets/img/2023-07-03-CS224n---Lecture-3-(Backprop-and-Neural-Networks).md/27.png)



#### (2) - **1**     $\frac{\partial s}{\partial W}$


jacobian matrixì˜ ì˜ˆì‹œì—ì„œ Nê°œì˜ í•¨ìˆ˜, Mê°œì˜ inputì´ ìˆëŠ” ê²½ìš°ì— í•´ë‹¹


$\frac{\partial s}{\partial W} = \frac{\partial s}{\partial h}\frac{\partial h}{\partial z}\frac{\partial z}{\partial W} = u^Tdiag(f\prime(z))x^T$


![28](/assets/img/2023-07-03-CS224n---Lecture-3-(Backprop-and-Neural-Networks).md/28.png)



#### (2) - 2     $\frac{\partial s}{\partial b}$


jacobian matrixì˜ ì˜ˆì‹œì—ì„œ Nê°œì˜ í•¨ìˆ˜, 1ê°œì˜ inputì´ ìˆëŠ” ê²½ìš°ì— í•´ë‹¹


$\frac{\partial s}{\partial b}=\frac{\partial s}{\partial h}\frac{\partial h}{\partial z}\frac{\partial z}{\partial b} = u^Tdiag(f\prime(z))$


![29](/assets/img/2023-07-03-CS224n---Lecture-3-(Backprop-and-Neural-Networks).md/29.png)


> ğŸ’¡ **ì™œ** $\frac{\partial z}{\partial b}=I$ **ì¸ê°€ìš”?**  
> $z = Wx + b$ ì´ê³ , $z_i = Wx + b_i$ ì´ê¸° ë•Œë¬¸ì—, $z$ì™€ $b$ëŠ” indexê°€ ê°™ì„ ë•Œë§Œ ì˜í–¥ì„ ë°›ì•„ ì¼ë‹¨ diagonal matrixì…ë‹ˆë‹¤. ê·¼ë°, zì˜ ì‹ì„ ë³´ë©´ bê°€ directly added ë˜ì–´ ìˆê¸° ë•Œë¬¸ì—, bë¥¼ ë°”ê¾¸ëŠ” ê²ƒì´ ì§ì ‘ì ìœ¼ë¡œ zì— ì˜í–¥ì„ ì¤ë‹ˆë‹¤. ê·¸ë ‡ê¸° ë•Œë¬¸ì—, $\frac{\partial z_i}{\partial b_i} = 1$ ì´ê¸° ë•Œë¬¸ì—, Identity Matrixê°€ ë˜ëŠ” ê²ƒì…ë‹ˆë‹¤



#### (1) $x$  (input)    (Mx1)


![30](/assets/img/2023-07-03-CS224n---Lecture-3-(Backprop-and-Neural-Networks).md/30.png)


$\frac{\partial s}{\partial x}$ **-** Jacobian matrixì˜ ì˜ˆì‹œì—ì„œ Nê°œì˜ í•¨ìˆ˜, Mê°œì˜ Inputì´ ìˆëŠ” ê²½ìš°ì— í•´ë‹¹


$\frac{\partial s}{\partial b}=\frac{\partial s}{\partial h}\frac{\partial h}{\partial z}\frac{\partial z}{\partial x} = u^Tdiag(f\prime(z))W$


![31](/assets/img/2023-07-03-CS224n---Lecture-3-(Backprop-and-Neural-Networks).md/31.png)


> ğŸ’¡ **ì™œ** $\frac{\partial z}{\partial x} = W$**ì¸ê°€ìš”?**  
> ìœ„ì—ì„œ ì–˜ê¸°í–ˆë“¯ì´, dot productì—ì„œì˜ í•œìª½ì„ ëŒ€ìƒìœ¼ë¡œ í¸ë¯¸ë¶„ì„ í•˜ë©´ ë‚˜ë¨¸ì§€ ê³„ì‚°ì˜ ëŒ€ìƒì´ ê²°ê³¼ë¬¼ì…ë‹ˆë‹¤.  
> **â€dot productì˜ derivateëŠ” ê³„ì‚°ì—ì„œì˜ ë‹¤ë¥¸ vectorì˜ transposeë¡œ ì£¼ì–´ì§„ë‹¤â€**


---

- **í—·ê°ˆë¦¬ëŠ” ì ?**

$u^Th$ ì²˜ëŸ¼ uì™€ hì˜ dot productëŠ” Tê°€ ë¶™ì–´ìˆì§€ë§Œ, $Wx+b$ì—ì„œ Wì™€ xì˜ dot productì—ëŠ” Tê°€ ë¶™ì–´ìˆì§€ ì•ŠìŒ. ê·¸ë ‡ê¸° ë•Œë¬¸ì— WëŠ” NxMì¸ì§€, MxNì¸ì§€ í—·ê°ˆë¦¼.


---


ğŸ’¡Â **ê³„ì‚°ëŸ‰ì„ ì¤„ì´ëŠ” ë°©ë²•**


$\frac{\partial s}{\partial W}$ = $\frac{\partial s}{\partial h} \frac{\partial h}{\partial z}$$\frac{\partial z}{\partial W}$


$\frac{\partial s}{\partial b}$ = $\frac{\partial s}{\partial h} \frac{\partial h}{\partial z}$$\frac{\partial z}{\partial b}$


(â†’ ê³µí†µë¶€ë¶„ ì¡´ì¬)


**â†’ avoid duplicated computation**


define $\delta$

<details>
  <summary>delta?</summary>


---


$\delta$ is defined in two methods


1) **small change in a variable or function**


can take the limit as delta approaches zero to obtain the derivative.


2) difference between the actual output and the desired output of a neural network


(local error signal)


This is used in backpropagation algorithms to calculate the gradients of the loss function with respect to the network parameters. The delta is propagated backwards through the layers of the network to update the weights and biases.


---



  </details>

#### $\delta $ = $\frac{\partial s}{\partial h} \frac{\partial h}{\partial z}$



#### $\frac{\partial s}{\partial W}$ = $\delta$ $\frac{\partial z}{\partial W}$



#### $\frac{\partial s}{\partial b} $ = $\delta$ $ \frac{\partial z}{\partial b}$


---



#### disagreement between jacobian form and the shape convention


![32](/assets/img/2023-07-03-CS224n---Lecture-3-(Backprop-and-Neural-Networks).md/32.png)



#### Letâ€™s Get $\frac{\partial z}{\partial W}$


$W \in \mathbb{R}^{n \times m}$


$\frac{\partial s}{\partial W} = \frac{\partial s}{\partial h}\frac{\partial h}{\partial z}\frac{\partial z}{\partial W} = u^Tdiag(f\prime(z))\frac{\partial z}{\partial W}$


![33](/assets/img/2023-07-03-CS224n---Lecture-3-(Backprop-and-Neural-Networks).md/33.png)


â‡’ ê²°ê³¼ë¬¼ì€ 1xM matrix


$\theta^{new} = \theta^{old}-\alpha \triangledown_\theta J(\theta)$


í•˜ì§€ë§Œ, **WëŠ” NxM matrix**ì´ê¸° ë•Œë¬¸ì—, **shape conventionì„ í•´ì¤˜ì•¼ í•¨**


ì´ì²˜ëŸ¼, **backpropagationì„ í•˜ë‹¤ë³´ë©´, SGD ë°©ë²•ì„ ì‚¬ìš©í•˜ê¸° í¸í•˜ì§€ ì•Šì€ í˜•íƒœê°€ ë˜ê¸°ë„ í•¨**


![34](/assets/img/2023-07-03-CS224n---Lecture-3-(Backprop-and-Neural-Networks).md/34.png)


ê·¸ë¦¼ì—ì„œ ë³¼ ìˆ˜ ìˆë‹¤ì‹œí”¼, $\frac{\partial z}{\partial W}$ëŠ” ë‘ê°€ì§€ í˜•íƒœ, NxM ë˜ëŠ” 1xM matrixë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤.


**(1xM matrixê°€ ë°”ë¡œ** $x^T$**ì— í•´ë‹¹)**


â‡’ $\frac{\partial s}{\partial W}$$= $ $\delta$ $\frac{\partial z}{\partial W}$ $ =$ $\delta^T$ $x^T$


($\delta$: local error signal at z)


($x$ is local input signal)

<details>
  <summary>ì™œ $\frac{\partial z}{\partial W} = x$ ì¼ê¹Œ ê°•ì˜ ver.</summary>


**ì™œ** $x$**ì¼ê¹Œ?** 


$\frac{\partial s}{\partial W}$$= $ $\delta$ $\frac{\partial z}{\partial W}$ $ =$ $\delta$ $\frac{\partial}{\partial W}(Wx + b)$


ë‹¤ì‹œ ì´ë ‡ê²Œ í‘œì‹œí•  ìˆ˜ ìˆëŠ”ë°,


ì´ì œ single weightì¸ $W_{ij}$ì— ëŒ€í•œ derivativeë¥¼ ìƒê°í•´ë³´ì


$W_{ij}$ ëŠ” $z_i$ì—ë§Œ ì˜í–¥ì„ ë°›ìŒ


ex. $W_{23}$ì€ ì˜¤ì§, $z_2$ì—ë§Œ ì˜í–¥ì„ ì£¼ì§€, $z_1$ì— ì˜í–¥ì„ ì£¼ì§„ ì•ŠìŒ



### $\frac{\partial z_i}{\partial W_{ij}} $$= \frac{\partial}{\partial W_{ij}}W_i\cdot x + b_i = \frac{\partial}{\partial W_{ij}}\Sigma^d_{k=1}W_{ik}x_k = x_j$


($\because z_i = W_i\cdot x + b_i$)


($\because W_i \cdot x = \Sigma^d_{k=1}W_{ik}x_k$, WëŠ” 2ì°¨ì› matrix)



#### â‡’ $\frac{\partial s}{\partial W}$$= $ $\delta$ $\frac{\partial z}{\partial W}$ $ =$ $\delta^T$ $x^T$



  </details>
**ê·¸ë˜ì„œ ì•„ë˜ì™€ ê°™ì€ ì‹ì´ ê°€ëŠ¥í–ˆë˜ ê²ƒì´ë‹¤.**


![35](/assets/img/2023-07-03-CS224n---Lecture-3-(Backprop-and-Neural-Networks).md/35.png)


ë¹„ìŠ·í•˜ê²Œ, $\frac{\partial s}{\partial b}$ $= h^T \circ f\prime(z)$ ëŠ” **row vector  (1xN)**


bëŠ” Nx1 column vectorì´ê¸° ë•Œë¬¸ì—, transposeë¥¼ í†µí•´ shape convention í•„ìš”

- Jacobian form â†’ chain rule ì ìš©ì— ìœ ë¦¬
- shape convention â†’ SGD ì ìš©ì— ìœ ë¦¬

ì–´ë–»ê²Œ í• ê¹Œ?

1. Use Jacobian forms as much as possible, reshape to follow the shape convention at the end
	- ìš°ë¦¬ê°€ í•œ ë°©ì‹ì…ë‹ˆë‹¤.
	- í•˜ì§€ë§Œ, ë§ˆì§€ë§‰ì— transpose $\frac{\partial s}{\partial b}$ to make the derivative a column vector, resulting in $\delta^T$
2. Always follow the shape convention
	- Look at dimensions to figure out when to transpose and/or reorder terms
	- The error message $\delta$ that arrives at a hidden layer has the same dimensionality as that hidden layer
