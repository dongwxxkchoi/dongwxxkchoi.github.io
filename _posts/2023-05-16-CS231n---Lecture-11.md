---
layout: single
date: 2023-05-16
title: "CS231n - Lecture 11"
use_math: true
tags: [강의/책 정리, ]
categories: [AI, ]
---


## Other Computer Vision Tasks


Image Classification 외에도 다양한 vision tasks를 cnn을 통해 수행할 수 있음


![0](/assets/img/2023-05-16-CS231n---Lecture-11.md/0.png)



### **Semantic Segmentation**

- **input:** 이미지
- **output:** 이미지의 **모든 픽셀**에 카테고리를 출력함
- **특징**
	- **특정 픽셀**이 “고양이”, “잔디”, “하늘”, “나무”, “배경” 중 **어디에 속하는지를 결정**함
	- 같은 category, 다른 개체를 구분하지 않음

		![1](/assets/img/2023-05-16-CS231n---Lecture-11.md/1.png)_암소 두마리의 구별이 불가능 → instance segmentation으로 해결 가능_

- **Sliding Window**

	![2](/assets/img/2023-05-16-CS231n---Lecture-11.md/2.png)

	- input image를 아주 작은 단위(pixel)로 쪼갠 후, 그 작은 영역에 대해 classification 수행
	- 아주 cost가 큼 (굉장히 비효율적; 좋은 idea 아님)
	- 인접한 영역은 겹쳐있어 특징을 공유할 수도 있음

		→ 개별적 적용에 좋지 않음

- **Fully Convolutional Network**

	![3](/assets/img/2023-05-16-CS231n---Lecture-11.md/3.png)

	- FC-layer가 없고 convolutional layer로만 구성
	- 중간중간 padding을 통해 크기 유지 (공간정보 손실 x)
	- 입력: Image
	- 출력: CxHxW tensor (C는 카테고리의 수)
		- 입력 이미지의 모든 픽셀 값에 대해 classification scores를 매긴 값
		- 각 픽셀별 c개의 data를 대상으로 argmax를 수행해 pred 산출
		- 모든 픽셀의 classification loss 계산해 평균 값을 취하고 back-propagation
- **Questions**
	- training data를 어떻게 만드는지?

		매우 cost 큰 작업, ex. 객체의 외관선을 그려 안을 채워넣기

	- Loss Functions?

		모든 픽셀에 Cross Entropy → 더하거나 평균화해서 loss 계산

- **Problem & Solution**

	입력 image의 spatial size를 유지해야 하기 때문에 cost가 큼


	→ **feature map을 downsampling and upsampling**해서 해결함


	![4](/assets/img/2023-05-16-CS231n---Lecture-11.md/4.png)


	image의 spatial resolution 전체를 대상으로 conv X


	→ conv layer를 소량만 사용 (downsampling - upsampling)

- **downsampling and upsampling**
	- **downsampling**

		→ max pooling, stride convolution

	- **upsampling**

		→ unpooling


		→ classification network와 차이점 (fc-layer X)


		→ spatial resolution을 다시 키워 입력 이미지의 해상도와 같도록 함

- **Unpooling**

	![5](/assets/img/2023-05-16-CS231n---Lecture-11.md/5.png)

	- Nearest Neighbor Unpooling

		receptive field로 값을 그대로 다 복사

	- Bed of Nails Unpooling

		unpooling region에만 값을 복사하고, 나머지는 0을 복사


		하나의 요소 제외 모두 0


		(zero는 평평, non-zero는 뾰족하게 값이 튀어서)

	- Max unpooling

		![6](/assets/img/2023-05-16-CS231n---Lecture-11.md/6.png)


		앞의 pooling과 unpooling을 연관 짓는 방법


		이전 max pooling시에 선택된 요소의 위치를 기억해, 그 위치에 값을 넣는 것


		**why?**


		segmentation 결과에서 객체들 간의 디테일한 경계가 명확할수록 좋은데, max pooling으로 인해, feature map의 비균진성이 발생한다. (2x2 pooling에서 어디서 왔는지 모름, 공간 정보 잃음) 즉, 어디에서 왔는지 알 수 없다. 기존 maxpool에서 뽑아온 자리로 값을 넣어줌으로, 공간 정보를 조금 더 디테일하게 다룰 수 있다. 

	- Transpose Convolution (Learnable)

		![7](/assets/img/2023-05-16-CS231n---Lecture-11.md/7.png)


		→ strided convolution처럼 학습할 수 있는 unsampling 방식


		어떤 방식으로 upsampling 할 지 학습함


		![8](/assets/img/2023-05-16-CS231n---Lecture-11.md/8.png)


		내적을 수행하지 않음


		input feature map에서 값을 하나 선택 (빨간색)


		그 스칼라 값을 filter와 곱해 출력의 3x3 영역에 그 값을 넣음


		즉, 입력 값이 필터에 곱해지는 가중치의 역할을 함


		출력 값은 필터 * 입력(가중치)


		![9](/assets/img/2023-05-16-CS231n---Lecture-11.md/9.png)


		이렇게 진행하다보면, 겹치는 경우가 생기는데, transpose convolution 간에 receptive field가 겹칠 수 있음


		→ 겹치는 경우엔 두 값을 더한다.


		→ 이 과정을 계속 반복해 spatial size를 키우는 것 

		- **특징**

			deconvolution (좋지 않음 이름), upconvolution, fractionally strided convolution 이라고도 불림


			(stride를 input/output 간 비율로 생각하면, stride 1/2 convolution)

		- **example**

			![10](/assets/img/2023-05-16-CS231n---Lecture-11.md/10.png)


			3x1 Transpose Convolution in 1 dimension


			a,b → x,y,z


			![11](/assets/img/2023-05-16-CS231n---Lecture-11.md/11.png)_left: stride 1 convolution, right: stride 1 transpose_


			X: kernel


			![12](/assets/img/2023-05-16-CS231n---Lecture-11.md/12.png)


			→ 오른쪽처럼 근본적으로 다른 연산이 됨

		- Questions?
			- 왜 sum? not average
				- Transpose이기 때문, but, receptive field의 크기에 따라 magnitude가 달라져서 문제.

					3x3 stride 2 transpose convolution → checkerboard artifacts 발생


					⇒ 그림 그려보자


					4x4 stride 2, 2x2 stride 2를 사용해 완화하기도 함

			- stride 1/2 convolution
				- 이 용어 보다는 transpose convolution이 좋다
undefined

### **Classification + Localization**


![13](/assets/img/2023-05-16-CS231n---Lecture-11.md/13.png)

- input: image
- output: category and object 위치
- 특징
	- 하나의 object를 대상으로 classify, localize
- 두 개의 출력 값

	![14](/assets/img/2023-05-16-CS231n---Lecture-11.md/14.png)

	- class scores를 통한 label (with softmax loss)
	- box coordinates (train 이미지에 좌표 label도 존재) (with l2 loss (다른 l1, smooth l1도 가능))

		→ 이 Loss 들은 예측한 Bbox와 GT Bbox 좌표 간의 차이에 대한 regression loss

	- Question?
		- 왜 두 개를 같이 학습?

			많은 사람들이 두 loss를 동시에 학습시킴 (일반적인 방법) 


			오분류 문제에서 까다로울 순 있음 


			→ 하나만 예측하는 것이 아니라, Bbox를 카테고리마다 하나 씩 예측함 


			→ Ground Truth 카테고리에 속한 예측된 Bbox에만 Loss와 연결시킴 

		- 두 개의 Loss 단위가 다른 경우에 gradient 계산 문제 없음?

			두 loss의 가중치를 조절하는 hyperparameter 존재 


			실제로 손실함수 값을 바꾸기 때문에, 비교하기가 까다로움


			→ 어려운 작업


			⇒ 다른 성능 지표를 도입해 cross validation으로 hyperparameter를 최적화 할 때 loss가 아니라 실제 모델의 성능 지표를 봄 

		- 앞 쪽의 큰 네트워크는 고정시키고 각 FC-layer만 학습시키는 방법? (분리)

			transfer learning → fine tune이 성능이 좋아짐


			네트워크를 freeze 하고 두 fc-layer를 학습 시킴


			fc-layer가 수렴하면 다시 합쳐 전체 시스템을 fine-tune



#### Human Pose Estimation 


![15](/assets/img/2023-05-16-CS231n---Lecture-11.md/15.png)


input: 사람 이미지


output: 각 관절의 위치


⇒ 사람의 포즈를 예측


	(ex. 14개의 관절의 위치로 사람의 포즈를 정의함)


ex. media-pipe


![16](/assets/img/2023-05-16-CS231n---Lecture-11.md/16.png)

- Question
	- Regression Loss?
		- L1, L2, smooth L1 등을 나타냄

			not classification loss (ex. softmax, cross entropy)


			출력이 관절의 위치처럼 연속적인 값이기에 regression loss를 사용


			⇒ classificaton + localization → regression output 개수가 고정되어 있음 


				→ pose estimation과 같은 문제도 풀 수 있음 



### Object Detection


![17](/assets/img/2023-05-16-CS231n---Lecture-11.md/17.png)

- input: image
- output: 여러 물체의 category + bbox (개수 고정 x)
- 특징
	- 객체의 수가 이미지마다 다름

		![18](/assets/img/2023-05-16-CS231n---Lecture-11.md/18.png)


		→ 값의 예측 수도 달라지고, 미리 알 수 없음


		→ 다른 paradigm 필요

- sliding window

![19](/assets/img/2023-05-16-CS231n---Lecture-11.md/19.png)


![20](/assets/img/2023-05-16-CS231n---Lecture-11.md/20.png)


![21](/assets/img/2023-05-16-CS231n---Lecture-11.md/21.png)


![22](/assets/img/2023-05-16-CS231n---Lecture-11.md/22.png)


![23](/assets/img/2023-05-16-CS231n---Lecture-11.md/23.png)


⇒ window를 sliding해가면서, 각 부분이 dog인지, cat인지, background인지를 출력함

	- 근데 어떻게 영역을 추출할까?
		- Objects가 몇 개 존재할지, 어디에 존재할지
		- 크기가 어떨지, 어떤 종횡비로 표현해야 할지?

			⇒ 다 커버하려면 brute-force 방식이 됨 (너무 큰 cost)

- Region Proposals

	![24](/assets/img/2023-05-16-CS231n---Lecture-11.md/24.png)


	→ 신호처리에서 발생하던 방식


	→ Object가 있을 법한 bbox를 제공함


		 how?


		이미지 내에 뭉텅진(blobby) 곳들을 찾아냄


		객체가 있을지도 모르는 후보 영역들임

		- Selective search

			GPU 2초간 돌리면 2000개의 region proposal 도출


			이 방법의 장점은 노이즈가 심하지만, recall은 높음


			(즉, 잘못된 추측도 많지만, 실제 객체가 있는 위치도 골라내긴 한다는 뜻)


		⇒ 이렇게 얻은 region proposal을 CNN의 입력으로 함 

- **R-CNN**

	위 방식을 활용한 network

	1. region proposal 추출

		![25](/assets/img/2023-05-16-CS231n---Lecture-11.md/25.png)

	1. warp

		![26](/assets/img/2023-05-16-CS231n---Lecture-11.md/26.png)


		동일한 CNN network에서 사용하기 위해, 이미지 같은 size로 scaling

	2. 각각의 convnet 통과
	3. classify regions with SVMs (최종 classification)

		![27](/assets/img/2023-05-16-CS231n---Lecture-11.md/27.png)

	4. region proposal 보정 위한 regression

		![28](/assets/img/2023-05-16-CS231n---Lecture-11.md/28.png)

	5. R-CNN은 output으로 BBox를 보정해 줄 offset 값 4개도 예측하는데, 이를 multi-task loss로 두고 학습
	- Questions?
		- offset이 항상 ROI의 안쪽으로만 작용할 수 있나?

			→ No. 외부로 향하기도 해야 함

		- 실제 객체가 없는 ROIs?

			→ background로

		- 필요한 데이터가 무엇인지?

			→ Fully Supervised. 모든 객체에 대한 BBox 

	- 문제

		![29](/assets/img/2023-05-16-CS231n---Lecture-11.md/29.png)

		- cost
		- 학습 과정 느림
		- test time도 느림 (한 이미지마다 30s)
- Fast R-CNN

	Fast R-CNN에서 R-CNN의 거의 모든 문제점 해결

	1. ROI

		![30](/assets/img/2023-05-16-CS231n---Lecture-11.md/30.png)


		CNN Feature map에 ROI를 Projection 시키고, 전체 이미지가 아닌 Feature map에서 가져옴 


		→ CNN의 Feature를 여러 ROIs가 공유할 수 있음 

	2. image 크기 맞춰주기 (ROI Pooling)

		![31](/assets/img/2023-05-16-CS231n---Lecture-11.md/31.png)

	3. FC-Layer로 넣어 Classifcation score와 Linear regression offset 계산

		![32](/assets/img/2023-05-16-CS231n---Lecture-11.md/32.png)

	4. 두 Loss를 합쳐 Multi-task Loss로 Backprop

		![33](/assets/img/2023-05-16-CS231n---Lecture-11.md/33.png)

	- ROI pooling (Max Pooling과 유사)

		![34](/assets/img/2023-05-16-CS231n---Lecture-11.md/34.png)

	- R-CNN과 Fast R-CNN을 비교

		![35](/assets/img/2023-05-16-CS231n---Lecture-11.md/35.png)


		region proposal 추출이 너무 오래 걸림

- Faster R-CNN

	네트워크에서 region proposal을 직접 만듦

	1. 입력 이미지 전체가 네트워크로 들어가 feature map 생성
	2. 별도의 region proposal network 통과
		1. classification loss (in RPN)
		2. bbox regression loss (in RPN)
	3. ROI Pooling ….
		1. classification loss (in Final)
		2. bbox regression loss (in Final)

	![36](/assets/img/2023-05-16-CS231n---Lecture-11.md/36.png)_region based method_

- Questions?
	- Multi-task learning ⇒ loss 별 나눠서 학습 한다면?

		→ 유의미한 변화 x. 한번에 하는 것이 계산량 적기 때문에 함

	- region proposal network ground truth가 없는데, 어떻게 학습?

		→ ground truth objects와 일정 threshold 이상 겹치는 proposal 있을 것


			→ 이런 region proposal은 positive


			→ 조금만 겹친다면 negative

	- classification loss in RPN?

		→ binary classification loss


![37](/assets/img/2023-05-16-CS231n---Lecture-11.md/37.png)


→ 하나의 regression 문제로 풀어보자 


→ Base BBox을 이용 (다양한 미리 정해진 bbox들)


⇒ 이들이 single-shot 계열


VGG, ResNet 등을 활용하기도 함 


![38](/assets/img/2023-05-16-CS231n---Lecture-11.md/38.png)


![39](/assets/img/2023-05-16-CS231n---Lecture-11.md/39.png)



### Instance Segmentation


객체 별로 객체의 위치를 알고 → Object Detection


Segmentation Mask를 예측 → Semantic Segmentation


![40](/assets/img/2023-05-16-CS231n---Lecture-11.md/40.png)


faster R-CNN과 유사


처음 입력 이미지가 CNN과 RPN을 거침 


특정 맵에서 RPN의 ROI만큼을 뜯어냄(project)


각 bbox마다 segmentation mask를 예측함 


RPN으로 뽑은 ROI 영역 내에서 각각 semantic semgentation 수행


feature map으로 부터 ROI Pooling(Align) 수행하면 두 갈래로 나뉨 

	- 첫 갈래는 Faster R-CNN
		- 각 Region Proposal이 어떤 카테고리 속하는지
		- 좌표 보정의 bbox regression
	- 두번째 semantic segmentation
		- mini network
		- 각 픽셀마다 객체인지 아닌지

	⇒ 즉, 모든 방법들을 통합시킨 방법이라 할 수 있음


	![41](/assets/img/2023-05-16-CS231n---Lecture-11.md/41.png)


	이거 하나로 모든 것 수행 가능…


	![42](/assets/img/2023-05-16-CS231n---Lecture-11.md/42.png)

- Question?

	Ms Coco → 80 category, 200000개 image


	사람 관절도 있음


	그렇다면 적은 학습 데이터로 좋은 성능을 낼 수 있을까?


	→ few-shot learning

