---
layout: single
date: 2023-08-03
title: "CS224n - Lecture 13 (Coreference Resolution)"
use_math: true
author_profile: false
tags: [ê°•ì˜/ì±… ì •ë¦¬, ]
categories: [AI, ]
---


## 1. Coreference Resolution


> ğŸ’¡ Identify all <u>**mentions**</u> that refer to the same entity in the word


![0](/assets/img/2023-08-03-CS224n---Lecture-13-(Coreference-Resolution).md/0.png)


ìœ„ì™€ ê°™ì´ <u>**ê°™ì€ ê°œì²´ë¥¼ íŒë³„í•˜ëŠ” ì‘ì—…**</u>ì„ Coreference Resolutionë¼ê³  í•œë‹¤.


---



### Coreference Resolution in Two Steps


![1](/assets/img/2023-08-03-CS224n---Lecture-13-(Coreference-Resolution).md/1.png)

1. Detect the mentions : ë§ ê·¸ëŒ€ë¡œ <u>**mentionì„ ì°¾ëŠ” ì‘ì—…**</u>ì´ë‹¤. ê·¸ëŸ¬ë¯€ë¡œ ì‰¬ìš´ ê³¼ì •ì´ë‹¤.
2. Cluster the mentions : mentionëœ <u>**ê°œì²´ë¥¼ ë¶„ë¥˜í•˜ëŠ” ì‘ì—…**</u>ì´ë‹¤. ì–¸ê¸‰ëœ ë¶€ë¶„ì´ ì–´ë–¤ ê°œì²´ì¸ì§€ ì•Œì•„ì•¼ í•˜ë¯€ë¡œ Detect the mentionsì„ í•˜ëŠ” ê³¼ì •ë³´ë‹¤ ì–´ë µë‹¤.

---



## 2. Mention Detection



#### Mentionì˜ ì¢…ë¥˜


![2](/assets/img/2023-08-03-CS224n---Lecture-13-(Coreference-Resolution).md/2.png)

1. ëŒ€ëª…ì‚¬
2. ê°œì²´ëª…
3. ëª…ì‚¬êµ¬


#### How to Detect?


![3](/assets/img/2023-08-03-CS224n---Lecture-13-(Coreference-Resolution).md/3.png)

1. <u>**ëŒ€ëª…ì‚¬ëŠ” POSë¡œ êµ¬ë¶„ì´ ê°€ëŠ¥**</u>í•˜ë‹¤. (POSëŠ” ë¬¸ì¥ ë‚´ ë‹¨ì–´ë“¤ì˜ í’ˆì‚¬ë¥¼ ì‹ë³„í•˜ì—¬ íƒœê·¸ë¥¼ ë¶™ì—¬ì£¼ëŠ” ê²ƒ) - POS Tagging
2. <u>**ê°œì²´ëª…ì€ ì•ì—ì„œ ë°°ìš´ NERì„ í†µí•´ ì°¾ëŠ”ë‹¤**</u>. (RNNì„ í†µí•´ NERì„ í•˜ëŠ” ê³¼ì •ì„ ì´ì „ì— ë°°ì› ì—ˆë‹¤.
3. <u>**ëª…ì‚¬êµ¬ëŠ” parserì„ í†µí•´ ê°ì§€**</u>í•œë‹¤ê³  í•œë‹¤. (14ì£¼ì°¨ì— ë°°ìš´ë‹¤ê³  í•¨)

---



#### Bad mentions


![4](/assets/img/2023-08-03-CS224n---Lecture-13-(Coreference-Resolution).md/4.png)


ì´ëŠ” ìœ„ì˜ 3ê°€ì§€ ì¡°ê±´ì— ë“¤ì–´ê°€ì§€ë§Œ mentionì´ë¼ê³  ë§í•˜ê¸° ì–´ë µë‹¤.



#### How to deal with these bad mentions?

- ì•ˆì¢‹ì€ mentionì„ ê±°ë¥¼ ìˆ˜ ìˆë„ë¡ classiferë¥¼ í•™ìŠµ ì‹œí‚¨ë‹¤.
- í•˜ì§€ë§Œ classifierë¥¼ í•™ìŠµì‹œí‚¤ëŠ” ë‹¨ê³„ë¥¼ ê±´ë„ˆ ë›¸ ë•Œê°€ ë§ì€ë° ê·¸ ì´ìœ ëŠ” ê·¸ëƒ¥ ì§„í–‰ì‹œì¼œë„ <u>**íŠ¹ì •í•œ entityë¥¼ ë‚˜íƒ€ë‚´ì§€ ì•ŠëŠ” mentionë“¤ì€ í˜¼ìì„œ ë¶„ë¥˜ë˜ê¸° ë•Œë¬¸**</u>ì— ì‹œìŠ¤í…œì— ë‚˜ìœ ì˜í–¥ì„ ë¼ì¹˜ì§€ ì•ŠëŠ”ë‹¤.

---



#### Avoiding a traditional pipeline system


> ğŸ’¡ POS tagger, NER, parserë¥¼ í•˜ë‚˜í•˜ë‚˜ ë”°ë¡œ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, ì´ëŸ¬í•œ ì¼ì„ í•œë²ˆì— í•  ìˆ˜ ìˆëŠ”, mentionì„ ë°œê²¬í•´ë‚´ëŠ” classifierë¥¼ í•™ìŠµì‹œí‚¬ ìˆ˜ ìˆë‚˜?


â†’  <u>**Yes**</u> 
mention detectionê³¼ coreference resolutionì„ 2 ë‹¨ê³„ë¡œ ë‚˜ëˆ„ì§€ ì•Šê³  <u>**end-to-end**</u>ë¡œ í•œë²ˆì— ì§„í–‰í•œë‹¤.


---



#### On to Coreference! First, some linguistics


![5](/assets/img/2023-08-03-CS224n---Lecture-13-(Coreference-Resolution).md/5.png)

- mentionì´ ê°™ì€ entityë¥¼ ë‚˜íƒ€ë‚¸ë‹¤ë©´ <u>**coreference**</u>ë¼ê³  í•œë‹¤.
- ë¬¸ì¥ ì†ì—ì„œ ì•ì— ë‚˜ì˜¨ ë‹¨ì–´ë¥¼ ê°€ë¦¬í‚¤ëŠ” ê²ƒì„ <u>**anaphora**</u>ë¼ê³  í•œë‹¤. (ë’¤ì— ë‚˜ì˜¤ëŠ” ë‹¨ì–´ê°€ ì•ì— ë‚˜ì˜¤ëŠ” ë‹¨ì–´ì— ì˜í•´ì„œ í•´ì„ ë  ë•Œ)

---



#### Anaphora vs. Coreference


![6](/assets/img/2023-08-03-CS224n---Lecture-13-(Coreference-Resolution).md/6.png)


![7](/assets/img/2023-08-03-CS224n---Lecture-13-(Coreference-Resolution).md/7.png)

- ìœ„ ì‚¬ì§„ì˜ ì˜ˆì‹œ ë¬¸ì¥ì—ì„œ concertì™€ ticketsì€ ì„œë¡œ ë‹¤ë¥¸ entityë¥¼ ì´ì•¼ê¸° í•˜ê³  ìˆê¸°ì— coreferenceí•˜ë‹¤ê³  ì´ì•¼ê¸°í•  ìˆ˜ ì—†ì§€ë§Œ, ticketì€ concertì˜ ticketì„ ëœ»í•˜ê¸° ë•Œë¬¸ì—(ticketì˜ ëœ»ì´ ì•ì— ë‚˜ì˜¤ëŠ” concertë¼ëŠ” ë‹¨ì–´ì— ì˜í•´ì„œ í•´ì„ë˜ê¸° ë•Œë¬¸ì—), concertì™€ ticketì€ anaphoricí•œ ê´€ê³„ì— ìˆë‹¤ê³ í•  ìˆ˜ ìˆë‹¤.


#### Cataphora


CataphoraëŠ” Anaphoraì˜ ë°˜ëŒ€ ë§ì´ë‹¤. AnaphoraëŠ” ë’¤ì— ë‚˜ì˜¤ëŠ” ë‹¨ì–´ì˜ ì˜ë¯¸ë¥¼ ì•ì— ë‚˜ì˜¤ëŠ” ë‹¨ì–´ì—ì„œ ì°¾ëŠ”ë‹¤ë©´, cataphoraëŠ” ì•ì—ë‚˜ì˜¤ëŠ” ë‹¨ì–´ì˜ ì˜ë¯¸ë¥¼ ë’¤ì—ì„œ ì°¾ëŠ”ë‹¤.


ìµœê·¼ì—ëŠ” cataphraë¼ëŠ” ê°œë…ì€ ì˜ ì‚¬ìš©ë˜ì§€ ì•ŠëŠ”ë‹¤.



## 4. Hobbsâ€™ naive algorithm : ëŒ€ëª…ì‚¬ì˜ reference ì°¾ê¸°


![8](/assets/img/2023-08-03-CS224n---Lecture-13-(Coreference-Resolution).md/8.png)

- ì—¬ê¸°ì„œ ìš°ë¦¬ëŠ” himì— ëŒ€í•œ referenceë¥¼ ì°¾ê³  ì‹¶ë‹¤.
1. **himì— í•´ë‹¹í•˜ëŠ” NPì—ì„œ ì‹œì‘**í•œë‹¤.
2. treeë¥¼ ê±°ìŠ¬ëŸ¬ ì˜¬ë¼ê°€ì„œ **NPë‚˜ Së¥¼ ì°¾ëŠ”ë‹¤.** (ì´ ì˜ˆì‹œì—ì„œëŠ” Së¥¼ ì°¾ì„ ìˆ˜ ìˆë‹¤)
3. ê·¸ í›„ Së¥¼ ê¸°ì¤€ìœ¼ë¡œ ì™¼ìª½ì—ì„œ ì˜¤ë¥¸ìª½ìœ¼ë¡œ BFSì‹ìœ¼ë¡œ treeë¥¼ í›‘ì–´ ë‚´ë ¤ê°€ê²Œ ë˜ëŠ”ë°, ì´ ë•Œ NPë¥¼ ë°œê²¬í•˜ë©´ ì´ NPëŠ” Sì™€ NP ì‚¬ì´ì— ë˜ ë‹¤ë¥¸ NPë‚˜ Sê°€ ì¡´ì¬í•  ê²½ìš° himì— ëŒ€í•œ reference í›„ë³´ê°€ ë  ìˆ˜ ìˆë‹¤. (ì—¬ê¸°ì„œëŠ” NPì™€ S ì‚¬ì´ì—ì„œ ë˜ ë‹¤ë¥¸ NPë‚˜ Së¥¼ ë°œê²¬í•  ìˆ˜ ì—†ê¸° ë•Œë¬¸ì— himì— ëŒ€í•œ referenceê°€ ë  ìˆ˜ ì—†ë‹¤.)
4. **í•œ ë¬¸ì¥ì„ ë‹¤ ì‚´íˆë‹¤ë©´, ê·¸ ì „ì˜ ë¬¸ì¥ì„ treeë¥¼ BFSë¡œ í›‘ëŠ”ë‹¤. ì´ë•Œ ë°œê²¬ëœ NPëŠ” himì— ëŒ€í•œ reference í›„ë³´**ê°€ ëœë‹¤.

â†’ ì´ ë¬¸ì¥ì—ì„œ himì€ Niall Fergusonì„ ë‚˜íƒ€ëƒ„ì„ ì•Œ ìˆ˜ ìˆë‹¤.



### Hobbsâ€™ algorithmì˜ í•œê³„


![9](/assets/img/2023-08-03-CS224n---Lecture-13-(Coreference-Resolution).md/9.png)


ì´ ë‘ ë¬¸ì¥ì€ ê°™ì€ êµ¬ì¡°ë¥¼ ê°€ì§€ê³  ìˆì§€ë§Œ itì´ ê°€ë¦¬í‚¤ê³  ìˆëŠ”ê²ƒì€ ë‹¤ë¥´ë‹¤. 
ì´ëŸ¬í•œ ê²½ìš°ì—ëŠ” ìœ„ì— ì„¤ëª…í•œ <u>**Hobb's algorithmì„ ì‚¬ìš©í•  ìˆ˜ ì—†ë‹¤.**</u>


---


![10](/assets/img/2023-08-03-CS224n---Lecture-13-(Coreference-Resolution).md/10.png)


<u>**ë¬¸ì¥ì„ ì™¼ìª½ì—ì„œ ì˜¤ë¥¸ìª½ìœ¼ë¡œ í›‘ì„ ê²ƒì¸ë°, ì´ ë•Œ ìƒˆë¡œìš´ mentionì„ ë°œê²¬í•  ë•Œ ë§ˆë‹¤ ì–´ë–¤ coreferenceì¸ì§€ classifyí•œë‹¤.**</u> 


![11](/assets/img/2023-08-03-CS224n---Lecture-13-(Coreference-Resolution).md/11.png)



#### Training


![12](/assets/img/2023-08-03-CS224n---Lecture-13-(Coreference-Resolution).md/12.png)


ìœ„ ìˆ˜ì‹ì„ ë³´ë©´ loss funtionì´ Binary Cross Entropyì´ë‹¤. miì™€ mjê°€ coreferentí•˜ë©´ yijê°€ +1ì´ ë˜ë¯€ë¡œ ê·¸ í™•ë¥ ë§Œí¼ loss functionì´ ì¤„ì–´ë“¤ê³  ì•„ë‹ˆë©´  -1ë¡œ loss functionì´ ê·¸ í™•ë¥ ë§Œí¼ ì¦ê°€í•˜ëŠ” í˜•íƒœì´ë‹¤.  



#### Test


![13](/assets/img/2023-08-03-CS224n---Lecture-13-(Coreference-Resolution).md/13.png)

- mentionë“¤ì„ pairë¡œ ë¬¶ì–´ì„œ classiferì— ì§‘ì–´ ë„£ëŠ”ë‹¤.
- Classifierì˜ ê²°ê³¼ê°€ ì–´ë– í•œ í™•ë¥ (ì„ê³„ê°’?)ì„ ê¸°ì¤€ìœ¼ë¡œ yes ë‚˜ no ê°€ ë‚˜ì˜¬ ê²ƒì´ë‹¤. ì˜ˆë¥¼ë“¤ì–´ 0.5ê°€ ê¸°ì¤€ê°’ì´ë¼ë©´ 0.5ë¥¼ ê¸°ì¤€ìœ¼ë¡œ coreference linkë¥¼ ê²°ì„±í•  ê²ƒì´ë‹¤.
- ì´ mentionë“¤ì„ clustering í•˜ê¸° ìœ„í•´ì„œ transitive closure ë°©ë²•ì„ ì‚¬ìš©í•  ê²ƒì´ë‹¤. Trasitive closureë€ Aê°€ Bì™€ coreferent í•˜ê³  Bê°€ Cë‘ coreferentí•˜ë‹¤ë©´ Aì™€ CëŠ” coreferentí•˜ë‹¤ ë¼ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤.
- transitive closureë¥¼ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— í˜¹ì‹œë‚˜ ì‹¤ìˆ˜ê°€ ì¼ì–´ë‚˜ë©´ ëª¨ë“  mentionì´ í•˜ë‚˜ë¡œ ë¬¶ì¼ ìˆ˜ ìˆëŠ” (over cluster) ìœ„í—˜ì„±ë„ ì¡´ì¬í•œë‹¤.
- coreferentë¥¼ ì´ë£¨ì§€ ì•ŠëŠ” mentionë„ ì¡´ì¬í•œë‹¤. ê·¸ë ‡ë‹¤ë©´ classifierì—ì„œëŠ” ëª¨ë“  ë‹¤ë¥¸ mentionì— ëŒ€í•´ì„œ no ê°’ì„ ë°°ì¶œí•  ê²ƒì´ê³  ì´ mentionì€ singleton mentionì´ ë  ê²ƒì´ë‹¤.


#### Disadvantage


![14](/assets/img/2023-08-03-CS224n---Lecture-13-(Coreference-Resolution).md/14.png)


ë§Œì•½ mentionì„ ë§ì´ í¬í•¨í•˜ëŠ” ê¸´ ë¬¸ì„œê°€ ìˆìœ¼ë©´, í•´ë‹¹í•˜ëŠ” mentionì„ ëª¨ë‘ ì°¾ì•„ë‚´ì„œ yesë¼ëŠ” ê°’ì„ ë°°ì¶œí•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ í•œ mentionì„ ì˜ í‘œí˜„í•˜ëŠ” íŠ¹ì •í•œ mention í•˜ë‚˜ë¥¼ ì°¾ì•„ë‚´ê³  ì‹¶ë‹¤.


---



## 6. Coreference Models: Mention Ranking


![15](/assets/img/2023-08-03-CS224n---Lecture-13-(Coreference-Resolution).md/15.png)


ë§ ê·¸ëŒ€ë¡œ sheê°€ ì–´ë–¤ mentionê³¼ coreferenceí•œì§€ ë³´ê³  ì‹¶ì„ë•Œ <u>**ì—¬ëŸ¬ mention í›„ë³´ë“¤ì„ ranking**</u>í•œë‹¤ê³  í•˜ì—¬ Mention Rankingì´ë‹¤.


coreferentí•œ mentionì´ ì—†ì„ ê²½ìš°ê°€ ìˆì„í…ë°, ì´ ë•Œë¥¼ ìœ„í•´ì„œ NAê°’ì„ í¬í•¨ì‹œí‚¨ë‹¤

- Sheì™€ ë‹¤ë¥¸ mentionì— ëŒ€í•œ pairì— softmaxë¥¼ ì ìš©ì‹œí‚¨ë‹¤. ì´ ë•Œ softmaxë¥¼ ì ìš©ì‹œì¼œì„œ ë‚˜ì˜¨ ê°’ë“¤ì˜ í•©ì€ 1ì´ ë  ê²ƒì´ë‹¤. ì´ë•Œ ìš°ë¦¬ê°€ ì›í•˜ëŠ” ê²ƒì€ ì–´ë– í•œ antecedentsì— ëŒ€í•´ì„œ ë†’ì€ í™•ë¥ ê°’ì„ ê°€ì§€ê²Œ ë˜ëŠ” ê²ƒì´ë‹¤. (prior reference ê°€ ìˆë‹¤ë©´ antecedentì— ëŒ€í•´ì„œ ë†’ì€ í™•ë¥ ê°’ì„ ì–»ì„ ê²ƒì´ê³ , ì—†ë‹¤ë©´ NAì™€ ë†’ì€ í™•ë¥ ê°’ì„ ì–»ì„ ê²ƒì´ë‹¤.)
- ê·¸ë¦¬ê³  ê°€ì¥ ë†’ì€ ê°’ì„ ê°€ì§„ ê²ƒì— í•œí•˜ì—¬ coreference linkë¥¼ ë§Œë“ ë‹¤.


#### Training


![16](/assets/img/2023-08-03-CS224n---Lecture-13-(Coreference-Resolution).md/16.png)


í•˜ë‚˜ì˜ referenceì— ëŒ€í•´ í™•ë¥ ì„ ëª¨ë‘ ë”í•œ í›„ negative logë¥¼ ì·¨í•œ í›„ documentì— ìˆëŠ” ëª¨ë“  mentionì„ ë”í•œë‹¤.



#### How do we compute the probabilites?


1) Non-neural statistical classifier


2) Simple neural network


3) More advanced model using LSTMs, attention, transformers



### 1) Non-neural statistical classifier


![17](/assets/img/2023-08-03-CS224n---Lecture-13-(Coreference-Resolution).md/17.png)

- You have whole bunch of features and you had a feature based statistical classifier which gave a score. And the above are the features that we could use.
- Throw these all features into a statistical classifier and that's sort of 2000s decade coref systems as to how they're built.


### 2) Simple neural network


![18](/assets/img/2023-08-03-CS224n---Lecture-13-(Coreference-Resolution).md/18.png)


Word embeddingsì™€ categorical featuresë¥¼ inputê°’ìœ¼ë¡œ í•˜ëŠ” neural networkë¥¼ êµ¬ì„±í•´ì„œ scoreê°’ì„ êµ¬í•œë‹¤.



## 7. End-to-end


![19](/assets/img/2023-08-03-CS224n---Lecture-13-(Coreference-Resolution).md/19.png)


![20](/assets/img/2023-08-03-CS224n---Lecture-13-(Coreference-Resolution).md/20.png)


![21](/assets/img/2023-08-03-CS224n---Lecture-13-(Coreference-Resolution).md/21.png)



#### Performance


![22](/assets/img/2023-08-03-CS224n---Lecture-13-(Coreference-Resolution).md/22.png)

