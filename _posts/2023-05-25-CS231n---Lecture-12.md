---
layout: single
date: 2023-05-25
title: "CS231n - Lecture 12"
use_math: true
author_profile: false
tags: [강의/책 정리, ]
categories: [AI, ]
---


## What’s in CNN?


→ 지금까지 CNN은 black box로 여겨졌음


→ conv layer 거치면서 다양한 변환 거치는데, 우리가 지금까지 이해하고 해석할 수 있었던 것은 class score, bbox, labeled pixel 등 최종 결과물


그렇다면

- CNN의 내부는 어떻게 생겼을까?
- 어떤 종류의 feature를 찾는 걸까?
- 어떻게 동작할까? 이미지에서 어떤 종류의 것들을 찾고 있을까?
- CNN 내부 분석하려면 어떤 테크닉 필요할까?


### Layer별로 뜯어보자!



#### AlexNet



#### 첫번째 Layer


![0](/assets/img/2023-05-25-CS231n---Lecture-12.md/0.png)_PyTorch model Zoo_


첫 conv layer에는 많은 필터 (3 x 11 x 11) 존재해


sliding window로 이미지를 돌면서 이미지의 일부 영역과 직접 내적을 수행하는데,


→ 이 필터를 시각화 시키면, 필터가 이미지에서 무엇을 찾는지 알 수 있음 


→ 11 x 11 x 3 rgb 이미지로 시각화 (0-255 사이로 normalize해서)


(CNN Layer의 학습된 가중치)


찾는것?

- edge 성분 (Oriented edges 감지)
- 다양한 각도와 위치에서의 보색

CNN Layer의 학습된 가중치 → 시각화가 무엇 찾는지 알 수 있나?

- template vector
- 내적 최대화 → 입력 값과 필터의 가중치 값이 동일한 경우


#### 중간 Layer


![1](/assets/img/2023-05-25-CS231n---Lecture-12.md/1.png)


→ 직접적으로 이미지의 형태로 시각화시킬 수 없다 


→ 시도는 해봤지만 무슨 이미지인지 알 수 없었음


(이미지와 직접 연결이 아닌 경우)


⇒ 더 fancy technique 필요



#### Last Layer


![2](/assets/img/2023-05-25-CS231n---Lecture-12.md/2.png)


→ 마지막 layer 직전 fully connected layer


→ AlexNet의 마지막 layer는 이미지를 표현하는 4096-dim 특징벡터를 입력으로 최종 class scores를 출력함


→ nearest neighbor 이용해서 4096-dim 특징 벡터 공간 상에서 유사도가 높은 이미지들을 찾아보자


	(거리가 가까운 이미지들을 시각화해서 보는 방법도 시각화 기법이 될 수 있음)


![3](/assets/img/2023-05-25-CS231n---Lecture-12.md/3.png)

- 제일 왼쪽 열: test image
- 나머지 사진들: AlexNet의 4096-dim 특징 벡터에서 계산한 nearest neighbor 결과

→ 서로 픽셀 값의 차이가 큰 경우도 있음


→ 픽셀 값의 차이 커도 특징 공간 내에서는 아주 유사한 특성 지님


이미지의 **semantic content한 특징들을 잘 찾아냈다**고 할 수 있음 

- Question?
	- loss를 통해 가까워지도록 학습하는 건가?

	→ supervised에서 그런 것 없지만, 사람들은 


		triplet/contrastive loss 등을 추가해 학습하기도 함 


		특정 공간에서의 계산이 가능하도록 가정과 제약조건을 마지막 레이어에 추가 


		(AlexNet엔 X)

- 차원축소 관점

	![4](/assets/img/2023-05-25-CS231n---Lecture-12.md/4.png)_MNIST t-SNE dimensionality reduction → 2-dim → 시각화_


	→ powerful한 algorithm인 **t-SNE** 이용


	→ 특징 공간 시각화를 위해 PCA보다 많이 사용함


	→ **clustering됨**


	이 기법을 CNN 네트워크의 마지막 레이어에도 적용하는 것 


	![5](/assets/img/2023-05-25-CS231n---Lecture-12.md/5.png)


	각 grid에 압축된 2-dim 특징들이 시각화됨


	[bookmark](https://cs.stanford.edu/people/karpathy/cnnembed/)


	**→ 불연속적인 의미론적 개념 (semantic notion)이 존재**

	- question

		한 이미지당 서로 다른 세 가지 정보 존재

		1. 픽셀로 된 원본 이미지
		2. 4096-dim 벡터
		3. t-SNE 이용 변환 값

		차원축소 과정에서 겹치지 않나?


		→ 겹치기 때문에, **nn 적용 시에 regular grid 기준 가장 가까운 이미지를 뽑음** 


		→ 따라서, **density를 보여주는 것은 아님**



#### Visualizing Activations


![6](/assets/img/2023-05-25-CS231n---Lecture-12.md/6.png)

- 까만 것은 Dead ReLU (특정 입력에 대해서 활성화 x)
- imagenet에 사람이 없는데도 사람을 인식한 것..!
- 3x224x224 입력

	각 레이어의 3차원 반환 값 (wxhxd)을 activation chunk라고 함 


	이걸 depth 별로 잘라내면 **activation map**



#### Maximally Activating Patches


![7](/assets/img/2023-05-25-CS231n---Lecture-12.md/7.png)


→ 어떤 이미지가 들어와야 각 뉴런들의 활성이 최대화되는지 시각화


	ex. conv5는 128x13x13 한 덩어리의 activation volume 갖음 


	이 중 **특정 channel (ex. 17번째)을 골라** 놓고, 이미지들을 통과시켜 conv5 features를 기록하고, **특정 channel의 feature map을 최대로 활성화하는지**를 봄


	→ 이는 이미지의 일부만을 보기 때문에, **특정 레이어의 특징을 최대화시키는 이미지의 일부분을 시각화** 함 (우측)


	특정 레이어의 활성화 정도를 기준으로 패치들을 정렬시키면 됨


	한 row에 있는 패치들이 하나의 뉴런(conv5 activation map의 하나의 scalar 값)에서 나온 것


	(한 채널 안의 모든 뉴런들은 모두 같은 가중치를 공유함)


	(각 채널 당 하나의 conv filter, 이에 상응하는 많은 뉴런 (activation map))


	(앞서 나온 patch들을 이미지의 모든 곳에서 추출할 수 있음)


	각 패치들은 데이터셋에서 나온 패치들을 정렬한 값들 


	이 패치들이 활성화 최대로 함


	동일 네트워크의 더 깊은 레이어에서 뉴런들을 최대로 활성화시키는 패치들


	더 깊은 레이어에서 와서 receptive field가 훨씬 넓음


	→ 훨씬 더 큰 patch들을 기준으로 찾음


	→ 입력 이미지에서 더 큰 구조(structures)들을 찾고 있음



#### Occlusion Experiments 


![8](/assets/img/2023-05-25-CS231n---Lecture-12.md/8.png)


입력의 어떤 부분이 분류를 결정짓는 근거가 되는지


가린 부분을 데이터셋의 평균 값으로 채워버린 후 네트워크에 통과시켜, 네트워크가 이미지를 예측한 확률 기록 


이 가림 패치를 전체 이미지에 대해 돌아가면서 같은 과정 반복 


(feature의 중요도를 알아내기 위해, 특정 feature 제외하고 머신러닝 진행해 evaluate 하는 것과 비슷한 느낌인듯)


![9](/assets/img/2023-05-25-CS231n---Lecture-12.md/9.png)


히트맵은 이미지를 가린 patch의 위치에 따른 네트워크의 예측 확률의 변화 의미


⇒ 이미지의 일부를 가렸는데 네트워크 스코어 변화 크게 발생하면 그 부분이 중요한 부분


⇒ Red → 중요, Yellow → 안 중요


⇒ 사람이 네트워크가 무엇을 하고 있는지에 대한 이해를 돕는 것



#### Saliency map


![10](/assets/img/2023-05-25-CS231n---Lecture-12.md/10.png)


Saliency map 


입력 이미지의 각 픽셀들에 대해 예측한 클래스 스코어의 그래디언트를 계산


1차 근사적 방법 → 어떤 픽셀이 영향력 있는지를 알려줌


입력 이미지의 각 픽셀에 대해서, 우리가 그 픽셀을 조금 바꿨을 때 클래스 스코어가 얼마나 바뀔지?


어떤 픽셀이 분류하는 데 있어서 어떤 픽셀들이 필요한 지를 알 수 있는 또 다른 방법이 될 수 있음 → 윤곽이 나타남


⇒ 이를 semantic segmentation에 활용할 수도 있음


(saliency maps + Grabcut) ⇒ semantic segmentation 수행 가능


(나중에 알아보기)


![11](/assets/img/2023-05-25-CS231n---Lecture-12.md/11.png)


하지만 성능은 좋지 않았음


but 작동은 함



#### guided back propagation


![12](/assets/img/2023-05-25-CS231n---Lecture-12.md/12.png)

- 네트워크의 **중간 뉴런**을 골라, **입력 이미지의 어떤 부분이, 내가 선택한 중간 뉴런의 값에 영향**을 주는지

	⇒ saliency map을 만듦


	각 픽셀에 대한 class score의 gradient 계산해 어떤 픽셀이 해당 뉴런에 영향을 주는지 앎.

- back propagation 과정에서 조금의 트릭

	**→ guided back propagation :** 조금 더 깨끗한 이미지 얻을 수 있음 


	![13](/assets/img/2023-05-25-CS231n---Lecture-12.md/13.png)


	backprop시 ReLU 통과할 때 조금의 변형을 가함


	양의 부호 gradient만 고려 x → 양의 부호인 gradient만 고려?


	⇒ 직접 보래요…


![14](/assets/img/2023-05-25-CS231n---Lecture-12.md/14.png)


→ guided backprop을 통해 더 선명하고 좋은 이미지를 얻을 수 있음



#### Gradient Ascent


![15](/assets/img/2023-05-25-CS231n---Lecture-12.md/15.png)


입력 이미지 의존적이지 않은 방법 → Gradient Ascent

- network의 가중치들을 전부 고정시킴 (즉, 가중치를 최적화하는 것이 아님)

	gradient ascent들을 통해 중간 뉴런 혹은 클래스 스코어를 최대화 시키는 이미지의 픽셀들을 만들어 냄 (입력 이미지의 픽셀 값을 바꿔서)

- regularization term이 필요

	생성된 이미지가 특정 네트워크의 특성에 overfit 되는 것 방지 

	- 특정 뉴런의 값을 최대화시키는 방향으로 생성되도록
	- **이미지가 자연스럽게 보이도록** (이 목적에 가까움)

![16](/assets/img/2023-05-25-CS231n---Lecture-12.md/16.png)

1. 이미지 초기화
2. 초기화를 하고 나면 이미지를 네트워크에 통과시키고 관심있는 뉴런의 스코어 계산
3. 이미지의 각 픽셀에 대한 뉴런 스코어의 gradient 계산해 back prop (gradient update는 x)
4. gradient ascent 이용 이미지 픽셀 자체를 업데이트 (스코어 최대화)

![17](/assets/img/2023-05-25-CS231n---Lecture-12.md/17.png)


L2 norm을 이용하는데, 큰 의미가 있는 것은 아님 (그 당시에 많이 사용하던 regularizer)


![18](/assets/img/2023-05-25-CS231n---Lecture-12.md/18.png)_결과물 1_


![19](/assets/img/2023-05-25-CS231n---Lecture-12.md/19.png)_결과물 2_

- multimodality를 다루는 다른 방법은 없는지?

	클래스 내에서도 다양한 mode가 있음


![20](/assets/img/2023-05-25-CS231n---Lecture-12.md/20.png)


regularizer를 더욱 향상시키고 이미지를 더 잘 시각화시키는 다른 방법

- L2 norm constraint 사용
- 최적화 과정에 이미지에 주기적으로 가우시안 블러 적용

	(중앙값에 가중치를 더 주고 주변은 더 흐리게 한다고 보면된다.)

- 주기적으로 값이 작은 픽셀들은 모두 0으로 만듦
- 낮은 기울기들의 pixel들을 0으로 만듦

	→ projected Gradient Descent


	(생성된 이미지를 더 좋은 특성을 가진 이미지 집합으로 주기적 매핑)


**중간의 뉴런에 적용한다면?**


![21](/assets/img/2023-05-25-CS231n---Lecture-12.md/21.png)


→ 이미지가 클수록, receptive fields가 더 큰 뉴런 (즉 뒤쪽의 뉴런)


→ 더 큰 구조와 복잡한 패턴 찾음


클래스마다 클러스터링


→ 한 클래스 내 서로 다른 모드들끼리 다시 한번 클래스 나뉨


→ 나뉜 모드들과 가까운 곳으로 초기화 해 줌 


이 부분은 multimodality 관련 잘 모르겠음


이렇게 하면 multimodality를 명시할 수 있다고 함 


![22](/assets/img/2023-05-25-CS231n---Lecture-12.md/22.png)


cf) 사전 정보 (prior)를 이용하는 방법


	![23](/assets/img/2023-05-25-CS231n---Lecture-12.md/23.png)


	FC6을 optimize


	이처럼 prior를 추가하면 아주 real한 이미지를 만들 수 있음



#### Fooling Images


![24](/assets/img/2023-05-25-CS231n---Lecture-12.md/24.png)


ex. 코끼리 이미지를 코알라라고 분류하도록 이미지를 조금씩 바꾸기


![25](/assets/img/2023-05-25-CS231n---Lecture-12.md/25.png)


Q. 중간 뉴런을 이해하는 것이 어떻게 최종 클래스 분류를 이해하는 데 도움을 줄 수 있는지?


→ 요약하면, 설명 가능함(해석 가능함)을 돕기 때문?


→ 그래서 시각화가 등장



#### DeepDream


![26](/assets/img/2023-05-25-CS231n---Lecture-12.md/26.png)

1. 모델이 이미지의 어떤 특징들을 찾고 있는 지를 짐작
2. 해당 레이어의 그래디언트를 activation 값으로 설정
3. back prob을 통해 이미지 업데이트

	→ 네트워크에 의해 검출된 해당 이미지의 특징들을 증폭하는 걸로 해석 가능


⇒ 이 과정들은 해당 레이어에서 나온 특징들의 L2 norm을 최대화하는 것으로 볼 수 있다.


![27](/assets/img/2023-05-25-CS231n---Lecture-12.md/27.png)

- Jitter Image

	이미지를 두 픽셀 정도 이동시킴


	regularizer 역할 → 자연스럽고 부드러운 이미지

- L1 Normalize gradients

	이미지 합성에 유용

- Clip pixel values

	0-255 사이에 있도록 clipping (projected gradient descent)


![28](/assets/img/2023-05-25-CS231n---Lecture-12.md/28.png)


![29](/assets/img/2023-05-25-CS231n---Lecture-12.md/29.png)


![30](/assets/img/2023-05-25-CS231n---Lecture-12.md/30.png)_개가 많이 등장하는 것은 개 category 가 많기 때문!_


![31](/assets/img/2023-05-25-CS231n---Lecture-12.md/31.png)_얕은 층의 레이어로 만든 경우 (엣지, 소용돌이 등이 보임)_


![32](/assets/img/2023-05-25-CS231n---Lecture-12.md/32.png)_멀티 스케일 프로세싱 수행 → 작은 이미지에 deep dream 수행 후 점점 이미지 크기 늘려나감(이를 반복)_


![33](/assets/img/2023-05-25-CS231n---Lecture-12.md/33.png)_위는 ImageNet, 아래는 MIT place의 경우_


Q. gradient 어디서 가져오나?


1/(x^2)가 있을 때, activation에 대한 gradient는 x


activation 값 자체를 보내게 되면, 1/(x^2)의 gradient를 계산하는 것과 동치 


이는 해당 레이어의 특징들(activations)의 norm을 최대화 하는 것과 동치


대부분의 실제 구현은 이를 명시적 계산 않고, gradient만 뒤로 보냄



#### Feature Inversion


![34](/assets/img/2023-05-25-CS231n---Lecture-12.md/34.png)

- 네트워크의 다양한 레이어에서 이미지의 어떤 요소들을 포착하고 있는 지를 짐작할 수 있음
- activation을 통해 이미지를 재구성한다.

	(이미지의 어떤 정보가 특정 벡터에서 포착되는지를 짐작할 수 있음)

- regularizer를 추가한 gradient ascent 이용함
- 스코어의 최대화 대신 특징 벡터간의 거리를 최소화시키는 방법을 이용함

	(기존에 계산한 특징 벡터와 새롭게 생성한 이미지로 계산한 특징벡터 간의 거리를 측정하는 것)

- total variation regularizer 이용

	상하좌우 인접 픽셀 간의 차이에 대한 페널티 부여


	→ 생성된 이미지가 자연스러운 이미지가 되도록 함 


	(아마, 차이가 크면 더 큰 페널티 부여되도록 한 듯; 픽셀 간 차이가 적어져 자연스러워짐)


![35](/assets/img/2023-05-25-CS231n---Lecture-12.md/35.png)

- VGG-16의 relu2_2 거쳐 나온 특징벡터로 이미지 재구성 시 2번째와 같음 (거의 완벽함)

	즉, relu2_2 에선 거의 이미지 정보를 날리지 않음

- 네트워크의 조금 더 깊은 곳을 보면 공간적 구조는 유지되지만 디테일은 사라짐

	디테일들은 깊어질수록 많이 없어짐

- 제일 깊어질수록, 의미론적 정보들만 유지하는 것처럼 보이기도 함

⇒ 끝판왕 style transfer



#### Texture Synthesis


![36](/assets/img/2023-05-25-CS231n---Lecture-12.md/36.png)


![37](/assets/img/2023-05-25-CS231n---Lecture-12.md/37.png)


→ scan line 따라서 한 픽셀씩 이미지 생성 (신경망 x)


→ 생성해야 할 픽셀 주변의 이미 생성된 픽셀 살펴봄 


	가장 가까운 픽셀 계산해 복사해 넣음


![38](/assets/img/2023-05-25-CS231n---Lecture-12.md/38.png)


복잡한 문제에 대한 texture synthesis를 해결하기 위해 신경망을 도입하려는 시도 나옴



#### Gram Matrix


![39](/assets/img/2023-05-25-CS231n---Lecture-12.md/39.png)


→ 특정 특징 벡터는 해당 지점의 이미지 특징을 담음


→ 특정 맵을 가지고 입력 이미지의 텍스트 기술자를 계산함


![40](/assets/img/2023-05-25-CS231n---Lecture-12.md/40.png)


→ 벡터들의 외적을 계산해 CxC 행렬 만듦


	이미지 내 서로 다른 두 지점에 있는 특징들 간의 co-occurence 담음


	CxC 행렬의 (i,j) 번째 요소의 값이 크다는 것은 두 입력 벡터의 i번째, j번째 요소가 모두 크다는 뜻임


→ 서로 다른 공간에서 동시에 활성화되는 특징이 무엇인지 2차 모멘트를 통해 어느 정도 포착해 낼 수 있음


![41](/assets/img/2023-05-25-CS231n---Lecture-12.md/41.png)


전부 수행해 결과에 대한 평균을 계산하면, CxC Gram matrix를 얻을 수 있게 됨


이 결과를 입력 이미지의 텍스처를 기술하는 텍스처 기술자로 사용함


⇒ 공간 정보를 모두 날렸다는 것이 흥미로움 (평균)


대신, 특징들 간의 co-occurence 만을 포착해 냄


cost적인 면에서 아주 좋음


![42](/assets/img/2023-05-25-CS231n---Lecture-12.md/42.png)

- 이미지 생성
- pretrained model 다운(VGG)
- 다양한 레이어에서 gram matrix 계산해서,
- 생성할 이미지 랜덤 초기화 후 gradient ascent
	- VGG 통과
	- gram matrix 계산
	- 원본과 생성 간 gram matrix 간 차이를 L2 norm 이용해 Loss로 계산함
	- back prob 통해 생성된 이미지의 픽셀의 gradient 계산해, gradient ascent로 조금씩 업데이트

		**이를 반복함**


![43](/assets/img/2023-05-25-CS231n---Lecture-12.md/43.png)


![44](/assets/img/2023-05-25-CS231n---Lecture-12.md/44.png)


![45](/assets/img/2023-05-25-CS231n---Lecture-12.md/45.png)

