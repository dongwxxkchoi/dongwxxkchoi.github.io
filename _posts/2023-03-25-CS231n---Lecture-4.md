---
layout: single
date: 2023-03-25
title: "CS231n - Lecture 4"
use_math: true
tags: [강의/책 정리, ]
categories: [AI, ]
---


### Computational Graphs


![0](/assets/img/2023-03-25-CS231n---Lecture-4.md/0.png)

- Linear Classification에 사용하는 weight 값을 평가하기 위한 `Loss Function`
- weight 값의 최적화를 위한 Gradient Descent(경사 하강법)과 같은 `Optimization`
- 여러 개의 Linear Classification을 사용할 경우 `Gradient` 값을 어떻게 찾을지 알아보자
- Computational Graph를 사용해서 함수를 표현함으로써 **`Backpropagation`**이라고 부르는 기술을 사용할 수 있다


### Backpropagation


![1](/assets/img/2023-03-25-CS231n---Lecture-4.md/1.png)


$$
q=x+y\\ \frac{\partial q}{\partial x}=1, \frac{\partial q}{\partial y}=1
$$


$$
q=x+y\\ \frac{\partial q}{\partial x}=1, \frac{\partial q}{\partial y}=1
$$


$$
f=qz\\ \frac{\partial f}{\partial q}=z, \frac{\partial f}{\partial z}=q
$$

- `Chain Rule`을 이용하여 Backpropagation을 구할 수 있다

![2](/assets/img/2023-03-25-CS231n---Lecture-4.md/2.png)

- `x`와 `y` 값을 입력으로 받고 `f`라는 함수에 집어넣어서 `z`라는 값을 얻는다
- z에서 x에 대한 Gradient 값 y에 대한 Gradient 값을 얻을 수 있다
- z를 끝으로 L(Loss)가 계산됐다고 하면, 우리는 먼저 마지막에서부터 L을 z로 먼저 미분한다
- L을 x, y로 미분한 값을 구할때면, 아까 구해놨던 z를 x, y로 미분한 값과 L을 z로 미분한 값을 곱하는 `Chain Rule`을 이용하여 구한다
- 이렇게 입력받고 Loss를 구하는 계산 과정을 `Forward Pass`라고 하고, 역으로 미분해가며 기울기 값을 구하는 과정은 `Back Pass`라고 한다


#### Example 1.


![3](/assets/img/2023-03-25-CS231n---Lecture-4.md/3.png)


![4](/assets/img/2023-03-25-CS231n---Lecture-4.md/4.png)

- Sigmoid Gate의 연산 부분은 Sigmoid 함수와 같은 형태이다
- Sigmoid 함수의 미분 결과는 $(1-sigmoid(x)) \times sigmoid(x)$와 같은 형태이다
- $Loss=(1 -0.73) \times0.73=0.2$의 값을 가지는 것을 볼 수 있다
- 이렇게 Chain Rule을 적용하면 모든 노드에 대해 한번에 미분할 수도 있다. 하지만 너무 복잡하기 때문에 그렇게 하지는 않고, Sigmoid와 같이 자주 사용하는 부분만 쉽게 계산하도록 한다


#### Patterns in Backward Flow

- add gate : 이미 가지고 있던 Gradient를 `각각의 노드로 분배` 해준다
- max gate : 더 `큰 쪽에만 Gradient를 전달`하고 `작은 쪽은 0 값`을 준다
- mul gate : 현재의 Gradient를 `각각 숫자에 곱해서` 바꿔주면 된다


### Gradients add at branches


![5](/assets/img/2023-03-25-CS231n---Lecture-4.md/5.png)

- 노드 하나에서 다른 노드 두 개로 모두 이어졌을 때, Backpropagation을 수행하면 저 뒤의 두 개의 노드에서 오는 미분 값을 더해야한다
- 앞의 노드 하나만 바뀌어도 뒤의 두 개의 노드가 모두 바뀐다


### Gradient for Vectorized Code


![6](/assets/img/2023-03-25-CS231n---Lecture-4.md/6.png)

- 지금까지 한 변수 값에 대한 Gradient를 계산했다
- 우리가 실제로 사용하는 입력값은 `Scaler 값`이 아닌 `Vector 값`이다
- Vector를 사용하기 위해서는 **다변수 벡터 함수(Vector-Valued Function of Multiple Variables)에 대한 일차미분값**인 **`Jacobian Matrix`**가 필요하다


#### Example 1.


![7](/assets/img/2023-03-25-CS231n---Lecture-4.md/7.png)

- 4096차원의 Input Matrix와 같은 경우, Jacobian Matrix는 `[4096 X 4096]`의 크기를 가진다
- 추가로 Minibatch를 100으로 한다면 Jacobian Matrix는 `[409600 X 409600]`의 크기를 가진다
- Jacobian Matrix는 Input Matrix의 길이를 변으로 가지는 정방행렬이다
- 입력의 각 요소, 첫 번째 차원은 오직 출력의 해당 요소에만 영향을 주기 때문에 Jacobian Matrix는 `대각행렬`이다


#### Example 2.      $f(x, W)=||W \cdot x||^2=\sum_{i=1}^n(W \cdot x)^2_i$


![8](/assets/img/2023-03-25-CS231n---Lecture-4.md/8.png)

- Linear Classifier 이후에 L2 Regularization으로 Loss를 사용하자고 한다
- L2에 입력 `q=Wx` 라고 할 때, `L2의 Gradient 값은 2q`임을 알 수 있다

![9](/assets/img/2023-03-25-CS231n---Lecture-4.md/9.png)

- 이후 W에 대한 Jacobian Matrix 값과 앞에서 L2에 대해 계산한 결과를 Chain Rule을 이용하여 계산한다

![10](/assets/img/2023-03-25-CS231n---Lecture-4.md/10.png)

- 이후 x에 대한 Jacobian Matrix 값과 앞에서 L2에 대해 계산한 결과를 Chain Rule을 이용하여 계산한다


### Modularized Impolemetation



{% raw %}
```python
class ComputationalGraph(object):
	#...
	def forward(inputs):
		# 1. [pass inputs to input gates ...]
		# 2. forward the computational graph:
		for gate in self.graph.nodes_topologically_sorted():
			gate.forward()
		return loss # the final gate in the graph outputs the loss

	def backward():
		for gate in reversed(self.graph.nodes_topologically_sorted()):
			gate.backward() # little piece of backprop (chain rule applied)
		return inputs_gradients
```
{% endraw %}


- Forward Pass에서는 노드의 출력을 계산하는 함수를 구현하고 Backward pass에서는 Gradient를 계산한다
- 각 노드를 Local하게 보고 Upstream Gradient와 함께 Chain Rule을 이용해서 Local Gradient를 구한다
- Forward의 값은 이후에 Backward Pass에서 사용하기 때문에 저장해야 한다


#### MultiplyGate



{% raw %}
```python
class MultiplyGate(object):
	def forward(x, y):
		z = x * y
		self.x = x
		self.y = y
		return z
	
	def backward(dz):
		dx = self.y * dz # [dL/dz * dz/dx]
		dy = self.x * dz # [dL/dz * dz/dy]
		return [dx, dy]
```
{% endraw %}



[link_preview](https://github.com/intel/caffe)


---



### Neural Networks



#### without the brain stuff


![11](/assets/img/2023-03-25-CS231n---Lecture-4.md/11.png)

- Neural Networks (인공 신경망)은 앞에서 배운 Linear Classifier을 2개 이상 쌓아올리는 형태이다
- ‘W라는 레이터 하나만을 지나는 것이 아니고, 다른 W 두 개를 지나면 더 정확해지지 않을까?’
- 여기서 중요한 부분은 Non-Linear Function을 사용해야 한다는 점이다

	→ Non-Linear Function 식이 중간에 끼어들어가지 않으면, 아무리 많은 Layer를 합쳐도 결국 하나의 레이어와 같은 결과를 낸다



{% raw %}
```python
import numpy as np
from numpy.random import randn

N, D_in, H, D_out = 64, 1000, 100, 10
x, y = randn(N, D_in), randn(N, D_out)
w1, w2 = randn(D_in, H), randn(H, D_out)

for t in range(2000):
	h = 1 / (1 + np.exp(-x.dot(w1)))
	y_pred = h.dot(w2)
	loss = np.square(y_pred - y).sum()
	print(t, loss)

	grad_y_pred = 2.0 * (y_pred - y)
	grad_w2 = h.T.dot(grad_y_pred)
	grad_h = grad_y_pred.dot(w2.T)
	grad_w1 = x.T.dot(grad_h * h * (1 - h))

	w1 -= 1e-4 * grad_w1
	w2 -= 1e-4 * grad_w2
```
{% endraw %}




#### with the brain stuff


![12](/assets/img/2023-03-25-CS231n---Lecture-4.md/12.png)



{% raw %}
```python
class NeuronL
	# ...
	def neuron_tick(inputs):
		""" assume inputs and weights are 1-D numpy arrays and bias is a number """
		cell_body_sum = np.sum(inputs * self.weights) + self.bias
		firing_rate = 1.0 / (1.0 + math.exp(-cell_body_sum)) # sigmoid activation function
		return firing_rate
```
{% endraw %}


- Neural Network는 뉴런의 구조와 Computational Graph의 구조의 유사성을 따온 구조이다
- Dendrites(수상돌기)는 실제로 복잡한 비선형 계산을 수행할 수 있다
- Synapses(시냅스)는 W 값처럼 단일 가중치가 아니며, 실제로 엄청 복잡한 비선형 시스템이다


### Activation Functions


![13](/assets/img/2023-03-25-CS231n---Lecture-4.md/13.png)



### Neural Networks : Architectures


![14](/assets/img/2023-03-25-CS231n---Lecture-4.md/14.png)

- 중간 중간에 모든 노드가 다음의 모든 노드에 영향을 끼치는 레이어를 `Fully-Connected Layer`라고 한다
- Neural Networks의 예에서도, 모든 W값들이 다음 값들에 영향을 미쳤으니 `Fully-Connected Layer`라고 볼 수 있다


{% raw %}
```python
# forward-pass of a 3-layer neural network:
f = lambda x: 1.0/(1.0 + np.exp(-x)) # activation function (use sigmoid)
x = np.random.randn(3, 1) # random input vector of three numbers (3X1)
h1 = f(np.dot(W1, x) + b1) # calculate first hidden layer activations (4X1)
h2 = f(np.dot(W2, h1) + b2) # calculate second hidden layer activations (4X1)
out = np.dot(W3, h2) + b3 # output neuron (1X1)
```
{% endraw %}


