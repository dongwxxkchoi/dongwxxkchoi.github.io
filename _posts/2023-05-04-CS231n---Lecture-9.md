---
layout: single
date: 2023-05-04
title: "CS231n - Lecture 9"
use_math: true
author_profile: false
tags: [강의/책 정리, ]
categories: [AI, ]
---


### LeNet-5


![0](/assets/img/2023-05-04-CS231n---Lecture-9.md/0.png)

- 산업에 성공적으로 적용된 최초의 Convolutional Network이다
- 이미지를 Input으로 받아서 Stride = 1인 5x5 필터를 거치고 Conv-Layer와 Pooling-Layer를 거쳐 마지막에 FC-Layer를 거친다


### AlexNet

- 2012년에 ImageNet Classification에서 우승한 모델이고, 최초의 Large Scale CNN이다
- 기본적으로 Conv - Max Pool - Normalization 구조가 두 번 반복되고 뒤에 Conv가 조금 더 붙고 그 뒤에 Max Pool이 있다. 마지막엔 FC-Layer가 몇 개 붙는다
- 5개의 Conv Layer와 2개의 FC-layer로 구성됩니다.
- 딥러닝 모델에서 아주 보편적으로 사용되는 ReLU를 처음 사용했다
- 딥러닝 모델 학습에 유용한 기법인 Data Augmentation을 엄청 사용했다
- 메모리가 3GB인 GTX 580 2개를 사용해서 Feature Map을 반씩 분산시켰다

![1](/assets/img/2023-05-04-CS231n---Lecture-9.md/1.png)



{% raw %}
```python
[227 X 227 X   3] Input Image
[ 55 X  55 X  96] CONV1 : 11 X 11 X 96(Stride = 4, Pad = 0) # (227 - 11)/4 + 1 = 55
[ 27 X  27 X  96] MAX POOL1 : 3 X 3(Stride = 2)             # (55 - 3)/2 + 1 = 27
[ 27 X  27 X  96] NORM1
[ 27 X  27 X 256] CONV2 : 5 X 5 X 256(Stride = 1, Pad = 2)  # (27 + 2 * 2 - 5)/1 + 1 = 27
[ 13 X  13 X 256] MAX POOL2 : 3 X 3(Stride = 2)             # (27 - 3)/2 + 1 = 13
[ 13 X  13 X 256] NORM2
[ 13 X  13 X 384] CONV3 : 3 X 3 X 384(Stride = 1, Pad = 1)  # (13 + 2 * 1 - 3)/1 + 1 = 13
[ 13 X  13 X 384] CONV4 : 3 X 3 X 384(Stride = 1, Pad = 1)  # (13 + 2 * 1 - 3)/1 + 1 = 13
[ 13 X  13 X 256] CONV5 : 3 X 3 X 256(Stride = 1, Pad = 1)  # (13 + 2 * 1 - 3)/1 + 1 = 13
[  6 X   6 X 256] MAX POOL3 : 3 X 3(Stride = 2)             # (13 - 3)/2 + 1 = 6
[4096] FC6 : 4096 Neurons
[4096] FC7 : 4096 Neurons
[1000] FC8 : 1000 Neurons
```
{% endraw %}



Input Image가 227 x 227 x 3 이고, 첫 번째 레이어인 Conv-Layer는 Stride = 4인 11  X 11 필터가 96개가 존재한다


Q1. 첫 레이어의 출력 사이즈는 몇 일까?

- 출력값의 차원: (전체 이미지 크기 - 필터 크기) / Stride + 1 = (227 - 11) / 4 + 1 = 55
- 55(width) x 55(height) x 96(depth)

Q2. 이 레이어의 전체 파라미터 갯수는 몇 개일까?

- (11 x 11 x 3) x 96 = 35K

두 번째 레이어인 Pooling-Layer는 Stride = 2인 3 X 3 필터가 존재한다


Q3. 두 번째 레이어의 출력값의 크기는 몇일까?

- 출력값의 차원: (전체 이미지 크기 - 필터 크기) / Stride + 1 = (55 - 3) / 2 + 1 = 27
- 27(width) x 27(height) x 96(depth)
- depth는 입력값과 변하지 않는다

Q4. 두 번째 레이어의 파라미터는 몇 개일까?

- Pooling-Layer에는 파라미터가 없다

	→ 파라미터는 우리가 학습 시키는 가중치인데, Pooling-Layer의 경우 가중치가 없고 특정 지역에서만 큰 값을 뽑아내는 역할을 해서 학습 시킬 파라미터가 없다



### ZFNet


![2](/assets/img/2023-05-04-CS231n---Lecture-9.md/2.png)



{% raw %}
```python
[227 X 227 X   3] Input Image
[110 X 110 X  96] CONV1 : 7 X 7 X 96(Stride = 2, Pad = 0)  # (227 - 7)/2 + 1 = 111????
[ 55 X  55 X  96] MAX POOL1 : 3 X 3(Stride = 2)            # (111 - 3)/2 + 1 = 55
[ 55 X  55 X  96] NORM1
[ 26 X  26 X 256] CONV2 : 5 X 5 X 256(Stride = 2, Pad = 2) # (55 + 2 * 2 - 5)/2 + 1 = 28
[ 13 X  13 X 256] MAX POOL2 : 3 X 3(Stride = 2)            # (28 - 3)/2 + 1 = 
[ 13 X  13 X 256] NORM2
[ 13 X  13 X 384] CONV3 : 3 X 3 X 384(Stride = 1, Pad = 1) # (13 + 2 * 1 - 3)/1 + 1 = 13
[ 13 X  13 X 384] CONV4 : 3 X 3 X 384(Stride = 1, Pad = 1) # (13 + 2 * 1 - 3)/1 + 1 = 13
[ 13 X  13 X 256] CONV5 : 3 X 3 X 256(Stride = 1, Pad = 1) # (13 + 2 * 1 - 3)/1 + 1 = 13
[  6 X   6 X 256] MAX POOL3 : 3 X 3(Stride = 2)            # (13 - 3)/2 + 1 = 6
[4096] FC6 : 4096 Neurons
[4096] FC7 : 4096 Neurons
[1000] FC8 : 1000 Neurons
```
{% endraw %}


- AlexNet의 Stride Size, Filter 수 같은 하이퍼 파라미터를 개선하여 Error Rate를 더 개선했다


### VGGNet


![3](/assets/img/2023-05-04-CS231n---Lecture-9.md/3.png)

- 16 ~ 19개의 Layer를 가지면서 훨씬 깊어졌다
- 3 X 3의 아주 작은 필터만 사용하면서 주기적으로 Pooling Layer를 수행한다

	→ 필터의 크기가 작으면 파라미터의 수가 작고, 큰 필터에 비해 레이어를 조금 더 많이 쌓아 Depth를 더 키울 수 있다


	→ 3 x 3 필터를 여러 개 쌓은 것은 결국 7 x 7 필터를 사용하는 것과 실질적으로 동일한 Receptive Filter를 가진다


![4](/assets/img/2023-05-04-CS231n---Lecture-9.md/4.png)


Q1. stride = 1 인 3 x 3필터를 세 개의 Receptive Field는 어떻게 될까?

- Receptive Field은 filter가 한번에 볼 수 있는 입력의 "Spatial Area" 이다.
- 필터들이 서로 겹치기 떄문에 결국 입력 레이어의 7 x 7 필터와 실질적으로 동일한 Receptive Field를 가지면서 더 깊은 레이어를 쌓을 수 있게 된다
- 더 깊게 쌓으면 Non-Linearity를 더 추가할 수 있고 파라미터 수도 더 적어지게 된다

Q2. 네트워크가 깊어질수록 레이어의 필터 갯수를 늘려야 하는가?

- Depth를 늘리는 이유 중 하나는 계산량을 일정하게 유지하기 위해서다. 네트워크가 깊어질수록 각 레이어의 입력을 Down Sampling하게 되어 Spatial Area가 작아질수록 필터의 Depth를 조금씩 늘려주게 된다
- Width, Height가 작아지기 때문에 Depth를 늘려도 부담이 없다

Q4. 앞서 계산한 메모리 중에 굳이 가지고 있지 않고 버려도 되는 부분이 있는가?

- Backward Pass시 Chain Rule을 계산할 때 대부분은 이용되기 때문에 반드시 가지고 있어야 한다
- 파라미터가 존재하는 곳의 메모리 사용 분포를 살펴보면 초기 레이어에서 많은 메모리를 사용한다
- Spatial Dimension이 큰 곳들이 메모리를 더 많이 사용하고, 마지막 레이어는 많은 파라미터를 사용한다. FC-Layer가 Dense Connection이기 때문에 엄청난 양의 파라미터를 사용한다
- 일부 네트워크는 FC-Layer를 삭제하기도 하는데, 이는 너무 많은 파라미터를 줄이기 위함이다


### GoogleNet


![5](/assets/img/2023-05-04-CS231n---Lecture-9.md/5.png)

- 2014년에 ImageNet Classification에서 우승한 모델이고, 22개의 레이어를 갖고 있다
- Inception Module을 사용해서 높은 계산량을 효율적으로 수행할 수 있다
- FC-Layer를 없애서, 전체 파라미터 수가 500만개로 AlexNet보다 12배 작다


#### Naive Way of Inception Module


![6](/assets/img/2023-05-04-CS231n---Lecture-9.md/6.png)

- 내부에 동일한 입력을 받는 다양한 필터가 ‘병렬’로 존재하고, 이 레이어의 입력을 받아서 다양한 Conv 연산을 한다
- 1 X 1, 3 X 3, 5 X 5 Conv-Layer에 3 X 3 Pooling-Layer가 있다. 각 레이어에서 각각의 출력 값이 나오는데 이 출력을 모두 Depth 방향으로 합치는 방식이다


#### Naive Way의 문제점


![7](/assets/img/2023-05-04-CS231n---Lecture-9.md/7.png)

- **1 X 1 Conv 128개 (Stride = 1, Padding = 0)**
	- $(28-1)/1 +1=28$ → 28 X 28 X 128
- **3 X 3 Conv 192개 (Stride = 1, Padding = 1)**
	- $(28+2\times1-3)/1+1=28$ → 28 X 28 X 192
- **5 X 5 Conv 128개 (Stride = 1, Padding = 2)**
	- $(28+2\times2-5)/1 +1=28$ → 28 X 28 X 96
- **3 X 3 Pooling (Stride = 1, Padding = 1)**
	- $(28+2\times1-3)/1 +1=28$ → 28 X 28 X 256
- **Filter Concatenation**
	- 28 X 28 X (128 + 192 + 96 + 128)
	- 28 X 28 X 672
	- 전체 연산량 : 854M
- Layer를 거칠 때마다 Depth가 점점 늘어가서 연산량이 많아진다.
- 이 문제를 해결하기 위해 GoogLeNet에서 사용한 key insight는 `Bottleneck Layer`이다

	→ Conv 연산을 시작하기 전 입력을 더 낮은 차원으로 보내는 방법이다


![8](/assets/img/2023-05-04-CS231n---Lecture-9.md/8.png)

- 1 X 1 Conv는 각 Spatial Location에서만 내적을 수행하여 Depth를 더 낮은 차원으로 Projection할 수 있다
- Input Feature Map들 간의 선형결합(Linear Combination) 이라고 할 수 있다

![9](/assets/img/2023-05-04-CS231n---Lecture-9.md/9.png)


Q1. 1X1 Conv를 수행하면 일부 정보손실이 발생하지 않는가?

- 정보손실이 발생할수는 있으나 동시에 Reducdancy가 있는 Input Features를 선형결합한다고 볼 수 있다. 1x1 Conv로 선형결합하고 ReLU같은 non-linearity를 추가하면 네트워크가 더 깊어지는 효과도 있다.

![10](/assets/img/2023-05-04-CS231n---Lecture-9.md/10.png)

- GoogleNet의 앞 : `Stem Network`
	- 처음에는 Conv-Layer, Pooling-Layer를 몇 번 반복한다
- GoogleNet의 중간 : `Stacked Inception Module`
	- Inception Module이 쌓이는데 모두 조금씩 다르다
- GoogleNet의 마지막 : `Classifier Output`
	- 마지막에는 Classifier 결과를 출력한다
- 계산량이 많은 FC-layer를 대부분 걷어냈고, 파라미터를가 줄어들어도 모델이 잘 동작함을 확인했다

![11](/assets/img/2023-05-04-CS231n---Lecture-9.md/11.png)

- GoogleNet의 가지 : `Auxiliary Classification Outputs to Inject Additional Gradient at Lower Layers`
	- 보조 분류기(Auxiliary Classifier)는 작은 미니 네트워크이다
	- Average Pooling과 1x1 Conv가 있고 FC-Layer도 몇개 붙고 SoftMax로 1000개의 ImageNet Class를 구분한다
	- 이 곳에서도 ImageNet Trainset Loss를 계산한다. 네트워크의 끝에서 뿐만 아니라 보조 분류기(Auxiliary Classifier)에서도 Loss를 계산하는 이유는 네트워크가 깊기 때문이다. 보조 분류기를 중간 레이어에 달아주면 추가적엔 그레디언트를 얻을 수 있고 따라서 중간 레이어의 학습을 도울 수 있다


###  ResNet


![12](/assets/img/2023-05-04-CS231n---Lecture-9.md/12.png)

- ResNet은 Residual Conections라는 방법을 사용하고 엄청나게 깊은 네트워크이다.


#### Shallower Layer & Deeper Layer


![13](/assets/img/2023-05-04-CS231n---Lecture-9.md/13.png)

- 56-Layer 네트워크는 엄청나게 많은 파라미터가 있다고 추측할 수 있고, Overfit한다고 생각할 수 있지만, Training Error와 Test Error 모두 56-Layer 네트워크가 더 높게 나온다

	→ 단순한 Overfitting 문제가 아니라는 걸 알 수 있다


> 💡 ResNet 저자들은 더 깊은 모델 학습시 Optimization에 문제가 생긴다고 가설을 세웠다

- 우선 더 얕은 모델의 가중치를 깊은 모델의 일부 레이어에 복사했고 나머지 레이어는 Identity Mapping을 했다

	→ Identity Mapping은 Input을 Output으로 내보내는 것을 말한다

- 그러면 Shallower Layer 만큼의 성능이 나와야 한다. Deeper Model의 학습이 제대로 안 되더라도 적어도 Shallower Model 만큼의 성능은 보장된다.


#### Residual Mapping


![14](/assets/img/2023-05-04-CS231n---Lecture-9.md/14.png)

- ResNet의 아이디어는 단순히 레이어를 쌓는 방법(Direct Mapping)이 아니라 Residual Mapping의 방법이다
- 레이어가 직접 `H(x)`를 학습하기 보다 이런 식으로 `H(x) - x`를 학습할 수 있도록 만들어준다
- 이를 위해서 Skip Connection을 도입하게 된다. Skip Connection은 가중치가 없으며 입력을 Identity Mapping으로 그대로 출력단에 보낸다
- 실제 레이어는 변화량$(\delta)$ 만 학습하면 되고, 입력 X에 대한 잔차(Residual)라고 할 수 있다.
- 최종 출력 값은 `Input X + 변화량 (Residual)` 이다
- 가령 Input = Output 이어야 하는 상황이라면, 레이어의 출력인 $F(x)$가 0 이어야 하므로(Residual = 0) 모든 가중치를 0으로 만들어주면 그만이다. 손쉽게 출력을 Identity로 만들어 줄 수 있는 것이다
- 네트워크는 Residual만 학습하면 된다. 출력 값도 결국엔 입력 입력 X에 가까운 값이다. 다만 X를 조금 수정한 값이다. 레이어가 Full Mapping을 학습하기 보다 이런 조금의 변화만 학습하는 것이다

Q5. 왜 Direct mapping(H(x))을 학습하는 것 보다 Residual(F(x))을 학습하는 것이 쉬운가?

- 그건 단순한 가설이다. Residual을 학습시키는 것은 X에 대한 변화량$(\delta)$을 학습시키는 것의 의미이다. 이 가설이 참이라면 내 모델의 일부는 학습된 Shallow Layers이고, 나머지 레이어들은 Identity라면 잘 동작해야 한다. 이는 대부분의 레이어가 잘 동작하려면 레이어의 출력이 Identity에 가까워야 할 지 모른다는 것을 암시한다. 그래서 Identity(Input) + 변화량$(\delta)$ 만 학습시키면 된다.
- 만약, Output = Input (identity)이어야만 하는 상황이면 F(x) = 0 이 되면 된다. 상대적으로 학습시키기 쉽다고 생각할 수 있고, 이런 방식으로 Identity Mapping에 가까운 값을 얻을 수 있다

![15](/assets/img/2023-05-04-CS231n---Lecture-9.md/15.png)

- 하나의 Residual Block은 두 개의 3 X 3 Conv-Layer로 이루어져 있다. 이 Residual Block을 아주 깊게 쌓아 올릴 수 있고, ResNet은 150 Layers까지 쌓아 올릴 수 있다
- 주기적으로 필터를 두배 씩 늘리고 Stride = 2를 이용하여 Down Sampling을 수행하고 네트워크의 초반에는 Conv-Layer가 추가적으로 붙고 네트워크의 끝에는 FC-Layer가 없다

	→ 대신 Global Average Pooling Layer를 사용한다.


	→ GAP는 하나의 Map 전체를 Average Pooling 하고 마지막에는 1000개의 클래스 분류를 위한 노드가 붙는다


![16](/assets/img/2023-05-04-CS231n---Lecture-9.md/16.png)

- ResNet의 경우 모델 Depth가 50 이상일 때 Bottleneck Layer를 도입하고, GoogleNet에서 사용한 방법과 유사하다
- Bottleneck Layer는 1 X 1 Conv를 도입하여 초기 필터의 Depth를 줄여준다. 가령 입력이 28 X 28 X 256 일 때 1 X 1 Conv를 적용하면 Depth가 줄어들어서 28 X 28 X 64가 된다
- 이로인해 3 x 3 Conv의 계산량이 줄어들고 뒤에 다시 1 X 1 Conv를 추가해서 Depth를 다시 256으로 늘린다.
- 실제로 ResNet은 모든 Conv Layer 다음 Batch Norm을 사용하고 초기화는 Xavier를 사용하는데 추가적인 Scaling Factor를 추가하여 2로 나눠준다. 이 방법은 SGD + Momentum에서 좋은 초기화 성능을 보인다
- Learning Rate는 스케줄링을 통해서 Validation Error가 줄어들지 않는 시점에서 조금씩 줄여준다
- Mini-Batch 사이즈는 256이고 Weight Decay도 적용하고, Dropout은 사용하지 않았다
- 실험결과를 보면 그들은 성능의 열화 없이 Deep Neural Network를 아주 잘 학습시킬 수 있었고, Back Propagation 시에 네트워크의 Gradient Flow를 아주 잘 가져갈 수 있었다
- ImageNet 학습을 위해서 152-Layers까지 시도해 보았으며 Cifar-10 으로는 1200-Layers까지 늘렸다. 그리고 네트워크가 깊어질수록 Training Error는 더 줄어듦을 알 수 있었다. 깊은 네트워크의 Train Error가 더 높아지는 경우는 없었다.


### Analysis of Deep Neural Network Models


![17](/assets/img/2023-05-04-CS231n---Lecture-9.md/17.png)

- VGGNet은 메모리도 크면서 계산량도 많지만 성능은 나쁘지 않은 모델로 가장 효율성이 낮은 모델이다
- GoogLeNet이 가장 효율적인 네트워크이다. X축에서 거의 왼쪽에 있을 뿐만 아니라 메모리 사용량도 작다
- 초기 모델 AlexNet은 Accuracy가 낮다. 계산량도 작고, 메모리 사용량이 비효율적이다
- ResNet의 경우 적당한 효율성을 갖고 있다. 메모리 사용량과 계산량을 중간정도이지만 Accuracy는 최상위이다

![18](/assets/img/2023-05-04-CS231n---Lecture-9.md/18.png)


[bookmark](https://arxiv.org/abs/1605.07678)



### Network in Network (NiN)


![19](/assets/img/2023-05-04-CS231n---Lecture-9.md/19.png)

- 기본 아이디어는 MLP Conv Layer이다. 네트워크 안에 작은 네트워크를 삽입하는 것으로 각 Conv-Layer 안에 MLP(Multi-Layer Perceptron)를 쌓습니다.

	→ 맨 처음에는 기존의 Conv-Layer가 있고 FC-Layer를 통해 Abstract Features를 잘 뽑을수 있도록 한다


	→ 단순히 Conv Filter만 사용하지 말고, 조금 복잡한 계층을 만들어서 Activation Map을 얻어보자는 아이디어이다

- NiN에서는 기본적으로는 FC-Layer를 사용하고 이를 1x1 Conv-Layer라고도 한다.
- Network in Network는 GoogLeNet과 ResNet보다 먼저 Bottleneck 개념을 정립했기 때문에 의미있는 아이디어이다


### ResNets Model



#### Identity Mappings in Deep Residual Networks


![20](/assets/img/2023-05-04-CS231n---Lecture-9.md/20.png)

- Direct Path를 늘려서 정보들이 앞으로 더욱 더 잘 전달되고, Back Propagation도 더 잘 될 수 있게 개선했다


#### Wide Residual Networks


![21](/assets/img/2023-05-04-CS231n---Lecture-9.md/21.png)

- ResNet 논문은 깊게 쌓는 것에 열중했지만, 사실 중요한 것은 Depth가 아닌 Residual 이라고 주장한 연구이다
- Residual Connection이 있다면 네트워크가 굳이 더 깊어질 필요가 없다며, Residual Block을 더 넓게 만들어 Conv-Layer 필터를 더 많이 추가했다
- 기존의 ResNet에는 Block 당 $F$개의 Filter만 있었다면 대신에 $F\times K$개의 필터로 구성했다. 각 레이어를 넓게 구성했더니 50-Layer만 있어도 152-Layer의 기존 ResNet보다 성능이 좋다는 것을 입증했다
- 네트워크의 Depth 대신에 Filter의 Width를 늘리면 병렬화가 더 잘되기 때문에 계산 효율이 증가한다
- 네트워크의 Depth를 늘리는 것은 Sequential한 증가이기 때문에 Conv Filter를 늘리는(Width) 편이 더 효율적이다


#### Aggregated Residual Transformations for Deep Neural Networks (**ResNeXt)**


![22](/assets/img/2023-05-04-CS231n---Lecture-9.md/22.png)

- 이 논문에서도 계속 Residual Block의 Width를 파고들어 Filter의 수를 늘린다.
- 각 Residual Block 내에 "다중 병렬 경로" 추가하고, 이들은 Pathways의 총 합을 Cardinality라고 불렀다
- 하나의 Bottleneck ResNet Block은 비교적 작지만 이런 Thinner Blocks을 병렬로 여러개 묶었다.

	→ ResNeXt과 Wide ResNet과의 연관성을 볼 수 있다.


	→ 여러 Layer를 병렬로 묶어준다는 점에서 Inception Module과도 연관있다.



#### Deep Networks with Stochastic Depth


![23](/assets/img/2023-05-04-CS231n---Lecture-9.md/23.png)

- 네트워크가 깊어지면 깊어질수록 Vanishing Gradient 문제가 발생한다.
- 기본 아이디어는 Train Time에 Dropout과 같이 레이어의 일부를 제거하는 것이다. 일부 네트워크를 골라서 Identity Connection으로 만들어버린다.

	→ Short Network면 Gradient가 더 잘 전달되어 트레이닝이 더 잘 될 수 있기 때문이다 

- Test Time에는 Full Deep Neural Network를 사용한다


### Non-ResNets Model



#### FractalNet : Ultra-Deep Neural Networks without Residuals


![24](/assets/img/2023-05-04-CS231n---Lecture-9.md/24.png)

- Residual Connection이 전혀 없다. Shallow / Deep Neural Network의 정보 모두를 잘 전달하는 것이 중요하다고 생각해서 오른쪽 그림처럼 Fractal한 모습이다
- FractalNet에서는 Shallow / Deep Neural Network 경로를 출력에 모두 연결한다. 다양한 경로가 존재하지만 Train Time에는 Dropout처럼 일부 경로만을 이용해서 Train하고 Test Time에는 Full Network를 사용한다.


#### Densely Connected Convolutional Networks (DenseNet)


![25](/assets/img/2023-05-04-CS231n---Lecture-9.md/25.png)

- DenseNet에는 Dense Block이란 것이 있다. 한 레이어가 그 레이어 하위의 모든 레이어와 연결이 되어 있다. 네트워크의 입력 이미지가 모든 Layer의 입력으로 들어가는 것이다. 그리고 모든 레이어의 출력이 각 레이어의 출력과 Concat 된다
- Concat된 값이 각 Conv-Layer의 입력으로 들어가고, 이 과정에서 Dimension을 줄여주는 과정이 포함된다. Dense Connection이 Vanishing Gradient 문제를 완화시킬 수 있다고 주장한다
- Dense Connection은 각 레이어의 출력이 다른 레이어에서 여러 번 사용되기 때문에 Feature를 더 잘 전달하고 더 잘 사용할 수 있게 해준다


### Efficient Networks



#### SqueezeNet


![26](/assets/img/2023-05-04-CS231n---Lecture-9.md/26.png)

- `Fire Modules`을 도입했다. **Squeeze-Layer**는 1 X 1 Filter들로 구성되고, 이 출력 값이 1 X 1, 3 X 3 Filter들로 구성되는 **Expand Layer**의 입력이 된다
- SqueezeNet는 ImageNet에서 AlexNet만큼의 Accuracy를 보이지만 파라미터는 50배 더 작아진다. 즉, 용량이 매우 작아진다.
