---
layout: single
date: 2023-06-29
title: "CS224n - Lecture 2 (Neural Classifiers)"
use_math: true
author_profile: false
tags: [ê°•ì˜/ì±… ì •ë¦¬, ]
categories: [AI, ]
---

> â˜‘ï¸ **Lecture 2 : Neural Classifiers**  
> 1. Finish looking at word vectors and word2vec  
> 2. Optimization basics  
> 3. Can we capture the essence of word meaning more effectively by counting?  
> 4. The Glove model of word vectors  
> 5. Evaluating word vectors  
> 6. Word senses  
> 7. Review of classification and how neural nets differ  
> 8. Introducing neural networks



## Word2vecì˜ parameterì™€ computations


word2vecì˜ íŠ¹ì§•ì€ corpus ì†ì˜ wordë¥¼ **uì™€ v ë‘ê°œì˜ ë²¡í„°ë¡œ ë‚˜íƒ€ë‚´ëŠ” ê²ƒ**ì´ì—ˆë‹¤.


**(context word & center word)**


ê°„ë‹¨í•˜ê²Œ í˜•íƒœë¥¼ ì‚´í´ë³´ë©´ ë°‘ì˜ í˜•íƒœë¥¼ ë³´ì¼ ê²ƒì´ë‹¤.


![0](/assets/img/2023-06-29-CS224n---Lecture-2-(Neural-Classifiers).md/0.png)


ë‹¤ë¥¸ ëª¨ë¸ì¸ **â€œBag of wordsâ€**ì€ ê° positionì— ëŒ€í•´ ê°™ì€ ì˜ˆì¸¡ì„ ë‚´ë¦¬ëŠ”ë°, 


ì´ëŠ” contextì— ë”°ë¼ wordì˜ ëœ»ì´ ë°”ë€ŒëŠ” ê²ƒì„ í‘œí˜„í•  ìˆ˜ ì—†ë‹¤.


ë°˜ë©´ word2vecì˜ ê²½ìš° contextì— ëŒ€í•´ ê³ ë ¤í•˜ê¸° ë•Œë¬¸ì—, ì•„ë˜ì™€ ê°™ì´ vectorë“¤ì„ 2ì°¨ì›ì— í‘œì‹œí–ˆì„ ë•Œ, ë¹„ìŠ·í•œ ë‹¨ì–´ë“¤ ë¼ë¦¬ ê·¸ë£¹ì„ í˜•ì„±í•˜ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤.


![1](/assets/img/2023-06-29-CS224n---Lecture-2-(Neural-Classifiers).md/1.png)_ì‹ ê¸°í•˜ë„¤ìš”â€¦ (samsungê³¼ nokiaë¼ë‹ˆ ì¬í•™ìŠµì´ í•„ìš”í• ë“¯â€¦)_


ì§€ë‚œ Lectureì—ì„œ gradient descent ê³¼ì •ì— ì“°ì´ëŠ” ë¯¸ë¶„ê³¼ì •ì„ ì•Œì•„ ë³´ì•˜ëŠ”ë°


ì´ë²ˆ Lectureì—ì„  ë³¸ê²©ì ìœ¼ë¡œ **Gradient Descent**ì— ëŒ€í•´ ì•Œì•„ë´…ë‹ˆë‹¤.



### Gradient Descent


ì§€ë‚œë²ˆì—, ìš°ë¦¬ëŠ” $L(\theta)$ì™€ $J(\theta)$ë¥¼ ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜í–ˆì—ˆë‹¤

- **data likelihood:** $L(\theta) = \Pi^T_{t=1}\Pi_{-m\leq j \leq m, j\ne0}P(w_{t+j}|w_t;\theta)$
- **objective function(cost, loss):** $J(\theta) = -\frac{1}{T}logL(\theta)$

(TëŠ” ì´ word ìˆ˜)


ìˆ˜ë°±ê°€ì§€ì˜ parameterê°€ ì¡´ì¬í•˜ê¸° ë•Œë¬¸ì—, **cost functionì¸** $J(\theta)$ì˜ global minimumì€ ìš°ë¦¬ê°€ ì•Œ ìˆ˜ ì—†ë‹¤. ëŒ€ì‹ , ì„ì˜ì˜ ì (ì´ˆê¸°í™”ëœ parameterì˜ ê°’)ì—ì„œ ì‹œì‘í•´ì„œ ë§¤ ìˆœê°„ìˆœê°„ ê·¸ë˜í”„ì—ì„œ minimumìœ¼ë¡œ í–¥í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ ì ì„ ì´ë™í•´, **local minimum**ì€ êµ¬í•  ìˆ˜ ìˆë‹¤.


parameterë“¤ì€ ë‹¤ìŒ ì‹ì²˜ëŸ¼ updateëœë‹¤.


![2](/assets/img/2023-06-29-CS224n---Lecture-2-(Neural-Classifiers).md/2.png)


![3](/assets/img/2023-06-29-CS224n---Lecture-2-(Neural-Classifiers).md/3.png)


í•˜ì§€ë§Œ ì´ ë°©ì‹ ì—­ì‹œ **ë‹¨ì ì´ ì¡´ì¬**í•œë‹¤.


$J(\theta)$ëŠ” ëª¨ë“  window ì•ˆì˜ corpusë¥¼ ëŒ€ìƒìœ¼ë¡œ í•˜ê¸° ë•Œë¬¸ì—, $\theta$ì˜ ìˆ˜ëŠ” ê±°ì˜ billionsì— ê°€ê¹Œìš¸ ì •ë„ë¡œ ë§ê³ , ê·¸ë ‡ê¸° ë•Œë¬¸ì— $\triangledown_\theta J(\theta)$**ë¥¼ êµ¬í•˜ëŠ” ê²ƒì€ costê°€ ë„ˆë¬´ expensive**


ê·¸ë ‡ê¸° ë•Œë¬¸ì— ë‚˜ì˜¨ ê°œë…ì´ **Stochastic Gradient Descent**



### Stochastic Gradient Descent


ì „ì²´ data ë‹¨ìœ„ë¡œ updateí•˜ëŠ” ê²ƒì´ ì•„ë‹Œ, 


**batch ë‹¨ìœ„ë¡œ ë‚˜ëˆ  ì—¬ëŸ¬ë²ˆì— ê±¸ì³ updateë¥¼ ì§„í–‰**


**window ë‹¨ìœ„ updateê°€ ì¼ì–´ë‚¨**


![4](/assets/img/2023-06-29-CS224n---Lecture-2-(Neural-Classifiers).md/4.png)


$\triangledown_\theta J_t(\theta)$ëŠ” sparseí•˜ê²Œ ì¼ì–´ë‚¨


ì™œëƒí•˜ë©´, word2vecì—ì„œ matrixëŠ” unique vocabì˜ ìˆ˜ì— ì˜í–¥ì„ ë°›ëŠ”ë°,


window ë‹¨ìœ„ë¡œ updateë¥¼ í•œë‹¤ë©´, window ì•ˆì— í¬í•¨ëœ unique vocabì˜ ìˆ˜ëŠ” matrix í¬ê¸° ëŒ€ë¹„ êµ‰ì¥íˆ ì ì„ ê²ƒ


ë§Œì•½ centor word ì–‘ ì˜†ìœ¼ë¡œ mê°œì˜ wordë¥¼ ê³ ë ¤í•œë‹¤ í–ˆì„ ë•Œ, $\triangledown_\theta J_t(\theta)$ëŠ” ì˜¤ì§ 2m+1ê°œì—ì„œë§Œ ì¼ì–´ë‚˜ê¸° ë•Œë¬¸ì—, ì„±ëŠ¥ì´ ì¢‹ì•„ì§


![5](/assets/img/2023-06-29-CS224n---Lecture-2-(Neural-Classifiers).md/5.png)


![6](/assets/img/2023-06-29-CS224n---Lecture-2-(Neural-Classifiers).md/6.png)



## Word2vec details


ì•ì—ì„œ, word2vecì€ 2ê°€ì§€ vector, centor wordì™€ context(outside) wordsë¥¼ ì‚¬ìš©í•œë‹¤ê³  ë°í˜”ë‹¤.


ê·¼ë° **ì™œ 2ê°€ì§€ì¼ê¹Œ?**


â†’ **easier optimization**


**(ì˜¤íˆë ¤ 1ê°€ì§€ vectorë§Œ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ë” ë³µì¡í•¨)**


ì´ 2ê°€ì§€ ë²¡í„°ë¥¼ ì‚¬ìš©í–ˆì„ ë•Œ, **2ê°€ì§€ ëª¨ë¸ì´ ì¡´ì¬**



#### 1. **Skip-grams (SG)**


**â†’ Predict context(outside) words / given center word**



#### 2. **Continuous Bag of Words (CBOW)**


**â†’ Predict center word / given context(outside) words**


ì´ ì™¸ì—ë„ **ì¶”ê°€ training method**ê°€ ìˆëŠ”ë°


**Negative sampling**ì´ë¼ê³  í•œë‹¤.


(ì§€ê¸ˆê¹Œì§€ ë°°ì›Œì˜¨ naive softmaxëŠ” ê°„ë‹¨í•˜ë‚˜, costê°€ ë§ì´ ë“œëŠ”ë°, **negative samplingì€ ì¢€ ë” efficient**)



### Negative sampling


ì´ ê°œë…ì˜ ë©”ì¸ ì•„ì´ë””ì–´ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.


> ğŸ§® **Negative sampling**  
>   
> binary logistic regressions for a true pair (center word and a word in its context window) versus several noise pairs (the center word paired with a random word)


ì¦‰, **1) ì›ë˜ì˜ center wordì™€ context wordì˜ ìŒ**ê³¼, **2) centor wordì™€ ì˜ë¯¸ ì—†ëŠ” wordë“¤ì˜ ìŒ**ì„ **binary classification**ìœ¼ë¡œ ë¹„êµí•œë‹¤ëŠ” ê²ƒ


objective functionì„ ì•Œì•„ë³´ì


$$
J_t(Î¸) = log~Ïƒ(u_o^Tv_c) + Î£_{i=1}^k E_{j\simeq P(w)}[log~Ïƒ(-u_j^Tv_c)]
$$

- $Ïƒ(x) = \frac{1}{1+e^{-x}}$

	0,1 ì‚¬ì´ ì •ê·œí™”, activation function, binary classificationì— fit

- t : time step
- log ì—°ì‚° ì´ìœ ? ì—°ì‚°ì‹œ í•©ì‚° ì—°ì‚°ìœ¼ë¡œ ë°”ê¿”ì£¼ê¸° ë•Œë¬¸
- k : negative sampling ìˆ˜
- P(w) : unigram distribution. (3/4 power)
- j : P(w)ì—ì„œ samplingëœ word (negative training example; kê°œ ì¡´ì¬)

ì‰½ê²Œ ë‚˜íƒ€ë‚´ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤


$$
J_{neg-sample}(u_o, v_c, U)=-log~Ïƒ(u^T_ov_c)-\Sigma_{kâˆˆ\{K~sampled~indices\}}~log~Ïƒ(-u^T_kv_c)
$$

- k : negative sampling ìˆ˜ (word probability ì´ìš©)
- $P(w) = U(w)^{3/4}/Z$, unigram distributionì„ 3/4 ì œê³±í•¨

	(ì´ë ‡ê²Œ 3/4 ì œê³±í•˜ëŠ” ê²ƒì€ ëœ ìì£¼ ë‚˜íƒ€ë‚˜ëŠ” ë‹¨ì–´ê°€ ë” ìì£¼ sampled ë˜ê²Œ í•¨)


	(negative sampleì´ ì˜¬ë°”ë¥´ê²Œ ì„ íƒë  ìˆ˜ ìˆë„ë¡)


**ìˆ˜ì‹ì„ ë‚˜ëˆ ì„œ ì‚´í´ë³´ì**

- $-log~Ïƒ(u^T_ov_c)$ : **target wordì™€ context word ì‚¬ì´ì˜ ìœ ì‚¬ì„±ì„ ì¸¡ì •**

	â†’ modelì´ target wordê°€ ì£¼ì–´ì¡Œì„ ë•Œ, context wordì— ëŒ€í•´ ë†’ì€ í™•ë¥ ì„ ì˜ˆì¸¡í•  ìˆ˜ ìˆë„ë¡ í•¨


	(cost functionì„ minimizeí•´ì•¼ í•˜ë¯€ë¡œ, targetê³¼ contextê°€ ìœ ì‚¬í•˜ë©´ cost function ì‘ì•„)

- $\Sigma_{kâˆˆ\{K~sampled~indices\}}~log~Ïƒ(-u^T_kv_c)$ : **negative sampleê³¼ centor word ê°„ì˜ ìœ ì‚¬ì„± ì¸¡ì •**

	â†’ ì£¼ì–´ì§„ context wordì— ëŒ€í•´ negative sampleì— ëŒ€í•´ ë‚®ì€ í™•ë¥ ì„ ì˜ˆì¸¡í•  ìˆ˜ ìˆë„ë¡ 


	â†’ negative sampleê³¼ ìœ ì‚¬í•˜ë‹¤ë©´ cost functionì´ ì»¤ì ¸ì•¼ í•œë‹¤.


	â†’ ë”°ë¼ì„œ negative sampleê³¼ ìœ ì‚¬í• ìˆ˜ë¡ $\Sigma_{kâˆˆ\{K~sampled~indices\}}~log~Ïƒ(-u_k^Tv_c)$ëŠ” ì‘ì•„ì§„ë‹¤. 


		(ì ˆëŒ“ê°’ì´ í° ìŒìˆ˜ê°€ ë¨)


	![7](/assets/img/2023-06-29-CS224n---Lecture-2-(Neural-Classifiers).md/7.png)_logarithmic sigmoid_


	â†’ ê²°ê³¼ì ìœ¼ë¡œ **negative sampleê³¼ ìœ ì‚¬í• ìˆ˜ë¡ cost functionì´ ì»¤ì§€ê¸° ë•Œë¬¸**ì—, gradient descent ê³¼ì •ì—ì„œ ë”ìš± negative sampleê³¼ ë©€ì–´ì§€ëŠ” ë°©í–¥ìœ¼ë¡œ í•™ìŠµì´ ì§„í–‰ëœë‹¤.


	(negative sampleì„ ì˜ ì„ ì •í•´ì•¼ í•˜ë¯€ë¡œ, ì—¬ê¸°ì„œ 3/4 ì œê³± ê°œë…ì´ ì¶”ê°€ëœ ê²ƒ)


In natural language processing, unigram distribution refers to the probability distribution of single words in a corpus. It is often used in language modeling and can be calculated by counting the frequency of each word in the corpus and then normalizing the counts to get probabilities. In some cases, the probabilities may be adjusted to account for the fact that less frequent words are more likely to be sampled as negative examples in models like word2vec.



### Example


![8](/assets/img/2023-06-29-CS224n---Lecture-2-(Neural-Classifiers).md/8.png)


â†’ ì˜†ì˜ í‘œëŠ” co-occurence vector (ë™ì‹œ ì¶œí˜„ vector) ë“¤ì„ matrixë¡œ ì •ë¦¬í•œ ê²ƒ


ì§€ê¸ˆì˜ ì˜ˆì‹œì—ì„ , **corpusì˜ ìˆ˜, vocabì˜ ìˆ˜ê°€ ì‘ì•„**


â†’ co-occurence matrixì˜ í¬ê¸°ê°€ ì‘ê¸° ë•Œë¬¸ì—, í•™ìŠµì— ë“œëŠ” ë¹„ìš©ì´ ì ë‹¤.


í•˜ì§€ë§Œ ì‹¤ìƒí™œì—ì„ , ìˆ˜ë§ì€ vocabì´ ì¡´ì¬í•˜ê¸° ë•Œë¬¸


â†’ co-occurence matrix ë„ˆë¬´ **high dimensionalí•´ì§**


â†’ í•™ìŠµ ë¹„ìš© í¬ê³ , ì˜¤íˆë ¤ í•™ìŠµ íš¨ê³¼ ì¢‹ì§€ ì•ŠìŒ


(too sparse, less robust)


ì£¼ë¡œ, 25-1000 dimensionì„ ìœ ì§€í•˜ëŠ” ê²ƒì´ ì¢‹ê¸° ë•Œë¬¸ì—, dimensionì„ ì¤„ì—¬ì•¼ ê² ë‹¤ëŠ” ëª©ì ì´ ë“±ì¥


â†’ ì´ë¥¼ ìœ„í•´ ì‚¬ìš©í•œ ê°œë…ì´ **SVD(Singular Value Decomposition)**


---



#### SVD (Singular Value Decomposition)


> ğŸ’¡ **SVD**  
> Factorizes X into $U\Sigma V^T$, where U and V are orthonormal


ChatGPT í”¼ì…œ


$U, \Sigma, V $ ëª¨ë‘ ë‹¤ì–‘í•œ ìš©ë„ë¡œ ì‚¬ìš©


![9](/assets/img/2023-06-29-CS224n---Lecture-2-(Neural-Classifiers).md/9.png)


---


ë‹¤ì‹œ ëŒì•„ì™€ì„œ, co-occurence matrixëŠ” ë‹¨ì§€, corpusì—ì„œ window ì•ˆì— centor wordì— ëŒ€í•´ context wordë¡œ ëª‡ ë²ˆ ë“±ì¥í•˜ëŠ”ì§€ë¥¼ checkí•´ matrixì— store í•˜ëŠ”, ê·¸ì € **raw counts ë§Œì„ ê¸°ë¡**í•˜ëŠ” matrix


íš¨ê³¼ì ì´ì§€ ì•ŠìŒ

- **High dimensionality**

	: sparse, high dimension â†’ poor result

- **Noise**

	: word embeddingì— ì•…ì˜í–¥


	(ëª‡ë²ˆ ë“±ì¥í•˜ì§€ ì•ŠëŠ” ë‹¨ì–´ ì¦‰, noise ë“¤ì€ ê´€ê³„ ì„¤ì •ì— ì•…ì˜í–¥ ê°€ëŠ¥)

- **Scale**

	: ë¬¸ì œ ìˆëŒ€ìš”~


 ë•Œë¬¸ì—, ëª‡ëª‡ fixë¥¼ í•˜ê¸°ë„ í•¨

- count ì¡°ì ˆ
	- log the frequencies
	- min(X, t) with t = 100
	- Ignore the function words
- ramped windows

	ê°€ê¹Œì´ ìˆëŠ” ë‹¨ì–´ë¥¼ ë” í¬ê²Œ ì¹´ìš´íŠ¸

- Pearson correlations instead of counts

	countì˜ ë°©ì‹ì„ ì•„ì˜ˆ ë°”ê¿ˆ


ì´ë ‡ê²Œ ë“±ì¥í•œ ë°©ì‹ì´ **COALS**


![10](/assets/img/2023-06-29-CS224n---Lecture-2-(Neural-Classifiers).md/10.png)


(í•˜ì§€ë§Œ ì—¬ì „íˆ Count based)



### Count based vs. direct prediction


GloVe â†’ word vectors 


linear algebra based methods on co-occurence matrices


â†’ ex. LSA, COALS

- Fast training
- Efficient usage of statistics
- Primarily used to capture word similarity
- Disproportionate importance given to large counts

models like skip-gram, CBOW


â†’ iterative neural updating algorithm

- Scales with corpus size
- Inefficient usage of statistics
- generate improved performance on other tasks
- Can capture complex patterns beyond word similarity

GloVe uses encoding

- ratios of co-occurence probabilities can encode meaning components
- weighted co-occurence matrices


### GloVe


GloVe algorithmì˜ ê°€ì¥ í° íŠ¹ì§•ì€, ë¹„ìŠ·í•œ context ì†ì—ì„œì˜ words ê°„ì˜ relationshipì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤ëŠ” ì 


ì˜ˆì‹œë¥¼ ì‚´í´ë³´ì


![11](/assets/img/2023-06-29-CS224n---Lecture-2-(Neural-Classifiers).md/11.png)


words ê°„ ê´€ê³„ì„±ì´ í¬ë‹¤ë©´, probabilityëŠ” large, ì•„ë‹ˆë¼ë©´ smallì¼ ê²ƒì´ë‹¤.


GloVe algorithmì€ ì´ë¥¼ í™œìš©í•œë‹¤.


![12](/assets/img/2023-06-29-CS224n---Lecture-2-(Neural-Classifiers).md/12.png)


log-bilinear modelì„ ì‚¬ìš©í•˜ëŠ”ë°,


$w_iÂ·w_j = log~P(i|j) $ë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆëŠ” ê²ƒì€, dot productë¥¼ ì‚¬ìš©í•´, P(k|j)ë¥¼ ê·¼ì‚¬í•˜ê² ë‹¤ëŠ” ëœ»ì„


â†’ dot productë¥¼ í†µí•´ P(k|j)ë¥¼ representí•˜ë ¤ê³  í•œë‹¤.


ì´ í•µì‹¬ ì•„ì´ë””ì–´ì—ì„œ ì•Œê³ ë¦¬ì¦˜ì´ ì‹œì‘í•¨


$$
w_iÂ·w_j = log~P(i|j)
$$


$$
J = \Sigma^V_{i, j=1}f(X_{ij})(w_i^T\tilde w_j + b_i + \tilde b_j - log~X_{ij})^2
$$

- fast training
- Scalable to huge corpora
- Good performance even with small corpus and small vectors

bëŠ” biasë¥¼ ì˜ë¯¸


~ëŠ” medianì„ ì˜ë¯¸


ê·¸ë ‡ê¸° ë•Œë¬¸ì—, wiwjì™€ logX_ijë¥¼ ê°™ê²Œ ê·¼ì‚¬ì‹œì¼œì•¼ í•˜ê¸° ë•Œë¬¸ì—, ê°™ì„ ìˆ˜ë¡ JëŠ” ì¤„ì–´ë“¤ê³ , bëŠ” ì¡°ì •ì„ ìœ„í•´ í•„ìš”í•œ ê°’ì¸ ê²ƒì´ë‹¤.


![13](/assets/img/2023-06-29-CS224n---Lecture-2-(Neural-Classifiers).md/13.png)


fì˜ ì—­í• ì€, ë” ë§ì´ co-occurence matrixë¡œ countë˜ëŠ” ë‹¨ì–´ì— ë” í° ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•˜ê² ë‹¤ëŠ” ê²ƒì´ê³ , ì¼ì • ìˆ˜ì¤€ ì´ìƒìœ¼ë¡œ ë„˜ì–´ê°€ëŠ” ë„ˆë¬´ ë§ì´ ë‚˜ì˜¤ëŠ” ë‹¨ì–´ë“¤(ex. function words)ì— ëŒ€í•´ì„  ì¼ì •ì¹˜ê¹Œì§€ë§Œ count í•˜ê² ë‹¤ëŠ” ëœ»ì´ë‹¤.


learn vector representations for each word in the vocabulary such that the dot product of two word vectors is proportional to the log co-occurrence probability of the corresponding words.


GloVe ë°©ì‹ í›Œë¥­í•¨



### Evaluation


How to evaluate in nlp?


2ê°€ì§€ ë°©ë²• ì¡´ì¬


**intrinsic vs extrinsic**

- **Intrinsic**
	- Evaluation on a specific/intermediate subtask
	- Fast to compute
	- Helps to understand that system
	- Not clear if really helpful unless correlation to real task is established

a:bì˜ ê´€ê³„ì„±ì„ c:?ì— ëŒ€í•´ ì ìš©í•´ ?(d)ë¥¼ ì°¾ëŠ” ê²ƒ


ex. man:woman :: king:? 


$$
d = arg~max_i\frac{(x_b-x_a+x_c)^Tx_i}{||x_b-x_a+x_c||}
$$


sol) ?ëŠ” queenì¼ ê²ƒ


x_a, x_b, x_cëŠ” a, b, cì— ëŒ€í•œ word vectorì„


bì™€ aì˜ ëº„ì…ˆ ì—°ì‚°ìœ¼ë¡œ ì–»ì€ ë²¡í„°ë¥¼ cì— ë”í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ í™•ì¸


argmax_iëŠ” ì´ ìˆ˜ì‹ì„ ìµœëŒ€í™”í•  ìˆ˜ ìˆëŠ” ië¥¼ ì„ íƒí•˜ê² ë‹¤ëŠ” ëœ»ì„


ex1) GloVe Visualizations


![14](/assets/img/2023-06-29-CS224n---Lecture-2-(Neural-Classifiers).md/14.png)


ex2)


![15](/assets/img/2023-06-29-CS224n---Lecture-2-(Neural-Classifiers).md/15.png)

- **Extrinsic**
	- Evaluation on a real task
	- Can take a long time to compute accuracy
	- Unclear if the subsystem is the problem or its interaction or other subsystems
	- If replacing exactly one subsystem with another improves accuracy â†’ Winning!

![16](/assets/img/2023-06-29-CS224n---Lecture-2-(Neural-Classifiers).md/16.png)


word2vecì—ì„  í•œ wordë¥¼ ì—¬ëŸ¬ê°œì˜ senseë¥¼ ê°€ì§„ wordë¡œ ë‚˜ëˆ”

