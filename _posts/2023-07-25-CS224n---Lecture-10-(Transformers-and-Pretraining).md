---
layout: single
date: 2023-07-25
title: "CS224n - Lecture 10 (Transformers and Pretraining)"
use_math: true
tags: [ê°•ì˜/ì±… ì •ë¦¬, ]
categories: [AI, ]
---


### What is Transformer?


Model pretraining three ways

1. Decoders
2. Encoders
3. Encoder-Decoders

ğŸ“˜**The byte-pair encoding algorithm : ì •ë³´ ì••ì¶• ì•Œê³ ë¦¬ì¦˜**

1. í•˜ë‚˜ì˜ Characterë¡œ ì‹œì‘í•œë‹¤. or end-of-word
2. Corpus of textë¥¼ ì´ìš©í•´ ì¸ì ‘í•œ ë¬¸ìë¼ë¦¬ subwordë¥¼ ë§Œë“ ë‹¤.
3. ë°˜ë³µ
4. ìƒˆë¡œìš´ vocabularyë¥¼ ì™„ì„±

EX. 

- taaaaasty â†’ taa## aaa## sty, aaa## ê°€ ê°•ì¡°í•˜ëŠ” ê²ƒìœ¼ë¡œ ê½¤ë‚˜ ì“°ì´ëŠ” ê²ƒì„ ë°œê²¬í•  ìˆ˜ë„ ìˆìŒ.
- Transformerify â†’ Transformer## , ify ì•„í•˜ , ifyëŠ” ì ‘ë¯¸ì‚¬ë¡œ ë§ì´ ì“°ì´ëŠ” êµ¬ë‚˜

â—í•´ë‹¹ ê°•ì˜ì—ì„œ ì–˜ê¸°í•˜ëŠ” wordëŠ” full wordì¼ ìˆ˜ë„ ìˆê³  subwordì¼ ìˆ˜ë„ ìˆë‹¤. ê·¸ëŸ°ë° transformerëŠ” self-attention operationì‹œì— ì´ëŸ¬í•œ ì •ë³´ë¥¼ ê°–ì§€ ì•ŠëŠ”ë‹¤. 


---



#### Model pre-training and embedding


> â€œ You shall know a word by the company it keeps â€œ J.R. Firsth 1957


â—embeddingì˜ ë¬¸ì œì ì€ ì£¼ë³€ì„ ê³ ë ¤í•˜ì§€ ì•ŠëŠ”ë‹¤ëŠ” ê²ƒ. ì˜ˆë¥¼ ë“¤ì–´ ê°™ì€ ë‹¨ì–´ì— ëŒ€í•´ì„œ ì˜ë¯¸ê°€ ë‹¬ë¼ì§€ë”ë¼ë„ Word2vec embeddingì€ ê°™ì„ ê²ƒì´ë‹¤. 


![0](/assets/img/2023-07-25-CS224n---Lecture-10-(Transformers-and-Pretraining).md/0.png)


ì¦‰, ë¬¸ë§¥ì„ ê³ ë ¤í•˜ì§€ ì•Šì€ word embeddingë§Œì„ pretraindí•˜ì˜€ì§€ë§Œ ìš°ë¦¬ì˜ downstream taskëŠ” contextual aspectê°€ í•„ìš”í•˜ë‹¤! ë˜í•œ ë§ì€ parameterê°€ randomly initializedëœë‹¤.


**âˆ´ NLP ë„¤íŠ¸ì›Œí¬ì˜ ëª¨ë“  prameterì— ëŒ€í•´ pretrainingì´ í•„ìš”í•˜ë‹¤.** 

1. â€œRepresentation of languageâ€
2. â€œParameter initailizationsâ€ for strong NLP model
3. â€œProbability distributionsâ€ sample from

 


**Then, How to pretraining whole models?**


![1](/assets/img/2023-07-25-CS224n---Lecture-10-(Transformers-and-Pretraining).md/1.png)


â“ RIsk overfitting on our data when theyâ€™re doing pretraining?


â†’ ì–¸ì–´ëŠ” ë§¤ìš° sparse í•´ì„œ ê±°ì˜ ê·¸ë ‡ì§€ ì•Šë‹¤. ì›Œë‚™ ë°ì´í„°ê°€ ë§ì•„ì„œ~



#### Pretraining / Fine-tuning


Pretraining : ì´ì „ì˜ ë‹¤ë¥¸ íƒœìŠ¤í¬ë¥¼ ë‹¤ë£¨ëŠ” ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ë¥¼ í™œìš©í•˜ëŠ” ë°©ë²•.


Fine-tuning : Pretrainingí•œ ê°€ì¤‘ì¹˜ë¥¼ í† ëŒ€ë¡œ ì¶”ê°€ë¡œ í•™ìŠµ( ë¯¸ì„¸ ì¡°ì • )í•˜ëŠ” ë°©ë²•. ex) ì¶”ê°€ ë°ì´í„° íˆ¬ì…ì„ í†µí•œ weight ê°±ì‹ 


![2](/assets/img/2023-07-25-CS224n---Lecture-10-(Transformers-and-Pretraining).md/2.png)



#### Stochastic gradient descent


pretrainingì˜ minimaì—ì„œ gradinent descentë¥¼ ì§„í–‰í•˜ë©° fine tuningí•˜ëŠ” ê²ƒì´ just workí•œë‹¤â€¦


---



#### Pretraining for three types of architectures

1. Decoders
2. Encoders
3. Encoder-Decoders

![3](/assets/img/2023-07-25-CS224n---Lecture-10-(Transformers-and-Pretraining).md/3.png)



#### Pretraining Decoder


![4](/assets/img/2023-07-25-CS224n---Lecture-10-(Transformers-and-Pretraining).md/4.png)


![5](/assets/img/2023-07-25-CS224n---Lecture-10-(Transformers-and-Pretraining).md/5.png)


backpropagation ê³¼ì •ì„ í†µí•´ ì „ì²´ì— ëŒ€í•œ fine-tuning ê°€ëŠ¥. But Linear layer wasnâ€™t pretrained.


Contract is that it is defining probabiliy distributions. 


we donâ€™t need to use it as a probability distribution(?)


![6](/assets/img/2023-07-25-CS224n---Lecture-10-(Transformers-and-Pretraining).md/6.png)


![7](/assets/img/2023-07-25-CS224n---Lecture-10-(Transformers-and-Pretraining).md/7.png)


As Generator( predict , like dialogue ) at fine tuning time. fine tune probability distribution.  


â­`Unlike before, last layer is pretrained. Still fine tuning the whole thing. â€˜



#### GPT ( Generative Pretrained Transformer )

- Transformer decoder with 12 layers.
- 768-dimensional hidden states, 3072-dimensional feed-forward hidden layers
- Byte-pair encoding with 40,000 merges
- Trained on BooksCorpus: over 7000 unique books.

â­**How to finuend it?**


[https://eggplant-culotte-a36.notion.site/GPT-1-Improving-Language-Understanding-by-Generative-Pre-Training-8dfe31b8e4914ebeb6a64230cd9a4ecb](https://eggplant-culotte-a36.notion.site/GPT-1-Improving-Language-Understanding-by-Generative-Pre-Training-8dfe31b8e4914ebeb6a64230cd9a4ecb)


+) GPT-2 is just a bigger GPT, has larger hidden units, more layers.



#### Pretraining Encoder


 Benefit of Encoder is to get bidirectional context.



#### BERT : masked language modeling


[https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805)


![8](/assets/img/2023-07-25-CS224n---Lecture-10-(Transformers-and-Pretraining).md/8.png)


![9](/assets/img/2023-07-25-CS224n---Lecture-10-(Transformers-and-Pretraining).md/9.png)


![10](/assets/img/2023-07-25-CS224n---Lecture-10-(Transformers-and-Pretraining).md/10.png)


ê¸°ì¡´ Bi-directionalëŠ” ì¢Œ-ìš°, ìš°-ì¢Œë¥¼ í†µí•´ ë¬¸ì¥ ì „ì²´ë¥¼ ì˜ˆì¸¡í•œë‹¤. BERTëŠ” ëœë¤í•˜ê²Œ ë¬¸ì¥ì˜ 15í¼ì— ëŒ€í•´ ì˜ˆì¸¡ì„ ì§„í–‰í•©ë‹ˆë‹¤. ê·¸ ì¤‘ 1) 80í¼ëŠ” [MASK]ë¡œ ë°”ê¾¸ê³  2) 10í¼ëŠ” Randomí•˜ê²Œ ë°”ê¾¸ê³  3) 10í¼ëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€í•œë‹¤. ( ì´ ë•Œ predictëŠ” ì—¬ì „íˆ ì§„í–‰í•œë‹¤. ) ê·¸ë¦¬ê³  ì´ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ì‘ì—…( MLM )ìœ¼ë¡œ ì§„í–‰í•œë‹¤.

- BERT Pretraining : is expensive and was pretrained with 64 TPU.
- BERT Fintuning : is practical and common on a 1 GPU.

> Pretrain once, finetune many times


ë‹¤ì–‘í•œ taskì— ëŒ€í•´ finetuning BERTì˜ ê²°ê³¼


![11](/assets/img/2023-07-25-CS224n---Lecture-10-(Transformers-and-Pretraining).md/11.png)


! BERT > GPT in Bidirectional context. !


But , BERTëŠ” ê° í† í°ì´ ë…ë¦½ì ì´ë¯€ë¡œ sequenceë¥¼ generateí•  ìˆ˜ ì—†ë‹¤. 


ğŸ”—Â **Extensions of BERTâ€¦**


*** 



#### Pretraining encoder-decoders : what pretraining objective to use?( T5 )


span corruption : 


***



### GPT 3, In-Context learning


learning â€œwithout gradient stepsâ€, just to do pretraining. These models are still not well-understood.


whopping 175billion parameters!



#### In-Context Learning


í”„ë¡¬í”„íŠ¸ì˜ ë‚´ìš©ë§Œìœ¼ë¡œ í•˜ê³ ì í•˜ëŠ” task ë¥¼ ìˆ˜í–‰í•˜ëŠ” ì‘ì—…. ë§ ê·¸ëŒ€ë¡œ prompt ë‚´ ë§¥ë½ì  ì˜ë¯¸(in-context)ë¥¼ ëª¨ë¸ì´ ì´í•´í•˜ê³ (learning), ì´ì— ëŒ€í•œ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ê²ƒ


â†’ Zero-shot learning, One-shot Learning, Few-shot Learning

