---
layout: single
date: 2023-07-18
title: "CS224n - Lecture 9 (Self-Attention, Transformer)"
use_math: true
tags: [ê°•ì˜/ì±… ì •ë¦¬, ]
categories: [AI, ]
---

> âœ… **Index**  
> 1. Problem of RNNs (Recurrent models)  
> 2. Review of Attention  
> 3. Self-Attention  
> 4. Structure of Transformer



## Problem of RNNs (Recurrent models)


**RNNì˜ íŠ¹ì§• : ìˆœì°¨ì  ì—°ì‚°**

- ì™¼ìª½ â†’ ì˜¤ë¥¸ìª½
- present time step(t)ì˜ hidden state â†’ future time step(t+1)ì˜ hidden state

![0](/assets/img/2023-07-18-CS224n---Lecture-9-(Self-Attention,-Transformer).md/0.png)_chefì™€ was ì‚¬ì´ sequenceì— ëŒ€í•œ ì—°ì‚°ì´ í•„ìš”_


**ê°€ê¹Œìš´ ë‹¨ì–´ë“¤ì˜ ì˜ë¯¸ì— ëŒ€í•´ ì˜í–¥**ì„ ë§ì´ ì£¼ê³  ë°›ì•„, word embeddingì— ê²°ê³¼ê°€ ì˜ ë°˜ì˜ëœë‹¤ëŠ” ì¥ì ì´ ìˆì§€ë§Œ, ë‹¨ì  ì—­ì‹œ ì¡´ì¬í•˜ëŠ”ë°,

1. **Long distance dependency problem**

	input sequenceì˜ ê¸¸ì´ê°€ ê¸¸ì–´ì§ˆìˆ˜ë¡, ì¦‰ **ë¨¼ ê±°ë¦¬ì˜ ë‹¨ì–´ì¼ìˆ˜ë¡ ìƒí˜¸ì‘ìš©ì´ ì–´ë ¤ì›€** 


	**gradient vanishing**ì˜ ë¬¸ì œë¡œ dependency í•™ìŠµì´ ì–´ë µë‹¤

2. **Lack of Parallelizability**

	ìˆœì°¨ì  ì—°ì‚° ë•Œë¬¸ì—, **GPUë¥¼ í™œìš©í•œ ë³‘ë ¬ ì—°ì‚°ì´ ë¶ˆê°€ëŠ¥**í•˜ë‹¤. 


	íŠ¹ì • hidden stateì˜ **ì—°ì‚°ì„ ìœ„í•´** **ì´ì „ hidden stateê°€ í•„ìš”**


	**O(sequence length)**


	(recurrent modelì˜ ì¼ë°˜ì ì¸ ë¬¸ì œ)


![1](/assets/img/2023-07-18-CS224n---Lecture-9-(Self-Attention,-Transformer).md/1.png)_í•´ë‹¹ hidden state ì—°ì‚°ë˜ê¸° ì „ ê¹Œì§€ì˜ ìµœì†Œ ì—°ì‚° íšŸìˆ˜_



#### â†’ ê·¸ë ‡ê¸° ë•Œë¬¸ì—, **Modelì´ë‚˜ Dataê°€ ì»¤ì§ˆìˆ˜ë¡ ë” í° ì•½ì **ì´ ë¨



### How about word windows?


word window ë°©ì‹ë„ local contextë¥¼ aggregate í•  ìˆ˜ ìˆë‹¤.


â†’ sequence lengthê°€ ì¦ê°€ëœë‹¤ê³  í•˜ë”ë¼ë„, ë³‘ë ¬ì²˜ë¦¬ê°€ ë¶ˆê°€ëŠ¥í•œ ì—°ì‚° ì¦ê°€ X


(í•œë²ˆì— ì—¬ëŸ¬ windowì— ëŒ€í•œ ë³‘ë ¬ ì—°ì‚° ê°€ëŠ¥)


![2](/assets/img/2023-07-18-CS224n---Lecture-9-(Self-Attention,-Transformer).md/2.png)_í•´ë‹¹ hidden state ì—°ì‚°ë˜ê¸° ì „ ê¹Œì§€ì˜ ìµœì†Œ ì—°ì‚° íšŸìˆ˜_


í•˜ì§€ë§Œ, long distanceì— ëŒ€í•œ **lack of dependency ë¬¸ì œëŠ” í•´ê²°ë˜ì§€ ì•ŠìŒ**


![3](/assets/img/2023-07-18-CS224n---Lecture-9-(Self-Attention,-Transformer).md/3.png)


ì˜ˆë¥¼ ë“¤ì–´ h1ì™€ hkì˜ dependencyë¥¼ ë°˜ì˜í•˜ê¸° ìœ„í•´ì„  window layerë¥¼ ë†’ì´ ìŒ“ì•„ì•¼ í•¨


ìƒí˜¸ì‘ìš©í•  ìˆ˜ ìˆëŠ” ìµœëŒ€ ê¸¸ì´ëŠ” **ì „ì²´ sequenceì˜ ê¸¸ì´ / window size** 


**ë¶„ëª… í•œê³„ì ì´ ì¡´ì¬**


â†’ long distance dependency : O


    lack of parallelizability :  X (not solved)


_â€œAttentionì€ ì´ ë‘ ê°€ì§€ í•œê³„ì ì„ ëª¨ë‘ í•´ê²°í•  ìˆ˜ ìˆëŠ” solutionâ€_



## Review of Attention


ê¸°ì¡´ Attentionì€ **decoderì—ì„œ encoderë¡œ query**ë¥¼ ë‚ ë ¤, **attention scoreë¥¼ í†µí•´ ì£¼ëª©í•´ì•¼ í•  ë¶€ë¶„ì„ íŒŒì•…**í•˜ëŠ” mechanismì´ì—ˆìŠµë‹ˆë‹¤.


---


<From Lecture 7>



#### Attention Score


![4](/assets/img/2023-07-18-CS224n---Lecture-9-(Self-Attention,-Transformer).md/4.png)


<u>**í•˜ë‚˜ì˜ ë””ì½”ë”ì™€ ê° ì¸ì½”ë”ë¥¼ ë‚´ì í•˜ì—¬ ìŠ¤ì¹¼ë¼ ê°’ì„ êµ¬í•˜ë©´ ê·¸ê²ƒì´ ë°”ë¡œ ê°ê°ì˜ attention score**</u>ì´ë‹¤. 


ì¦‰, attention scoreëŠ” **í˜„ì¬ ì‹œì ì˜ ë””ì½”ë”ì˜ ì •ë³´**ì™€ **ì¸ì½”ë”ì˜ ë§¤ ì‹œì ì˜ ì •ë³´ ê°„ ìœ ì‚¬ë„**ë¥¼ ì˜ë¯¸



#### Attention Distribution


![5](/assets/img/2023-07-18-CS224n---Lecture-9-(Self-Attention,-Transformer).md/5.png)


<u>**attention scoreë¥¼ softmax í•¨ìˆ˜ì— í†µê³¼ì‹œì¼œ ìƒì„±í•œ í™•ë¥  ë¶„í¬**</u>ì´ë‹¤. 


ìœ„ì˜ ì˜ˆì‹œì—ì„œëŠ” ilì— ê°€ì¥ ë¶„í¬ê°€ ì§‘ì¤‘ë˜ì—ˆìœ¼ë¯€ë¡œ ê°€ì¥ ë¨¼ì € heë¥¼ ìƒì„±í•œë‹¤.



#### Attention Output


![6](/assets/img/2023-07-18-CS224n---Lecture-9-(Self-Attention,-Transformer).md/6.png)


<u>**attention distributionì„ ê°€ì¤‘ì¹˜ë¡œ í•˜ì—¬ ì¸ì½”ë”ì˜ ê° hidden stateë¥¼ ê°€ì¤‘í•©**</u>í•œ ê²ƒì´ë‹¤. 


attention outputì€ **ë†’ì€ attentionì„ ê°€ì§„ hidden stateì˜ ì •ë³´ë¥¼ í¬í•¨**í•˜ê³  ìˆë‹¤.


![7](/assets/img/2023-07-18-CS224n---Lecture-9-(Self-Attention,-Transformer).md/7.png)


ë§ˆì§€ë§‰ìœ¼ë¡œ <u>**attention outputê³¼ ë””ì½”ë”ì˜ hidden stateë¥¼ ê²°í•©í•˜ì—¬**</u><u>_**y**_</u><u>**^ë¥¼ ì‚°ì¶œ**</u>í•œë‹¤. 


ê·¸ë¦¬ê³  ê° ë””ì½”ë”ì—ì„œ ìœ„ì˜ ê³¼ì •ì„ ë°˜ë³µí•œë‹¤. 


---


ì •ë¦¬í•˜ìë©´,

1. ì¸ì½”ë” RNNì„ ê±°ì¹˜ë©´ì„œ hidden state ìƒì„± (recurrent)
2. ë””ì½”ë”ë¥¼ ê±°ì¹˜ë©´ì„œ í•´ë‹¹ time stepì˜ decoder inputê³¼ ì´ì „ ì¸ì½”ë” hidden stateë“¤ ê°„ì˜ dot productë¥¼ í†µí•´ attention scores íšë“ (recurrent)
3. í•´ë‹¹ time stepì˜ attention scoreë“¤ì„ softmaxë¥¼ ê±°ì¹˜ë©´ì„œ distributionì„ êµ¬í•¨
4. ê°€ì¥ í™•ë¥ ì´ ë†’ì€ outputì„ ë°›ì•„ì™€ ë‹¤ìŒ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡

	(timestep ì¦ê°€ì‹œí‚¤ë©´ì„œ 2~4 ë°˜ë³µ)


ì´ ê³¼ì •ì—ì„œ Attentionì€ **ê° wordâ€™s representationì„ query**ë¡œ ì—¬ê²¨ **set of valuesì˜ ì •ë³´ì— ì ‘ê·¼í•˜ê³  ê·¸ë“¤ì„ í†µí•©**í•œë‹¤.


ì—¬ê¸°ì„œ **single sentenceì— ëŒ€í•œ attention**ì— ëŒ€í•´ ìƒê°í•´ë³´ì


number of unparallelizable operationsëŠ” sequence lengthë¥¼ ì¦ê°€ì‹œí‚¤ì§€ ì•ŠìŒ


ëª¨ë“  wordsê°€ ëª¨ë“  layerì—ì„œ interact â†’ O(1)


ê¸°ì¡´ì˜ attentionì˜ 1, 2 ê³¼ì •ì´ recurrentí•œ ê²ƒì´ ë¬¸ì œ


â†’ self-attentionì€ ì´ëŸ° encoder, decoder êµ¬ì¡°ì—ì„œ ë²—ì–´ë‚˜ **ìê¸° ìì‹  ì•ˆì—ì„œ attention ê³¼ì •**ì„ ìˆ˜í–‰í•˜ë©´ì„œ **1, 2 ê³¼ì •ì„ parallelizedí•˜ê²Œ ì „í™˜í•œ ëª¨ë¸**ì…ë‹ˆë‹¤. 



## Self-Attention 


![8](/assets/img/2023-07-18-CS224n---Lecture-9-(Self-Attention,-Transformer).md/8.png)


encoder, decoderë¡œ ì´ë£¨ì–´ì§„ ê¸°ì¡´ attentionê³¼ëŠ” ë‹¤ë¥´ê²Œ, 


self ì¦‰, **ìê¸° ìì‹ ì— ëŒ€í•´ì„œ attentionì„ ìˆ˜í–‰**í•œë‹¤ëŠ” ëœ»ì…ë‹ˆë‹¤.


ìœ„ì— ì‚¬ì§„ì„ ë³´ë©´, **í•œ ë¬¸ì¥ì¸ input sequence ì•ˆì—ì„œ attention ê³¼ì •ì´ ìˆ˜í–‰**ë˜ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆìŒ


ëª¨ë“  wordsê°€ ëª¨ë“  layerì—ì„œ ìƒí˜¸ì‘ìš©í•˜ë¯€ë¡œ, **maximum interaction distance**ëŠ” **O(1)**


ê·¸ë ‡ë‹¤ë©´ ì´ì „ attentionì—ì„œ ìˆ˜í–‰ë˜ì—ˆë“¯ì´, query ì—­í• ì„ í–ˆë˜ ê° timestepì—ì„œì˜ decoderì˜ hidden stateì™€ encoderì˜ hidden state ë“±ë“±ì´ í•„ìš”í•œë° í•œ ë¬¸ì¥ ì•ˆì—ì„œ ì—­í• ì´ êµ¬ë¶„ë˜ì–´ì•¼ í•¨


â†’ ì—¬ê¸°ì„œ ë‚˜ì˜¨ ê°œë…ì´ **query, key, value**

<details>
  <summary>chat gpt ì„¤ëª…</summary>


in self-attention, the query, key, and value vectors can be thought of as the elements that are used to compute the attention scores. Specifically, given a query vector, the dot product of the query vector with each key vector generates a set of attention scores, which are then used to weight the value vectors to produce the final output.


In other words, the attention mechanism can be viewed as a process of assigning importance to different parts of the input sequence (the values) based on how relevant they are to the current element being processed (the query), with the relevance being determined by the similarity between the current element and the other elements in the sequence (the keys).


assign importance to **different parts of the input sequence (value)**


based on how relevant they are to the **current element being processed (query)**


relevance being determined by the similarity between the current element and the **other elements in the sequence** **(key)**


The keys are used to retrieve the relevant information from the input sequence by computing a dot product between the keys and the query vector.


The resulting attention scores indicate the importance of each position in the input sequence with respect to the current output position. The values are then weighted by these attention scores and combined to produce the context vector, which contains the information that is relevant to the current output position.



  </details>
self-attentionì— ëŒ€í•´ ì´í•´í•˜ë ¤ë©´, query, key, valueì— ëŒ€í•œ ì´í•´ë¥¼ í•´ì•¼ í•¨.



### Query, Key, Value


ì´ ì„¸ ìš”ì†ŒëŠ” ëª¨ë‘ input sequenceë¡œ ë¶€í„° ê³„ì‚°ë˜ì–´ ë‚˜ì˜¤ëŠ” ìš”ì†Œë“¤


ì´ **ì„¸ ìš”ì†Œ ê°„ì˜ computationì„ í†µí•´ì„œ self-attention ê³¼ì •ì´ ìˆ˜í–‰**ë¨


![9](/assets/img/2023-07-18-CS224n---Lecture-9-(Self-Attention,-Transformer).md/9.png)


ì´ ìš”ì†Œë“¤ì€ attention êµ¬ì¡°ì—ì„œì˜ 

- **Query - Decoderì˜ hidden state (í˜„ì¬ ìš°ë¦¬ê°€ ìœ ì¶”/ì²˜ë¦¬í•˜ëŠ” ì •ë³´ì— ëŒ€í•œ hidden state)**
- **Key - Encoderì˜ hidden states (Queryì™€ ì–¼ë§ˆë‚˜ ì—°ê´€ë˜ì–´ìˆëŠ”ì§€ ì²´í¬í•´ì•¼í•  Encoderì˜ LSTMì…€ì˜ hidden vectors)**
- **Value - Encoderì˜ hidden states (Keyì™€ Query ê°„ì˜ relevanceë¥¼ ì¸¡ì •í•´ì„œ input sequenceì˜ ê° ë¶€ë¶„ì— importance ë¶€ì—¬)**

ì— í•´ë‹¹í•©ë‹ˆë‹¤.


ì´ ì„¸ ìš”ì†ŒëŠ” **ëª¨ë‘ ê°™ì€ sourceë¡œ ë¶€í„° ë„ì¶œ**ë©ë‹ˆë‹¤.


sequence $\mathbf{x}_{1:n}$ ì† token $\mathbf{x}_i$ ì´ë¼ í•  ë•Œ, 

- **Query :** $\mathbf{q}_1, \mathbf{q}_2, ..., \mathbf{q}_T, ~~~\mathbf{q}_i \in \mathbb{R}^d, ~~ \mathbf{q}_i = Q\mathbf{x}_i, ~~~Q\in\mathbb{R}^{d\times d}$

ì´ê³ , ê° token $\mathbf{x}_j \in \{x_1, ...~, x_n\}$ ì— ëŒ€í•´ì„œ,

- **Key :** $\mathbf{k}_1, \mathbf{k}_2, ..., \mathbf{k}_T,~~~\mathbf{k}_j \in \mathbb{R }^d, ~~~\mathbf{k}_j = K\mathbf{x}_j,~~~K\in\mathbb{R}^{d\times d}$
- **Value :** $\mathbf{v}_1, \mathbf{v}_2, ..., \mathbf{v}_T,~~~ \mathbf{v}_j \in \mathbb{R}^d,~~~ \mathbf{v}_j = V\mathbf{v}_j,~~~V\in\mathbb{R}^{d\times d}$

ì´ë‹¤.


(dëŠ” hyperparameter) (iê°€ query, jê°€ key)


![10](/assets/img/2023-07-18-CS224n---Lecture-9-(Self-Attention,-Transformer).md/10.png)

1. $e_{ij} = q_i^Tk_j$ëŠ” **key-query ê°„ affinities** (**ìœ ì‚¬ë„**)
2. $\alpha_{ij}$ëŠ” **ê°** $\mathbf{v}_j$(**value**)ì˜ **ê¸°ì—¬ë„**(strength of contribution)ë¥¼ ì¡°ì ˆ

	ì–´ë–¤ dataë¥¼ ì£¼ëª©í•´ì•¼ í• ì§€ë¥¼ ì •í•˜ëŠ” ì—­í• 


	v_jê°€ ì¡´ì¬í•˜ëŠ” ì´ìœ ëŠ” a_ijì™€ëŠ” ë‹¤ë¥´ê²Œ jí•˜ë‚˜ë¥¼ ê´€í†µí•˜ëŠ” valueì˜ ì—­í• ì„ í•˜ê¸° ë•Œë¬¸


	Vê°€ ì—†ë‹¤ë©´, í•´ë‹¹ ì‹œí€€ìŠ¤ ì•ˆì—ì„œì˜ ì§ì ‘ì  ì—°ê²°ì— ëŒ€í•´ì„œë§Œ outputì´ ì—°ê´€ë¨. Jë¥¼ ê´€í†µí•˜ëŠ” valueê°€ ìˆì–´ì•¼ ì´ì „ ì •ë³´ê¹Œì§€ ë‹¤ ì•„ìš°ë¥¼ ìˆ˜ ìˆë‹¤

3. outputì¸ contextual representation $\mathbf{h}_i$ of $\mathbf{x}_i$ ëŠ” sequenceì˜ valuesì— ëŒ€í•œ linear combination (queryì— ëŒ€í•œ values)

![11](/assets/img/2023-07-18-CS224n---Lecture-9-(Self-Attention,-Transformer).md/11.png)


$\alpha_{ij}$ : scalar,  $v_j$ : vector (**d x 1**)


![12](/assets/img/2023-07-18-CS224n---Lecture-9-(Self-Attention,-Transformer).md/12.png)


$q_i,~~ k_j$ : vector (**d x 1**)


â‡’ **h_ië“¤ì„ concatenateí•´ì„œ ìµœì¢… outputì¸ h ìƒì„±**


---


**Keyì™€ Value ê°„ì˜ ê´€ê³„?**


â†’ word2vec skip-gramì˜ **center vector - context vector ê´€ê³„**ì™€ ì¼ì • ë¶€ë¶„ ìœ ì‚¬


 


![13](/assets/img/2023-07-18-CS224n---Lecture-9-(Self-Attention,-Transformer).md/13.png)


<word2vec skip-gramâ€™s center & context>


center â†’ word ê·¸ ìì²´


context â†’ wordê°€ ë‚˜íƒ€ë‚˜ëŠ” context


<self-attentionâ€™s key & value>


**key** â†’ used to **match the query** with appropriate value


**value** â†’ **information related to the input sequence**


---



### Sequence order problem - Position representations


ì§€ê¸ˆê¹Œì§€ ë‚˜ì˜¨ ì •ë³´ë¡œë§Œ íŒŒì•…í–ˆì„ ë•Œ, ë¬¸ì œëŠ” **sequence orderë¥¼ ë‚˜íƒ€ë‚¼ informationì´ ì—†ë‹¤ëŠ” ê²ƒ.** 


ê¸°ì¡´ attentionì˜ ê²½ìš°ëŠ” ì´ì „ hidden stateê°€ í˜„ì¬ hidden stateì— ë°˜ì˜ë˜ê¸° ë•Œë¬¸ì— ì´ëŸ° orderê°€ í‘œí˜„ëì§€ë§Œ, ìœ„ì—ëŠ” ê·¸ëŸ° ì—­í• ì„ ìˆ˜í–‰í•˜ëŠ” ìš”ì†Œê°€ X



#### Position representations


example> _â€œthe oven cooked the bread soâ€_


	            â€œ_the bread cooked the oven soâ€_


![14](/assets/img/2023-07-18-CS224n---Lecture-9-(Self-Attention,-Transformer).md/14.png)


![15](/assets/img/2023-07-18-CS224n---Lecture-9-(Self-Attention,-Transformer).md/15.png)


$\alpha_{so, n}$ ëŠ” **soê°€ query**ì¼ ë•Œ, **në²ˆì§¸ wordì— ëŒ€í•œ ê¸°ì—¬ë„** (index n)


$$
\alpha_{so, 0} = \frac{exp(\mathbf{q}_{so}^T\mathbf{k}_{the})}{exp(\mathbf{q}_{so}^T\mathbf{k}_{the})+...+exp(\mathbf{q}_{so}^T\mathbf{k}_{bread})}
$$


$$
\alpha_{ij} = \frac{exp(\mathbf{q}_i^T\mathbf{k}_j)}{\Sigma^n_{j^\prime=1}exp(\mathbf{q}_i^T\mathbf{k_{j^\prime}})}
$$


$\alpha \in \mathbb{R}^5$ëŠ” weight, ì´ë¥¼ í†µí•´ $\mathbf{h}_{so}$ë¥¼ ê³„ì‚°í•¨


ì—°ì‚°ê³¼ì •ì—ì„œ ë´¤ë“¯ì´, orderì— ëŒ€í•œ ê³ ë ¤ ì•„ì˜ˆ x



#### **sol** 


1) use vectors that are **already position-dependent as inputs**


$P\in\mathbb{R}^{N\times d}$, Nì€ maximum length of any sequence


($p_i \in \mathbb{R}^d$, for $i\in\{1, 2, ..., T\}$ â†’ **position vectors)**


**self-attention blockì—** $p_i$**ë¥¼ ë”í•´ì„œ ì‚¬ìš©**


![16](/assets/img/2023-07-18-CS224n---Lecture-9-(Self-Attention,-Transformer).md/16.png)


2) **change the self-attention operation itself**


â†’ $\alpha$ **ìì²´ë¥¼ ë°”ê¾¸ëŠ” ë°©ë²•**


![17](/assets/img/2023-07-18-CS224n---Lecture-9-(Self-Attention,-Transformer).md/17.png)


$\mathbf{k}_{1:n}\mathbf{q}_i \in \mathbb{R}^n$ are the original attention scores (keyì™€ query ê³±)



### Elementwise nonlinearity


**stack self-attention layers â†’ stacked LSTM layersë¡œ ê°€ëŠ¥?**


![18](/assets/img/2023-07-18-CS224n---Lecture-9-(Self-Attention,-Transformer).md/18.png)


No. ìœ„ì™€ ê°™ì´ ë‹¨ìˆœíˆ ë‘ self-attention layerë¥¼ ìŒ“ëŠ”ë‹¤ê³  í•´ì„œ non-linearityë¥¼ ë§Œë“¤ì–´ ë‚¼ ìˆ˜ ì—†ìŒ â†’ í•œ self-attention layerì™€ ì°¨ì´ê°€ ì—†ë‹¤


í•œ self-attention layer ì´í›„ì—, ë…ë¦½ì ìœ¼ë¡œ activation í•¨ìˆ˜ë¥¼ ë¨¹ì—¬ì•¼ í•¨


![19](/assets/img/2023-07-18-CS224n---Lecture-9-(Self-Attention,-Transformer).md/19.png)


$W_1 \in \mathbb{R}^{5d\times d}, W_2 \in \mathbb{R}^{d\times 5d}$



### Future masking


**autoregressive modeling**


training ê³¼ì •ì—ì„œ **ë¯¸ë˜ì˜ inputì„ í™•ì¸í•  ìˆ˜ ì—†ë„ë¡** masking (decoder)


ì¼ë°˜ì ì¸ Language Modelingì—ì„œ, wordë¥¼ ë‹¤ìŒ ìƒí™©ì—ì„œ ì˜ˆì¸¡ì„ í•œë‹¤.


![20](/assets/img/2023-07-18-CS224n---Lecture-9-(Self-Attention,-Transformer).md/20.png)


$f$ : sequenceë¥¼ $R^{|V|}$ë¡œ **mapping í•´ì£¼ëŠ” í•¨ìˆ˜**


ì¼ë°˜ì ì¸ ëª¨ë¸ì—ì„  ì˜ˆì¸¡ì„ ìˆ˜í–‰í•  ë•Œ ë¯¸ë˜ë¥¼ ë³¼ ìˆ˜ ì—†ë‹¤.


![21](/assets/img/2023-07-18-CS224n---Lecture-9-(Self-Attention,-Transformer).md/21.png)


![22](/assets/img/2023-07-18-CS224n---Lecture-9-(Self-Attention,-Transformer).md/22.png)


â‡’ -$\infin$ë¥¼ ë„£ì–´ì£¼ë©´ ë‚˜ì¤‘ì— softmax í†µê³¼ ì‹œ 0ì´ ë¨ (exponential)


í›„ì— transformer encoder-decoder êµ¬ì¡°ê°€ ë“±ì¥í•˜ëŠ”ë°, encoderê°€ ì•„ë‹Œ decoderì— future masking ê¸°ë²• ì‚¬ìš©. 


![23](/assets/img/2023-07-18-CS224n---Lecture-9-(Self-Attention,-Transformer).md/23.png)



### Summary


1) self-attention


2) position representations


3) elementwise nonlinearities


4) future masking (in LM) 



## Structure of Transformer


2023; most used architecture in NLP â†’ Transformer


Transformer â†’ stacked Blocksë¡œ êµ¬ì„±ë˜ì–´ ìˆëŠ” self-attention êµ¬ì¡°ì— ê¸°ë°˜í•œ ëª¨ë¸


![24](/assets/img/2023-07-18-CS224n---Lecture-9-(Self-Attention,-Transformer).md/24.png)


![25](/assets/img/2023-07-18-CS224n---Lecture-9-(Self-Attention,-Transformer).md/25.png)


**encoder**


â†’ input sequenceë¥¼ ì¸ì½”ë”©í•´ fixed-length vector representationìœ¼ë¡œ ë‚˜íƒ€ë‚´ëŠ” ê²ƒ


**decoder**


â†’ encoderì—ì„œ ìƒì„±ëœ tokenê³¼ ë‚´ë¶€ì—ì„œ ì´ì „ self-attentionì„ í†µí•´ ìƒì„±ëœ tokenì„ í†µí•´ output sequence tokenì„ ìƒì„±í•˜ëŠ” ê²ƒì´ ëª©ì 


4ê°€ì§€ íŠ¹ì§•

- **multi-head** self-attention
- **layer normalization**
- **residual connections**
- **attention scaling**


### Multi-head Self-Attention


![26](/assets/img/2023-07-18-CS224n---Lecture-9-(Self-Attention,-Transformer).md/26.png)

- ê°™ì€ input
- **key, query, valueë¥¼ ë‚˜ëˆ„ì–´** **ê°ê¸° ë‹¤ë¥¸ key, query, value**ë¥¼ ì •ì˜
- **self-attentionì„ ë™ì‹œì— ì—¬ëŸ¬ ë²ˆ ì§„í–‰ (parallelization)**
- **outputì„ concatenate**

![27](/assets/img/2023-07-18-CS224n---Lecture-9-(Self-Attention,-Transformer).md/27.png)


![28](/assets/img/2023-07-18-CS224n---Lecture-9-(Self-Attention,-Transformer).md/28.png)

- $h$ê°œì˜ heads
- $K^{(l)},Q^{(l)},V^{(l)} \in \mathbb{R}^{d\times d / h}$,    $l $ in $\{1, ..., h\}$
- head ë³„ë¡œ key, query, value matrix  $\mathbf{k}^{(l)}_{1:n},\mathbf{q}^{(l)}_{1:n},\mathbf{v}^{(l)}_{1:n}$ ê°ê° ì¡´ì¬

![29](/assets/img/2023-07-18-CS224n---Lecture-9-(Self-Attention,-Transformer).md/29.png)


$\mathbf{h}_i = O[\mathbf{v}_i^{(1)};...~; \mathbf{v}_i^{(h)}]$  ($O \in \mathbb{R}^{d\times d}$)


$\mathbf{h}_i^{(l)}$ ëŠ” $\frac{d}{h}$ dimensionì´ë¯€ë¡œ, ì´ë“¤ì„ ìµœì¢…ì ìœ¼ë¡œ concatenateí•´ì„œ ìµœì¢… $\mathbf{h}_i$ë¥¼ êµ¬í•¨


$\mathbf{h}_i$ë¥¼ concatenateí•´ì„œ ìµœì¢…ì ìœ¼ë¡œ $\mathbf{h}$ ìƒì„±



#### Sequence-tensor form


ê° head outputì˜ ì¶•ì†Œëœ ì°¨ì›ì„ ì–»ëŠ” ê³¼ì •


single headì˜ ê²½ìš°, $\mathbf{x}_{1:n} $  in  $\mathbb{R}^{n\times d}$

- value vectors â†’ $\mathbf{x}_{1:n}V$
- key vectors â†’ $\mathbf{x}_{1:n}K$
- query vectors â†’ $\mathbf{x}_{1:n}Q$

![30](/assets/img/2023-07-18-CS224n---Lecture-9-(Self-Attention,-Transformer).md/30.png)


(query $\cdot$ key)


![31](/assets/img/2023-07-18-CS224n---Lecture-9-(Self-Attention,-Transformer).md/31.png)


![32](/assets/img/2023-07-18-CS224n---Lecture-9-(Self-Attention,-Transformer).md/32.png)_single-head operation_



#### **ì´ê±¸ multi-head attentioní•œë‹¤ê³  ìƒê°í•´ë³´ì**

1. $\mathbf{x}_{1:n}Q,\mathbf{x}_{1:n}K,\mathbf{x}_{1:n}V$ë¥¼ $\mathbb{R}^{n,h,d/h}$ í˜•íƒœë¡œ reshape í•´ì•¼ í•¨ (d â†’ h x d/h ë¡œ)
2. matricesë¥¼ $\mathbb{R}^{h,n,d/h}$ë¡œ transpose (n, h, d/h â†’ h, n, d/h)

	(h sequences, n length, dimension d/h)


	â†’ **headë¥¼ ë§ˆì¹˜ batch ì²˜ëŸ¼ ì‚¬ìš©**í•˜ëŠ” íš¨ê³¼ ($\mathbf{q}_i, \mathbf{k}_i, \mathbf{v}_i$)


	â†’ ê°ê°ì´ lower-rankì—ì„œ ì—°ì‚°ë¨


![33](/assets/img/2023-07-18-CS224n---Lecture-9-(Self-Attention,-Transformer).md/33.png)_multi-head operation_



### Layer Normalization


â†’ modelì´ ë” ë¹ ë¥´ê²Œ trainí•  ìˆ˜ ìˆë„ë¡ ë•ëŠ” ê¸°ë²•

- hidden vector valuesì— ìˆëŠ” **uninformativeí•œ variationì„ ì¤„ì—¬ ë” ì•ˆì •ì ì¸ inputì„ ì „ë‹¬**
- **ê° layer** ì—ì„œì˜ **normalization**(mean & standard deviation)

$x \in \mathbb{R}^d$ ì„ ëª¨ë¸ ê°ê°ì˜ word vectorë¼ê³  í•˜ì


$\mu = \Sigma^d_{j=1}x_j~~~\mu\in\mathbb{R}$ : mean


$\sigma = \sqrt{\frac{1}{d}\Sigma^d_{j=1}(x_j-\mu)^2}~~~\sigma\in\mathbb{R}$ : standard deviation


$\gamma\in\mathbb{R}^d$  **gain** parameters


$\beta\in\mathbb{R}^d $  **bias** parameters


![34](/assets/img/2023-07-18-CS224n---Lecture-9-(Self-Attention,-Transformer).md/34.png)_ğœ–ì€ zero division í”¼í•˜ê¸° ìœ„í•¨; ìƒˆë¡­ê²Œ Output ì •ì˜_



#### Residual Connections


layerì˜ inputì„ ê·¸ layerì˜ outputì— ì¶”ê°€í•´ì£¼ëŠ” ê²ƒ


**â‡’ skip-connection**


**ê¸°ì¡´**


![35](/assets/img/2023-07-18-CS224n---Lecture-9-(Self-Attention,-Transformer).md/35.png)


$X^{(i)} = Layer(X^{(i-1)})$


**Residual Connections**


![36](/assets/img/2023-07-18-CS224n---Lecture-9-(Self-Attention,-Transformer).md/36.png)


$X^{(i)} = X^{(i-1)} + Layer(X^{(i-1)})$


![37](/assets/img/2023-07-18-CS224n---Lecture-9-(Self-Attention,-Transformer).md/37.png)_other representation_


**íš¨ê³¼**


1) add ì—°ì‚°ì€ **gradient vanishing ë¬¸ì œë¡œë¶€í„° ììœ ë¡­ë‹¤**


	â†’ ì–¸ì œë‚˜ **local gradientê°€ 1**


	â†’ **ë” ê¹Šì€ network**ë¥¼ êµ¬ì„±í•  ìˆ˜ ìˆìŒ


2) ì¦ë¶„ë³€í™˜ì„ í†µí•´ **ìƒˆë¡­ê²Œ randomly initialized layersë¡œ í•™ìŠµí•˜ì§€ ì•Šê³ ë„ ì¢‹ì€ í•™ìŠµ ê°€ëŠ¥**


	â†’ ì¦ë¶„ë³€í™˜: ë³€í™˜ì´ ì—°ì†ì ìœ¼ë¡œ ì¼ì–´ë‚˜ëŠ” ê²½ìš°ì— ë§¤ë²ˆ ê³„ì‚°ì„ ë‹¤ì‹œ í•˜ì§€ ì•Šê³ , ì§ì „ì˜ ë³€í™˜ìœ¼ë¡œ ì–»ì–´ì§„ ê°’ì— ë¹„êµì  ê°„ë‹¨í•œ ê³„ì‚°ì„ ì ìš©í•˜ì—¬ ë³€í™˜


![38](/assets/img/2023-07-18-CS224n---Lecture-9-(Self-Attention,-Transformer).md/38.png)


**Add**


![39](/assets/img/2023-07-18-CS224n---Lecture-9-(Self-Attention,-Transformer).md/39.png)


**â†’ pre-normalization (better)**


**Norm**


![40](/assets/img/2023-07-18-CS224n---Lecture-9-(Self-Attention,-Transformer).md/40.png)


**â†’ post-normalization**



### Attention logit scaling


**dimensionì´ ì»¤ì§ì— ë”°ë¼, dot productì˜ ê²°ê³¼ë„ ì»¤ì§€ëŠ” ê²½í–¥**ì´ ìˆë‹¤.


â†’ ì´ë¥¼ scaling í•´ì£¼ëŠ” ê²ƒ


ê¸°ì¡´ Transformerì˜ ìµœì¢… outputì€ softmaxë¡œ ì‚°ì¶œ


![41](/assets/img/2023-07-18-CS224n---Lecture-9-(Self-Attention,-Transformer).md/41.png)


**softmaxì˜ ê°’ì´ í•œ labelë¡œ ì ë¦°ë‹¤**ëŠ” ê²ƒ


â†’ íŠ¹ì • ë‹¨ì—ë§Œ ê°•í•˜ê²Œ attentionì´ ê°€í•´ì§


â†’ **input ë³€í™”ì— ë”°ë¥¸ output ë³€í™”ê°€ ì‘ì•„ì§€ëŠ” ê²ƒì´ë¯€ë¡œ, gradientê°€ ì‘ì•„ì§€ëŠ” íš¨ê³¼**


â†’ í•™ìŠµì— ì•…ì˜í–¥


ê·¸ëŸ¬ë¯€ë¡œ gradientë¥¼ ì˜ ì „íŒŒì‹œí‚¤ê¸° ìœ„í•´ì„œ dot productì˜ ê²°ê³¼ê°€ ë„ˆë¬´ ì»¤ì§€ì§€ ì•Šë„ë¡ scalingì„ ì·¨í•¨


![42](/assets/img/2023-07-18-CS224n---Lecture-9-(Self-Attention,-Transformer).md/42.png)



#### Transformer


![43](/assets/img/2023-07-18-CS224n---Lecture-9-(Self-Attention,-Transformer).md/43.png)



### Transformer Encoder


![44](/assets/img/2023-07-18-CS224n---Lecture-9-(Self-Attention,-Transformer).md/44.png)


(future masking ì‚¬ìš© x)

1. single sequence $\mathbf{w}_{1:n}$ input ë°›ìŒ
2. Embedding $E$  â†’  $\mathbf{x}_{1:n}$
3. Add Position Embedding (position vector ì¶”ê°€)
4. multi-head attention
5. Add & Norm (Residual connection + Layer Normalization)
6. Feed-Forward network
7. Add & Norm (Residual connection + Layer Normalization)

ì´ ê³¼ì •ì„ ê±°ì³ encodingì´ ìˆ˜í–‰ë¨


**Uses of the Transformer Encoder**


Transformer EncoderëŠ” autoregressivelyí•˜ê²Œ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” ìƒí™©ì´ ì•„ë‹ ë•Œ ì¢‹ì€ ì„±ëŠ¥ì„ ë°”ë¤¼í•¨


ì „ì²´ sequenceì— ëŒ€í•´ ê°•í•œ representationì„ ì›í•  ë•Œ in



### Transformer Decoder


![45](/assets/img/2023-07-18-CS224n---Lecture-9-(Self-Attention,-Transformer).md/45.png)


(future masking ì‚¬ìš©)

1. single sequence $\mathbf{y}_{1:m}$ input ë°›ìŒ
2. Embedding $E$
3. Add Position Embedding (position vector ì¶”ê°€)
4. Masked multi-head attention
5. Add & Norm (Residual connection + Layer Normalization)
6. Multi-head Attention
7. Add & Norm (Residual connection + Layer Normalization)
8. Feed-Forward
9. Add & Norm (Residual connection + Layer Normalization)
10. Linear
11. Softmax

**Uses of the Transformer Decoder**


â†’ GPT-2, GPT-3, BLOOM



### Transformer Encoder-Decoder


**inputì„ two sequence ë°›ìŒ**



#### cross-attention


**í•œ sequenceì—ì„œ keyì™€ value**ë¥¼, **ë‹¤ë¥¸ sequenceì—ì„œ queryë¥¼ ì •ì˜**


![46](/assets/img/2023-07-18-CS224n---Lecture-9-(Self-Attention,-Transformer).md/46.png)


(Encoderì˜ output hidden state ì´ìš©í•´ì„œ key, value ì •ì˜)


$\mathbf{h}^{(y)}$ of sequence $\mathbf{y}_{1:m}$ 


![47](/assets/img/2023-07-18-CS224n---Lecture-9-(Self-Attention,-Transformer).md/47.png)


ì¦‰, queryëŠ” y, keyì™€ valueëŠ” x


![48](/assets/img/2023-07-18-CS224n---Lecture-9-(Self-Attention,-Transformer).md/48.png)_ìœ„: query, ì•„ë˜: key, value_



#### **Application**

- **Language Modeling**: In language modeling, there is only one sequence, which is the input sequence. Therefore, cross-attention is not used in this task.
- **Text Classification**: In text classification, the two sequences used in cross-attention are the **input sequence and a fixed set of class embeddings**, which represent the possible output classes. The class embeddings are learned during training and are used to help the model attend to relevant information in the input sequence for the given classification task.
- **Question Answering**: In question answering, the two sequences used in cross-attention are **the input sequence and a set of question embeddings**. The question embeddings are learned during training and are used to help the model attend to relevant information in the input sequence for answering the given question.
- **Summarization**: In summarization, the two sequences used in cross-attention are the encoder output sequence and the decoder input sequence. The encoder output sequence contains the **contextual representation of the input sequence**, while the **decoder input sequence contains the summary generated so far**. The cross-attention mechanism allows the decoder to attend to relevant parts of the input sequence while generating the summary.


### Great Results with Transformers


![49](/assets/img/2023-07-18-CS224n---Lecture-9-(Self-Attention,-Transformer).md/49.png)


**machine translation**


![50](/assets/img/2023-07-18-CS224n---Lecture-9-(Self-Attention,-Transformer).md/50.png)


**document generation**



### ê°œì„ ì ?


![51](/assets/img/2023-07-18-CS224n---Lecture-9-(Self-Attention,-Transformer).md/51.png)


**sequence length Tì— ë”°ë¥¸** $O(T^2d)$**ì˜ ê¸‰ê²©í•œ ìƒìŠ¹ì´ ë¬¸ì œ**


**â‡’ í•´ê²°ì±…: Linformer**


![52](/assets/img/2023-07-18-CS224n---Lecture-9-(Self-Attention,-Transformer).md/52.png)


**â‡’ í•´ê²°ì±…: BigBird**


![53](/assets/img/2023-07-18-CS224n---Lecture-9-(Self-Attention,-Transformer).md/53.png)

