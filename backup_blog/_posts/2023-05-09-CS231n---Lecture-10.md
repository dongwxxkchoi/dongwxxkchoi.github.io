---
layout: single
date: 2023-05-09
title: "CS231n - Lecture 10"
use_math: true
author_profile: false
tags: [강의/책 정리, ]
categories: [AI, ]
---


### Recurrent Neural Networks (RNN)

- Vanilla Neural Networks는 고정된 크기의 Vector를 입력으로 받아, 고정된 크기의 Vector를 출력하여 Mapping을 수행하는 한계가 있다
- RNN은 Vector Sequence에 대해 연산을 수행할 수 있고, 더 구체적으로 만들 수 있다

![0](/assets/img/2023-05-09-CS231n---Lecture-10.md/0.png)

1. **One to One**

	Vanilla Neural Networks로 Fixed-Sized Input을 Fixed-Sized Output을 처리한다

2. **One to Many**

	Sequence Output (e.g. Image Captioning / Image → Sequence of Words)

3. **Many to One**

	Sequence Input (e.g. Sentiment Classification / Sequence of Words → Sentiment)

4. **Many to Many 1**

	Sequence Input & Output (e.g. Machine Translation / Sequence of Words in English→ Sequence of Words in French) 

5. **Many to Many 2**

	Synced Sequence Input & Output (Video Classification on Frame Level)



#### Sequential Processing of Non-Sequence Data


![1](/assets/img/2023-05-09-CS231n---Lecture-10.md/1.png)

- Fixed-Sized Input & Output이 필요한 상황에서도 RNN은 상당히 중요하게 사용된다
- `가변 과정(Processing)`인 경우에 RNN은 일부분씩 순차적으로 처리할 수 있기 때문에 유용하다.

![2](/assets/img/2023-05-09-CS231n---Lecture-10.md/2.png)



#### Recurrent Core Cell


![3](/assets/img/2023-05-09-CS231n---Lecture-10.md/3.png)

- Input x가 RNN으로 들어오면, 새로운 State Vector을 만들어내기 위해 Fixed Function을 사용하여 Input Vector와 Output Vector을 결합한다
- RNN 내부의 _`Hidden State`_에서 새로운 입력을 받아들여 Update되는 방식으로, 네트워크가 다양한 입출력을 다룰 수 있는 여지를 제공한다

![4](/assets/img/2023-05-09-CS231n---Lecture-10.md/4.png)


![5](/assets/img/2023-05-09-CS231n---Lecture-10.md/5.png)


> 👉🏻 1. Sequence Vector $x_t$를 Input으로 받는다  
> 2. 모든 Step마다 Hidden State를 업데이트한다  
> 3. Output을 내보낸다


$$
h_t = f_w(h_{t-1}, x_t) \\ \downarrow \\ h_t =tanh(W_{hh}h_{t-1}+W_{xh}x_t) \\ y_t=W_{hy}h_t
$$

- 가중치 행렬 $W_{xh}$와 입력 $x_t$의 곱으로 나타내고, 가중치 행렬 $W_{hh}$은 이전 Hidden State 값인 $h_{t-1}$와 곱해진다
- 두 입력 $(h, x)$에 대한 행렬 곱 연산이 있고, 두 연산 결과 값을 더해준다 ($tanh$ → Non-Linearity)
- RNN은 Hidden State를 가지며 Recurrently Feed Back하는 특징을 갖고 있다


#### RNN : Computational Graph


![6](/assets/img/2023-05-09-CS231n---Lecture-10.md/6.png)


> 💡 - $f_W(h_0, x_1)=h_1$  
>   
> - $f_W(h_1, x_2)=h_2$  
>   
> - $f_W(h_{t-1}, x_t)=h_t$

	- $f_W(h_0, x_1)=h_1$

		함수 $f_W$의 Input으로 대부분 0으로 초기화 시키는 Initial Hidden State인 $h_0$와 입력값 $x_1$이 들어오고 Output으로 $h_1$이 나온다

	- $f_W(h_1, x_2)=h_2$

		다음에는 Hidden State인 $h_1$와 입력값 $x_2$이 들어오고 Output으로 $h_2$이 나온다

	- $f_W(h_{t-1}, x_t)=h_t$

		앞서 살펴본 과정의 반복으로 $f_W$의 Input으로 Previous Hidden State인 $h_{t-1}$와 입력값 $x_t$이 들어오고 Output으로 $h_t$이 나온다

- 이 과정에서 $h_{t-1}$와 $x_t$는 달라지지면 가중치 행렬 $W$는 매번 동일하게 사용된다.
- RNN 모델의 Backprop을 위한 행렬 $W$의 Gradient를 구하려면 각 Step에서의 $W$에 대한 Gradient를 전부 계산한 뒤에 이 값들을 모두 더해주면 된다


#### Many to Many


![7](/assets/img/2023-05-09-CS231n---Lecture-10.md/7.png)

- RNN의 Output $h_t$가 또 다른 네트워크의 Input으로 들어가서 $y_t$를 만들어내고, $y_t$는 매 Step의 Class Score라고 할 수 있다
- 각 Sequence마다 Ground Truth Label이 있다고 가정하면, 각 Step마다 개별적으로 $y_t$에 대한 Loss를 계산할 수 있다
- RNN의 최종 Loss는 각 개별 Loss들의 합이 된다
- Loss Flowing은 각 Step에서 이루어지고, 각 Time Step마다 가중치행렬 .$W$에 대한 Local Gradient를 계산할 수 있고, 최종 Gradient에 더한다


#### Many to One


![8](/assets/img/2023-05-09-CS231n---Lecture-10.md/8.png)

- 최종 Hidden State가 전체 Sequence 내용에 대한 일종의 요약이기 때문에 네트워크 최종 Hidden State에서만 결과값이 나온다


#### One to Many


![9](/assets/img/2023-05-09-CS231n---Lecture-10.md/9.png)

- Fixed-Sized Input을 받지만, Variably-Sized Output인 네트워크에서는 대부분 Fixed-Sized Input은 모델의 Initial Hidden State를 초기화 시키는 용도로 사용하고, RNN은 모든 Step에서 Output을 가진다
- Variably-Sized Output을 가지는 경우에도 그래프를 Unroll 할 수 있습니다.


#### Sequence to Sequence : Many to One + One to Many

- Machine Translation에 사용되는 Sequence to Sequence 모델이다

![10](/assets/img/2023-05-09-CS231n---Lecture-10.md/10.png)

- Encoder → Many to One
	- English Sentence와 같은 Variably-Sized Input을 Final Hidden State를 통해 전체 Sentence를 하나의 Vector로 요약한다
- Decoder → One to Many
	- Encoder에서 요약한 하나의 Vector를 Variably-Sized Output으로 내보낸다
- Variably-Sized Output은 매 Step마다 적절한 단어를 출력하고, 전체 Computational Graph를 풀어서 전체 학습 과정을 해석해보면 Output Sentence의 각 Loss들을 합해 Backprob을 진행한다


#### Character-Level Natural Language Model


![11](/assets/img/2023-05-09-CS231n---Lecture-10.md/11.png)

- 네트워크는 문자열 Sequence를 읽어 오고, 현재 문맥에서 다음 문제를 예측해야 한다
- `‘hello’`라는 단어를 제시하면 List로는 `[h, e, l, o]`이다
- **Train Time**
	- Training Sequence의 각 단어를 입력으로 넣어줘야 해서, 'hello'가 RNN의 $x_t$이고, Vocanulary는 4개이다
	- 각 글자는 하나의 Vector로 표현이 가능하며, 이 Vector는 1이 하나 있고, 나머지는 0인 Vector로 One-Hot Encoding 방식으로 표현한다
	- Forward Pass에서 네트워크는 첫 번째 Step에서 입력 문자 ‘h’가 들어오고, 첫 번째 RNN Cell로는 'h'가 들어가고 ‘h’ 다음에 나올 문자를 예측하는 $y_h$를 출력한다
	- 이 과정을 반복하며 모델을 다양한 문장으로 학습 시킨다면 결국 모델은 이전 문장의 문맥을 참고하여 다음 문자가 무엇인지를 학습한다
- **Test Time**
	- Training 모델을 활용할 수 있는 방법들 중 하나는 Model로부터 Sampling하는 것이다
	- Train Time에 모델이 봤을 법한 문장을 모델 스스로 생성해 내는 것이다
	- Test Time에서는 이 결과(Score)를 Sampling에 이용하고, Score를 확률 분포로 표현하기 위해 Softmax 함수를 사용할 수 있다
	- 확률 분포에서 'e'가 나왔고, 이를 다음 스텝의 네트워크 입력으로 넣어줍니다. 'e'를 다시 vector [0,1,0,0]으로 만들어주고 그 다음 Input으로 넣어주면 네트워크는 두 번째 출력을 만들어낸다
	- 이렇게 학습된 모델만 가지고 새로운 문장을 만들어 내기 위해 이 과정을 반복한다
	- 전체 문장을 만들기 위해  Time Step마다 확률 분포에서 문자를 하나씩 뽑아낸다

**샘플링이 뭐죠????**



#### Backpropagation Through Time


![12](/assets/img/2023-05-09-CS231n---Lecture-10.md/12.png)

- Forward Pass의 경우, 전체 Sequence가 끝날 때까지 Output이 생기고, 반대로 Backward Pass에서도 전체 Sequence를 가지고 Loss를 계산한다
- 전체 Sequence가 아주 길다면 Gradient를 계산하기 위해 전체 문서를 다 거쳐야하고, 반복하는 과정이 아주 느리고, 메모리 부족의 문제로 이어진다


#### Truncated Backpropagation


![13](/assets/img/2023-05-09-CS231n---Lecture-10.md/13.png)

- Train Time에 한 Step을 일정 단위로 자르고, 일정 Step만 Forward Pass하고, 이 Sub Sequence의 Loss를 계산하고 Gradient Step을 진행한다
- 이 전체 과정을 반복하고, 이전에 계산한 Hidden State는 계속 유지한다


#### RNN Language Model - Latent Structure


![14](/assets/img/2023-05-09-CS231n---Lecture-10.md/14.png)


![15](/assets/img/2023-05-09-CS231n---Lecture-10.md/15.png)

- 학습 초기에는 의미없는 문장만 뱉어내다가 학습을 시킬 수록 의미 있는 문장을 만들어 냅니다. 학습이 끝나면 셰익스피어 느낌을 내는 문장을 만들어 낸다
- 더 학습을 길게 시키면 훨씬 더 긴 문장도 만들어 낼 수 있습니다. 이것은 셰익스피어의 느낌을 따라하는 것 뿐만 아니라 문장의 구조를 완벽하게 익히는 과정과도 비슷하다
- 모델은 학습 과정 속에서 시퀀스 데이터의 숨겨진 구조**`Latent Structure`**을 알아서 학습합니다.


#### Searching for interpretable cells

- RNN에는 Hidden Vector가 있고, 이 Vector가 계속 업데이트 된다
- 이 Vector를 추출해보면 해석 가능한 의미 있는 것들이 올 수도 있지 않을까하는 추측이다

![16](/assets/img/2023-05-09-CS231n---Lecture-10.md/16.png)


![17](/assets/img/2023-05-09-CS231n---Lecture-10.md/17.png)

- Vector를 하나 뽑은 다음에 이 Sequence를 한 번 Forward 시켜보는 것으로 여기서 각 색깔은 Sequence를 읽는 동안 앞서 뽑은 Hidden Vector의 값을 의미한다

![18](/assets/img/2023-05-09-CS231n---Lecture-10.md/18.png)

- 따옴표(quote)를 찾는 벡터이다. 이런 식으로 그저 모델이 다음 문자를 예측하도록 학습 시켰을 뿐이지만 모델은 더 유용한 것을 학습하고 있다

![19](/assets/img/2023-05-09-CS231n---Lecture-10.md/19.png)

- 줄바꿈을 위해 현재 줄의 단어 갯수를 세는 듯해 보이는 Vector이다


###  Image Captioning


![20](/assets/img/2023-05-09-CS231n---Lecture-10.md/20.png)

- Input은 이미지이고, Output은 자연어로 된 Variably-Sized Caption이다
- Input으로 들어온 이미지가 CNN을 통해 요약된 이미지 정보가 들어있는 Vector를 출력한다
- RNN의 초기 Step의 입력으로 들어가서 Caption에 사용할 문자를 하나씩 만들어낸다

![21](/assets/img/2023-05-09-CS231n---Lecture-10.md/21.png)

1. CNN의 Input으로 이미지를 넣는다
	- 마지막 Layer에서 전체 이미지 정보를 요약한 4,096-D Vector를 출력한다
2. 초깃값 입력
	- 모델이 문장을 생성해 내기 위해 초기 값을 넣어준다

	$$
	h=tanh(W_{xh}x+W_{hh}h) \\ \downarrow \\ h = tanh(W_{xh}x+W_{hh}h+W_{ih}v)
	$$

	- 앞서 살펴본 RNN 모델의 가중치 행렬에 이미지 정보인 $W_{ih}$도 추가해야 한다
3. Vocabulary의 모든 Scores에 대한 분포를 계산
	- 엄청나게 큰 분포에서 Sampling을 하고 그 단어를 다음 Step의 입력으로 다시 넣어준다
	- Sampling 된 $y_0$가 들어가면 다시 Vocabulary에 대한 분포를 추정하고 다음 단어를 만든다
	- 모든 스텝이 종료되면 한 문장이 만들어진다


#### Image Captioning with Attention


![22](/assets/img/2023-05-09-CS231n---Lecture-10.md/22.png)

- CNN으로 벡터 하나를 만드는 것이 아닌 공간 정보를 가지고 있는 Grid Of Vector를 만든다
- Forward Pass시 매 스텝 Vocabularty에서 샘플링 할 때, 모델이 이미지에서 보고 싶은 위치에 대한 분포를 만들어낸다
- 이미지의 각 위치에 대한 분포는 Train Time에 모델이 어느 위치를 봐야하는 지에 대한 'Attention'이라 할 수 있다

![23](/assets/img/2023-05-09-CS231n---Lecture-10.md/23.png)

- Train을 마치면 모델이 Caption을 생성하기 위해 이미지의 Attention을 이동시키는 모습을 볼 수 있다
- Caption을 만들어 내기 위해 이미지 내에 다양한 곳들에 Attention을 주는 것을 확인할 수 있다


#### RNN with Attention


**Visual Question Answering**


![24](/assets/img/2023-05-09-CS231n---Lecture-10.md/24.png)


Visual Question Answering에서는 입력이 이미지와 이미지에 관한 질문으로 2개이다


**Many to One**


![25](/assets/img/2023-05-09-CS231n---Lecture-10.md/25.png)

- 모델은 자연어 문장(질문)을 입력으로 받아 RNN이 질문을 Vector로 요약하고, CNN이 이미지 요약을 한다
- RNN/CNN에서 나온 Vector를 조합하면 질문에 대한 분포를 예측할 수 있다


#### Multilayer RNNs


![26](/assets/img/2023-05-09-CS231n---Lecture-10.md/26.png)

- Multilayer RNN은 첫 번째 RNN으로 Input이 들어가서 첫 번째 Hidden State를 만들어 낸다. 이렇게 만들어진 Hidden State Sequence를 다른 RNN의 Input으로 넣어줄 수 있다
- RNN에서도 모델이 깊어질수록 다양한 문제들에서 성능이 더 좋아지고, 많은 경우 3~4 layer RNN을 사용한다
- 보통 엄청나게 깊은 RNN 모델을 사용하지는 않고, 일반적으로 2~4 layer RNN을 사용한다


#### Vanilla RNN Gradient Flow


![27](/assets/img/2023-05-09-CS231n---Lecture-10.md/27.png)

- RNN은 Backward Pass시 $h_t$에 대한 Loss의 미분값을 얻고, Loss에 대한 $h_{t-1}$의 미분값을 계산하게 된다.
- Gradient가 $tanh$ `Gate`를 타고 흘러가고, `Mat Mul Gate`를 통과하고 Back Propagation은 결국 이 Transpose(가중치 행렬)을 곱하게 된다
- 이는 매번 Vanilla RNN Cells를 하나 통과할 때마다 가중치 행렬의 일부를 곱하게 된다는 의미이다
- RNN이 여러 Sequence의 Cell을 쌓아 올리는 사실을 고려하면 Gradient가 RNN 모델의 Layers Sequence를 통해 어떤 방식으로 전달되는지 생각해 볼 수 있다

![28](/assets/img/2023-05-09-CS231n---Lecture-10.md/28.png)

- 만약 우리가 $h_0$에 대한 Gradient를 구하고자 한다면 결국 모든 RNN Cells를 거쳐야 하는데, Cell이 하나를 통과할 때마다 각 Cell의 행렬 W transpose factors가 관여하고, $h_0$의 Gradient를 계산하는 식을 써보면 아주 많은 가중치 행렬이 개입하게 되며 안 좋다
- 만약 곱해지는 값이 1보다 큰 경우라면 점점 값이 커질 것이고, 1보다 작은 경우라면 점점 작아져서 0이 된다
	- `행렬의 특이값이 1보다 크다면` $h_0$의 Gradient는 아주 커지게 되고, 이를 Exploding Gradients라고 한다. Back Propagation시 레이어가 깊어질 수록 Gradient가 기하급수적으로 증가하는 현상이다
	- `행렬의 특이값이 1보다 작다면` $h_0$의 Gradeint가 아주 작아지게 되고, 이를 Vanishing Gradients라고 한다

	**→ 만약 두 상황이 일어나지 않으려면 곱해지는 행렬의 특이값이** **`1`****이여야 한다**


**Gradient Clipping**



{% raw %}
```python
grad_norm = np.sum(grad * grad)
if grad_norm > threshold:
	grad *= (threshold / grad_norm)
```
{% endraw %}


- Gradient Clipping은 Gradient를 계산하고 L2-Norm이 임계값보다 큰 경우 그레디언트가 최대 임계값을 넘지 못하도록 조정한다
- 반대로 Vanishing Gradients를 다루려면 더 복잡한 RNN 아케텍처가 필요하고 이는 LSTM에 관한 것이다


### Long Short Term Memory (LSTM)


![29](/assets/img/2023-05-09-CS231n---Lecture-10.md/29.png)

- LSTM은 Exploding Gradients, Vanishing Gradients 문제를 완화시키기 위해 디자인된 모델이다
- LSTM에는 한 Cell 당 Hidden State인 $h_t$와 Cell State인 $c_t$가 있다
- Cell State는 LSTM 내부에만 존재하며 밖에 노출되지 않는 변수이다. LSTM도 $h_{t-1}, x_t$ 2개를 입력으로 받고 4개의 Gates를 계산한다
- 이 Gates를 $c_t$를 업데이트하는데 이용하고 $c_t$로 다음 스텝의 Hidden State를 업데이트한다

![30](/assets/img/2023-05-09-CS231n---Lecture-10.md/30.png)

1. $g$ **- Gate Gate** : Cell을 얼마나 포함시킬 것인가?
	- Input Cell을 얼마나 포함시킬지 결정하는 가중치이다
	- `i, f, o Gate`는 sigmoid를 사용해서 값이 [0, 1] 이다. 하지만 `g Gate`는 tanh를 사용해서 값이 [-1, 1] 이다
2. $f$ **- Forget Gate** : 이전 Cell에 대한 정보를 얼마나 지울 것인가?
	- Previous Cell State$(c_{t-1})$은 Forget Gate와 Element-Wise Multiplication하고 결과 벡터$(c_{t-1})$는 0 또는 1 일 것이다
	- 따라서 Forget Gate=0인 Element는 Previous Cell State를 잊고, Forget Gate=1인 Element는 Previous Cell State를 기억한다
3. $i$ **- Input Gate** : Cell에 대한 입력 ($x_t$에 대한 가중치)
	- 각 Element에 대해 이 Cell State를 사용하고 싶으면 1이 되고, 쓰고 싶지 않으면 0이 된다
4. $o$ **- Output Gate** : $c_t$를 얼마나 밖으로 드러낼 것인가?
	- Hidden Gate를 계산할 때 Cell State$(c_t)$를 얼마나 노출시킬지를 결정한다

![31](/assets/img/2023-05-09-CS231n---Lecture-10.md/31.png)

- 모델의 Input으로 $c_{t-1}, h_{t-1}, x_t$가 들어온다
- $h_{t-1}$와 $x_t$를 쌓고 여기에 가중치 행렬 4개를 곱해 Gate를 만든다
- $f$는 $c_{t-1}$와 Element Wise Multiplication하고 $i, g$의 곱을 더해 Cell State를 만든다. $c_t$는 $tanh$를 거친 후 $o$와 곱해져서 다음 Hidden state인 $ h_t$를 만든다


#### LSTM Gradient Flow

- Gradient는 Upstream Gradient와 Forget Gate의 Element Wise Multiplication이다.

	→ $f$와 곱해지는 연산이 Matrix Multiplication이 아닌 Element-Wise Multiplication이라서 좋다


	→ Element-Wise Multiplication을 통해 매 Step $f$값이 변해서 Exploding / Vanishing Gradient 문제를 쉽게 해결한다


	→ 최종 Hidden State인 $h_t$를 가장 첫 Cell State$(c_0)$까지 Back Propagation하면, RNN처럼 매 Step마다 $tanh$를 거칠 필요없이 단 한 번만 $tanh$를 거치면 된다


![32](/assets/img/2023-05-09-CS231n---Lecture-10.md/32.png)

- Gradient가 모델의 종단이 Loss에서 가장 처음 Cell State$(c_0)$까지 흘러가며 방해를 덜 받는다
