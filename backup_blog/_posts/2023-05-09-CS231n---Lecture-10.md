---
layout: single
date: 2023-05-09
title: "CS231n - Lecture 10"
use_math: true
author_profile: false
tags: [ê°•ì˜/ì±… ì •ë¦¬, ]
categories: [AI, ]
---


### Recurrent Neural Networks (RNN)

- Vanilla Neural NetworksëŠ” ê³ ì •ëœ í¬ê¸°ì˜ Vectorë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„, ê³ ì •ëœ í¬ê¸°ì˜ Vectorë¥¼ ì¶œë ¥í•˜ì—¬ Mappingì„ ìˆ˜í–‰í•˜ëŠ” í•œê³„ê°€ ìˆë‹¤
- RNNì€ Vector Sequenceì— ëŒ€í•´ ì—°ì‚°ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆê³ , ë” êµ¬ì²´ì ìœ¼ë¡œ ë§Œë“¤ ìˆ˜ ìˆë‹¤

![0](/assets/img/2023-05-09-CS231n---Lecture-10.md/0.png)

1. **One to One**

	Vanilla Neural Networksë¡œ Fixed-Sized Inputì„ Fixed-Sized Outputì„ ì²˜ë¦¬í•œë‹¤

2. **One to Many**

	Sequence Output (e.g. Image Captioning / Image â†’ Sequence of Words)

3. **Many to One**

	Sequence Input (e.g. Sentiment Classification / Sequence of Words â†’ Sentiment)

4. **Many to Many 1**

	Sequence Input & Output (e.g. Machine Translation / Sequence of Words in Englishâ†’ Sequence of Words in French) 

5. **Many to Many 2**

	Synced Sequence Input & Output (Video Classification on Frame Level)



#### Sequential Processing of Non-Sequence Data


![1](/assets/img/2023-05-09-CS231n---Lecture-10.md/1.png)

- Fixed-Sized Input & Outputì´ í•„ìš”í•œ ìƒí™©ì—ì„œë„ RNNì€ ìƒë‹¹íˆ ì¤‘ìš”í•˜ê²Œ ì‚¬ìš©ëœë‹¤
- `ê°€ë³€ ê³¼ì •(Processing)`ì¸ ê²½ìš°ì— RNNì€ ì¼ë¶€ë¶„ì”© ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì— ìœ ìš©í•˜ë‹¤.

![2](/assets/img/2023-05-09-CS231n---Lecture-10.md/2.png)



#### Recurrent Core Cell


![3](/assets/img/2023-05-09-CS231n---Lecture-10.md/3.png)

- Input xê°€ RNNìœ¼ë¡œ ë“¤ì–´ì˜¤ë©´, ìƒˆë¡œìš´ State Vectorì„ ë§Œë“¤ì–´ë‚´ê¸° ìœ„í•´ Fixed Functionì„ ì‚¬ìš©í•˜ì—¬ Input Vectorì™€ Output Vectorì„ ê²°í•©í•œë‹¤
- RNN ë‚´ë¶€ì˜Â _`Hidden State`_ì—ì„œ ìƒˆë¡œìš´ ì…ë ¥ì„ ë°›ì•„ë“¤ì—¬ Updateë˜ëŠ” ë°©ì‹ìœ¼ë¡œ, ë„¤íŠ¸ì›Œí¬ê°€ ë‹¤ì–‘í•œ ì…ì¶œë ¥ì„ ë‹¤ë£° ìˆ˜ ìˆëŠ” ì—¬ì§€ë¥¼ ì œê³µí•œë‹¤

![4](/assets/img/2023-05-09-CS231n---Lecture-10.md/4.png)


![5](/assets/img/2023-05-09-CS231n---Lecture-10.md/5.png)


> ğŸ‘‰ğŸ» 1. Sequence Vector $x_t$ë¥¼ Inputìœ¼ë¡œ ë°›ëŠ”ë‹¤  
> 2. ëª¨ë“  Stepë§ˆë‹¤ Hidden Stateë¥¼ ì—…ë°ì´íŠ¸í•œë‹¤  
> 3. Outputì„ ë‚´ë³´ë‚¸ë‹¤


$$
h_t = f_w(h_{t-1}, x_t) \\ \downarrow \\ h_t =tanh(W_{hh}h_{t-1}+W_{xh}x_t) \\ y_t=W_{hy}h_t
$$

- ê°€ì¤‘ì¹˜ í–‰ë ¬ $W_{xh}$ì™€ ì…ë ¥ $x_t$ì˜ ê³±ìœ¼ë¡œ ë‚˜íƒ€ë‚´ê³ , ê°€ì¤‘ì¹˜ í–‰ë ¬ $W_{hh}$ì€ ì´ì „ Hidden State ê°’ì¸ $h_{t-1}$ì™€ ê³±í•´ì§„ë‹¤
- ë‘ ì…ë ¥ $(h, x)$ì— ëŒ€í•œ í–‰ë ¬ ê³± ì—°ì‚°ì´ ìˆê³ , ë‘ ì—°ì‚° ê²°ê³¼ ê°’ì„ ë”í•´ì¤€ë‹¤ ($tanh$ â†’ Non-Linearity)
- RNNì€ Hidden Stateë¥¼ ê°€ì§€ë©°Â Recurrently Feed Backí•˜ëŠ” íŠ¹ì§•ì„ ê°–ê³  ìˆë‹¤


#### RNN : Computational Graph


![6](/assets/img/2023-05-09-CS231n---Lecture-10.md/6.png)


> ğŸ’¡ - $f_W(h_0, x_1)=h_1$  
>   
> - $f_W(h_1, x_2)=h_2$  
>   
> - $f_W(h_{t-1}, x_t)=h_t$

	- $f_W(h_0, x_1)=h_1$

		í•¨ìˆ˜ $f_W$ì˜ Inputìœ¼ë¡œ ëŒ€ë¶€ë¶„ 0ìœ¼ë¡œ ì´ˆê¸°í™” ì‹œí‚¤ëŠ” Initial Hidden Stateì¸ $h_0$ì™€ ì…ë ¥ê°’ $x_1$ì´ ë“¤ì–´ì˜¤ê³  Outputìœ¼ë¡œ $h_1$ì´ ë‚˜ì˜¨ë‹¤

	- $f_W(h_1, x_2)=h_2$

		ë‹¤ìŒì—ëŠ” Hidden Stateì¸ $h_1$ì™€ ì…ë ¥ê°’ $x_2$ì´ ë“¤ì–´ì˜¤ê³  Outputìœ¼ë¡œ $h_2$ì´ ë‚˜ì˜¨ë‹¤

	- $f_W(h_{t-1}, x_t)=h_t$

		ì•ì„œ ì‚´í´ë³¸ ê³¼ì •ì˜ ë°˜ë³µìœ¼ë¡œ $f_W$ì˜ Inputìœ¼ë¡œ Previous Hidden Stateì¸ $h_{t-1}$ì™€ ì…ë ¥ê°’ $x_t$ì´ ë“¤ì–´ì˜¤ê³  Outputìœ¼ë¡œ $h_t$ì´ ë‚˜ì˜¨ë‹¤

- ì´ ê³¼ì •ì—ì„œ $h_{t-1}$ì™€ $x_t$ëŠ” ë‹¬ë¼ì§€ì§€ë©´ ê°€ì¤‘ì¹˜ í–‰ë ¬ $W$ëŠ” ë§¤ë²ˆ ë™ì¼í•˜ê²Œ ì‚¬ìš©ëœë‹¤.
- RNN ëª¨ë¸ì˜ Backpropì„ ìœ„í•œ í–‰ë ¬Â $W$ì˜ Gradientë¥¼ êµ¬í•˜ë ¤ë©´ ê° Stepì—ì„œì˜Â $W$ì— ëŒ€í•œ Gradientë¥¼ ì „ë¶€ ê³„ì‚°í•œ ë’¤ì— ì´ ê°’ë“¤ì„ ëª¨ë‘ ë”í•´ì£¼ë©´ ëœë‹¤


#### Many to Many


![7](/assets/img/2023-05-09-CS231n---Lecture-10.md/7.png)

- RNNì˜ Output $h_t$ê°€ ë˜ ë‹¤ë¥¸ ë„¤íŠ¸ì›Œí¬ì˜ Inputìœ¼ë¡œ ë“¤ì–´ê°€ì„œ $y_t$ë¥¼ ë§Œë“¤ì–´ë‚´ê³ , $y_t$ëŠ” ë§¤ Stepì˜ Class Scoreë¼ê³  í•  ìˆ˜ ìˆë‹¤
- ê° Sequenceë§ˆë‹¤ Ground Truth Labelì´ ìˆë‹¤ê³  ê°€ì •í•˜ë©´, ê° Stepë§ˆë‹¤ ê°œë³„ì ìœ¼ë¡œ $y_t$ì— ëŒ€í•œ Lossë¥¼ ê³„ì‚°í•  ìˆ˜ ìˆë‹¤
- RNNì˜ ìµœì¢… LossëŠ” ê° ê°œë³„ Lossë“¤ì˜ í•©ì´ ëœë‹¤
- Loss Flowingì€ ê° Stepì—ì„œ ì´ë£¨ì–´ì§€ê³ , ê° Time Stepë§ˆë‹¤ ê°€ì¤‘ì¹˜í–‰ë ¬ .$W$ì— ëŒ€í•œ Local Gradientë¥¼ ê³„ì‚°í•  ìˆ˜ ìˆê³ , ìµœì¢… Gradientì— ë”í•œë‹¤


#### Many to One


![8](/assets/img/2023-05-09-CS231n---Lecture-10.md/8.png)

- ìµœì¢… Hidden Stateê°€ ì „ì²´ Sequence ë‚´ìš©ì— ëŒ€í•œ ì¼ì¢…ì˜ ìš”ì•½ì´ê¸° ë•Œë¬¸ì— ë„¤íŠ¸ì›Œí¬ ìµœì¢… Hidden Stateì—ì„œë§Œ ê²°ê³¼ê°’ì´ ë‚˜ì˜¨ë‹¤


#### One to Many


![9](/assets/img/2023-05-09-CS231n---Lecture-10.md/9.png)

- Fixed-Sized Inputì„ ë°›ì§€ë§Œ, Variably-Sized Outputì¸ ë„¤íŠ¸ì›Œí¬ì—ì„œëŠ” ëŒ€ë¶€ë¶„ Fixed-Sized Inputì€ ëª¨ë¸ì˜ Initial Hidden Stateë¥¼ ì´ˆê¸°í™” ì‹œí‚¤ëŠ” ìš©ë„ë¡œ ì‚¬ìš©í•˜ê³ , RNNì€ ëª¨ë“  Stepì—ì„œ Outputì„ ê°€ì§„ë‹¤
- Variably-Sized Outputì„ ê°€ì§€ëŠ” ê²½ìš°ì—ë„ ê·¸ë˜í”„ë¥¼ Unroll í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.


#### Sequence to Sequence : Many to One + One to Many

- Machine Translationì— ì‚¬ìš©ë˜ëŠ” Sequence to Sequence ëª¨ë¸ì´ë‹¤

![10](/assets/img/2023-05-09-CS231n---Lecture-10.md/10.png)

- Encoder â†’ Many to One
	- English Sentenceì™€ ê°™ì€ Variably-Sized Inputì„ Final Hidden Stateë¥¼ í†µí•´ ì „ì²´ Sentenceë¥¼ í•˜ë‚˜ì˜ Vectorë¡œ ìš”ì•½í•œë‹¤
- Decoder â†’ One to Many
	- Encoderì—ì„œ ìš”ì•½í•œ í•˜ë‚˜ì˜ Vectorë¥¼ Variably-Sized Outputìœ¼ë¡œ ë‚´ë³´ë‚¸ë‹¤
- Variably-Sized Outputì€ ë§¤ Stepë§ˆë‹¤ ì ì ˆí•œ ë‹¨ì–´ë¥¼ ì¶œë ¥í•˜ê³ , ì „ì²´ Computational Graphë¥¼ í’€ì–´ì„œ ì „ì²´ í•™ìŠµ ê³¼ì •ì„ í•´ì„í•´ë³´ë©´ Output Sentenceì˜ ê° Lossë“¤ì„ í•©í•´ Backprobì„ ì§„í–‰í•œë‹¤


#### Character-Level Natural Language Model


![11](/assets/img/2023-05-09-CS231n---Lecture-10.md/11.png)

- ë„¤íŠ¸ì›Œí¬ëŠ” ë¬¸ìì—´ Sequenceë¥¼ ì½ì–´ ì˜¤ê³ , í˜„ì¬ ë¬¸ë§¥ì—ì„œ ë‹¤ìŒ ë¬¸ì œë¥¼ ì˜ˆì¸¡í•´ì•¼ í•œë‹¤
- `â€˜helloâ€™`ë¼ëŠ” ë‹¨ì–´ë¥¼ ì œì‹œí•˜ë©´ Listë¡œëŠ” `[h, e, l, o]`ì´ë‹¤
- **Train Time**
	- Training Sequenceì˜ ê° ë‹¨ì–´ë¥¼ ì…ë ¥ìœ¼ë¡œ ë„£ì–´ì¤˜ì•¼ í•´ì„œ, 'hello'ê°€ RNNì˜Â $x_t$ì´ê³ , VocanularyëŠ” 4ê°œì´ë‹¤
	- ê° ê¸€ìëŠ” í•˜ë‚˜ì˜ Vectorë¡œ í‘œí˜„ì´ ê°€ëŠ¥í•˜ë©°, ì´ VectorëŠ” 1ì´ í•˜ë‚˜ ìˆê³ , ë‚˜ë¨¸ì§€ëŠ” 0ì¸ Vectorë¡œ One-Hot Encoding ë°©ì‹ìœ¼ë¡œ í‘œí˜„í•œë‹¤
	- Forward Passì—ì„œ ë„¤íŠ¸ì›Œí¬ëŠ” ì²« ë²ˆì§¸ Stepì—ì„œ ì…ë ¥ ë¬¸ì â€˜hâ€™ê°€ ë“¤ì–´ì˜¤ê³ , ì²« ë²ˆì§¸ RNN Cellë¡œëŠ” 'h'ê°€ ë“¤ì–´ê°€ê³  â€˜hâ€™ ë‹¤ìŒì— ë‚˜ì˜¬ ë¬¸ìë¥¼ ì˜ˆì¸¡í•˜ëŠ” $y_h$ë¥¼ ì¶œë ¥í•œë‹¤
	- ì´ ê³¼ì •ì„ ë°˜ë³µí•˜ë©° ëª¨ë¸ì„ ë‹¤ì–‘í•œ ë¬¸ì¥ìœ¼ë¡œ í•™ìŠµ ì‹œí‚¨ë‹¤ë©´ ê²°êµ­ ëª¨ë¸ì€ ì´ì „ ë¬¸ì¥ì˜ ë¬¸ë§¥ì„ ì°¸ê³ í•˜ì—¬ ë‹¤ìŒ ë¬¸ìê°€ ë¬´ì—‡ì¸ì§€ë¥¼ í•™ìŠµí•œë‹¤
- **Test Time**
	- Training ëª¨ë¸ì„ í™œìš©í•  ìˆ˜ ìˆëŠ” ë°©ë²•ë“¤ ì¤‘ í•˜ë‚˜ëŠ” Modelë¡œë¶€í„° Samplingí•˜ëŠ” ê²ƒì´ë‹¤
	- Train Timeì— ëª¨ë¸ì´ ë´¤ì„ ë²•í•œ ë¬¸ì¥ì„ ëª¨ë¸ ìŠ¤ìŠ¤ë¡œ ìƒì„±í•´ ë‚´ëŠ” ê²ƒì´ë‹¤
	- Test Timeì—ì„œëŠ” ì´ ê²°ê³¼(Score)ë¥¼Â Samplingì— ì´ìš©í•˜ê³ , Scoreë¥¼ í™•ë¥  ë¶„í¬ë¡œ í‘œí˜„í•˜ê¸° ìœ„í•´ Softmax í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤
	- í™•ë¥  ë¶„í¬ì—ì„œ 'e'ê°€ ë‚˜ì™”ê³ , ì´ë¥¼ ë‹¤ìŒ ìŠ¤í…ì˜ ë„¤íŠ¸ì›Œí¬ ì…ë ¥ìœ¼ë¡œ ë„£ì–´ì¤ë‹ˆë‹¤. 'e'ë¥¼ ë‹¤ì‹œ vector [0,1,0,0]ìœ¼ë¡œ ë§Œë“¤ì–´ì£¼ê³  ê·¸ ë‹¤ìŒ Inputìœ¼ë¡œ ë„£ì–´ì£¼ë©´ ë„¤íŠ¸ì›Œí¬ëŠ” ë‘ ë²ˆì§¸ ì¶œë ¥ì„ ë§Œë“¤ì–´ë‚¸ë‹¤
	- ì´ë ‡ê²Œ í•™ìŠµëœ ëª¨ë¸ë§Œ ê°€ì§€ê³  ìƒˆë¡œìš´ ë¬¸ì¥ì„ ë§Œë“¤ì–´ ë‚´ê¸° ìœ„í•´ ì´ ê³¼ì •ì„ ë°˜ë³µí•œë‹¤
	- ì „ì²´ ë¬¸ì¥ì„ ë§Œë“¤ê¸° ìœ„í•´  Time Stepë§ˆë‹¤ í™•ë¥  ë¶„í¬ì—ì„œ ë¬¸ìë¥¼ í•˜ë‚˜ì”© ë½‘ì•„ë‚¸ë‹¤

**ìƒ˜í”Œë§ì´ ë­ì£ ????**



#### Backpropagation Through Time


![12](/assets/img/2023-05-09-CS231n---Lecture-10.md/12.png)

- Forward Passì˜ ê²½ìš°, ì „ì²´ Sequenceê°€ ëë‚  ë•Œê¹Œì§€ Outputì´ ìƒê¸°ê³ , ë°˜ëŒ€ë¡œ Backward Passì—ì„œë„ ì „ì²´ Sequenceë¥¼ ê°€ì§€ê³  Lossë¥¼ ê³„ì‚°í•œë‹¤
- ì „ì²´ Sequenceê°€ ì•„ì£¼ ê¸¸ë‹¤ë©´ Gradientë¥¼ ê³„ì‚°í•˜ê¸° ìœ„í•´ ì „ì²´ ë¬¸ì„œë¥¼ ë‹¤ ê±°ì³ì•¼í•˜ê³ , ë°˜ë³µí•˜ëŠ” ê³¼ì •ì´ ì•„ì£¼ ëŠë¦¬ê³ , ë©”ëª¨ë¦¬ ë¶€ì¡±ì˜ ë¬¸ì œë¡œ ì´ì–´ì§„ë‹¤


#### Truncated Backpropagation


![13](/assets/img/2023-05-09-CS231n---Lecture-10.md/13.png)

- Train Timeì— í•œ Stepì„ ì¼ì • ë‹¨ìœ„ë¡œ ìë¥´ê³ , ì¼ì • Stepë§Œ Forward Passí•˜ê³ , ì´ Sub Sequenceì˜ Lossë¥¼ ê³„ì‚°í•˜ê³  Gradient Stepì„ ì§„í–‰í•œë‹¤
- ì´ ì „ì²´ ê³¼ì •ì„ ë°˜ë³µí•˜ê³ , ì´ì „ì— ê³„ì‚°í•œ Hidden StateëŠ” ê³„ì† ìœ ì§€í•œë‹¤


#### RNN Language Model - Latent Structure


![14](/assets/img/2023-05-09-CS231n---Lecture-10.md/14.png)


![15](/assets/img/2023-05-09-CS231n---Lecture-10.md/15.png)

- í•™ìŠµ ì´ˆê¸°ì—ëŠ” ì˜ë¯¸ì—†ëŠ” ë¬¸ì¥ë§Œ ë±‰ì–´ë‚´ë‹¤ê°€ í•™ìŠµì„ ì‹œí‚¬ ìˆ˜ë¡ ì˜ë¯¸ ìˆëŠ” ë¬¸ì¥ì„ ë§Œë“¤ì–´ ëƒ…ë‹ˆë‹¤. í•™ìŠµì´ ëë‚˜ë©´ ì…°ìµìŠ¤í”¼ì–´ ëŠë‚Œì„ ë‚´ëŠ” ë¬¸ì¥ì„ ë§Œë“¤ì–´ ë‚¸ë‹¤
- ë” í•™ìŠµì„ ê¸¸ê²Œ ì‹œí‚¤ë©´ í›¨ì”¬ ë” ê¸´ ë¬¸ì¥ë„ ë§Œë“¤ì–´ ë‚¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ê²ƒì€ ì…°ìµìŠ¤í”¼ì–´ì˜ ëŠë‚Œì„ ë”°ë¼í•˜ëŠ” ê²ƒ ë¿ë§Œ ì•„ë‹ˆë¼ ë¬¸ì¥ì˜ êµ¬ì¡°ë¥¼ ì™„ë²½í•˜ê²Œ ìµíˆëŠ” ê³¼ì •ê³¼ë„ ë¹„ìŠ·í•˜ë‹¤
- ëª¨ë¸ì€ í•™ìŠµ ê³¼ì • ì†ì—ì„œ ì‹œí€€ìŠ¤ ë°ì´í„°ì˜ ìˆ¨ê²¨ì§„ êµ¬ì¡°**`Latent Structure`**ì„ ì•Œì•„ì„œ í•™ìŠµí•©ë‹ˆë‹¤.


#### Searching for interpretable cells

- RNNì—ëŠ” Hidden Vectorê°€ ìˆê³ , ì´ Vectorê°€ ê³„ì† ì—…ë°ì´íŠ¸ ëœë‹¤
- ì´ Vectorë¥¼ ì¶”ì¶œí•´ë³´ë©´ í•´ì„ ê°€ëŠ¥í•œ ì˜ë¯¸ ìˆëŠ” ê²ƒë“¤ì´ ì˜¬ ìˆ˜ë„ ìˆì§€ ì•Šì„ê¹Œí•˜ëŠ” ì¶”ì¸¡ì´ë‹¤

![16](/assets/img/2023-05-09-CS231n---Lecture-10.md/16.png)


![17](/assets/img/2023-05-09-CS231n---Lecture-10.md/17.png)

- Vectorë¥¼ í•˜ë‚˜ ë½‘ì€ ë‹¤ìŒì— ì´ Sequenceë¥¼ í•œ ë²ˆ Forward ì‹œì¼œë³´ëŠ” ê²ƒìœ¼ë¡œ ì—¬ê¸°ì„œ ê° ìƒ‰ê¹”ì€ Sequenceë¥¼ ì½ëŠ” ë™ì•ˆ ì•ì„œ ë½‘ì€ Hidden Vectorì˜ ê°’ì„ ì˜ë¯¸í•œë‹¤

![18](/assets/img/2023-05-09-CS231n---Lecture-10.md/18.png)

- ë”°ì˜´í‘œ(quote)ë¥¼ ì°¾ëŠ” ë²¡í„°ì´ë‹¤. ì´ëŸ° ì‹ìœ¼ë¡œ ê·¸ì € ëª¨ë¸ì´ ë‹¤ìŒ ë¬¸ìë¥¼ ì˜ˆì¸¡í•˜ë„ë¡ í•™ìŠµ ì‹œì¼°ì„ ë¿ì´ì§€ë§Œ ëª¨ë¸ì€ ë” ìœ ìš©í•œ ê²ƒì„ í•™ìŠµí•˜ê³  ìˆë‹¤

![19](/assets/img/2023-05-09-CS231n---Lecture-10.md/19.png)

- ì¤„ë°”ê¿ˆì„ ìœ„í•´ í˜„ì¬ ì¤„ì˜ ë‹¨ì–´ ê°¯ìˆ˜ë¥¼ ì„¸ëŠ” ë“¯í•´ ë³´ì´ëŠ” Vectorì´ë‹¤


###  Image Captioning


![20](/assets/img/2023-05-09-CS231n---Lecture-10.md/20.png)

- Inputì€ ì´ë¯¸ì§€ì´ê³ , Outputì€ ìì—°ì–´ë¡œ ëœ Variably-Sized Captionì´ë‹¤
- Inputìœ¼ë¡œ ë“¤ì–´ì˜¨ ì´ë¯¸ì§€ê°€ CNNì„ í†µí•´ ìš”ì•½ëœ ì´ë¯¸ì§€ ì •ë³´ê°€ ë“¤ì–´ìˆëŠ” Vectorë¥¼ ì¶œë ¥í•œë‹¤
- RNNì˜ ì´ˆê¸° Stepì˜ ì…ë ¥ìœ¼ë¡œ ë“¤ì–´ê°€ì„œ Captionì— ì‚¬ìš©í•  ë¬¸ìë¥¼ í•˜ë‚˜ì”© ë§Œë“¤ì–´ë‚¸ë‹¤

![21](/assets/img/2023-05-09-CS231n---Lecture-10.md/21.png)

1. CNNì˜ Inputìœ¼ë¡œ ì´ë¯¸ì§€ë¥¼ ë„£ëŠ”ë‹¤
	- ë§ˆì§€ë§‰ Layerì—ì„œ ì „ì²´ ì´ë¯¸ì§€ ì •ë³´ë¥¼ ìš”ì•½í•œ 4,096-D Vectorë¥¼ ì¶œë ¥í•œë‹¤
2. ì´ˆê¹ƒê°’ ì…ë ¥
	- ëª¨ë¸ì´ ë¬¸ì¥ì„ ìƒì„±í•´ ë‚´ê¸° ìœ„í•´ ì´ˆê¸° ê°’ì„ ë„£ì–´ì¤€ë‹¤

	$$
	h=tanh(W_{xh}x+W_{hh}h) \\ \downarrow \\ h = tanh(W_{xh}x+W_{hh}h+W_{ih}v)
	$$

	- ì•ì„œ ì‚´í´ë³¸ RNN ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ í–‰ë ¬ì— ì´ë¯¸ì§€ ì •ë³´ì¸ $W_{ih}$ë„ ì¶”ê°€í•´ì•¼ í•œë‹¤
3. Vocabularyì˜ ëª¨ë“  Scoresì— ëŒ€í•œ ë¶„í¬ë¥¼ ê³„ì‚°
	- ì—„ì²­ë‚˜ê²Œ í° ë¶„í¬ì—ì„œ Samplingì„ í•˜ê³  ê·¸ ë‹¨ì–´ë¥¼ ë‹¤ìŒ Stepì˜ ì…ë ¥ìœ¼ë¡œ ë‹¤ì‹œ ë„£ì–´ì¤€ë‹¤
	- Sampling ëœÂ $y_0$ê°€ ë“¤ì–´ê°€ë©´ ë‹¤ì‹œ Vocabularyì— ëŒ€í•œ ë¶„í¬ë¥¼ ì¶”ì •í•˜ê³  ë‹¤ìŒ ë‹¨ì–´ë¥¼ ë§Œë“ ë‹¤
	- ëª¨ë“  ìŠ¤í…ì´ ì¢…ë£Œë˜ë©´ í•œ ë¬¸ì¥ì´ ë§Œë“¤ì–´ì§„ë‹¤


#### Image Captioning with Attention


![22](/assets/img/2023-05-09-CS231n---Lecture-10.md/22.png)

- CNNìœ¼ë¡œ ë²¡í„° í•˜ë‚˜ë¥¼ ë§Œë“œëŠ” ê²ƒì´ ì•„ë‹Œ ê³µê°„ ì •ë³´ë¥¼ ê°€ì§€ê³  ìˆëŠ” Grid Of Vectorë¥¼ ë§Œë“ ë‹¤
- Forward Passì‹œ ë§¤ ìŠ¤í… Vocabulartyì—ì„œ ìƒ˜í”Œë§ í•  ë•Œ, ëª¨ë¸ì´ ì´ë¯¸ì§€ì—ì„œ ë³´ê³  ì‹¶ì€ ìœ„ì¹˜ì— ëŒ€í•œ ë¶„í¬ë¥¼ ë§Œë“¤ì–´ë‚¸ë‹¤
- ì´ë¯¸ì§€ì˜ ê° ìœ„ì¹˜ì— ëŒ€í•œ ë¶„í¬ëŠ” Train Timeì— ëª¨ë¸ì´ ì–´ëŠ ìœ„ì¹˜ë¥¼ ë´ì•¼í•˜ëŠ” ì§€ì— ëŒ€í•œ 'Attention'ì´ë¼ í•  ìˆ˜ ìˆë‹¤

![23](/assets/img/2023-05-09-CS231n---Lecture-10.md/23.png)

- Trainì„ ë§ˆì¹˜ë©´ ëª¨ë¸ì´ Captionì„ ìƒì„±í•˜ê¸° ìœ„í•´ ì´ë¯¸ì§€ì˜ Attentionì„ ì´ë™ì‹œí‚¤ëŠ” ëª¨ìŠµì„ ë³¼ ìˆ˜ ìˆë‹¤
- Captionì„ ë§Œë“¤ì–´ ë‚´ê¸° ìœ„í•´ ì´ë¯¸ì§€ ë‚´ì— ë‹¤ì–‘í•œ ê³³ë“¤ì— Attentionì„ ì£¼ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤


#### RNN with Attention


**Visual Question Answering**


![24](/assets/img/2023-05-09-CS231n---Lecture-10.md/24.png)


Visual Question Answeringì—ì„œëŠ” ì…ë ¥ì´ ì´ë¯¸ì§€ì™€ ì´ë¯¸ì§€ì— ê´€í•œ ì§ˆë¬¸ìœ¼ë¡œ 2ê°œì´ë‹¤


**Many to One**


![25](/assets/img/2023-05-09-CS231n---Lecture-10.md/25.png)

- ëª¨ë¸ì€ ìì—°ì–´ ë¬¸ì¥(ì§ˆë¬¸)ì„ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ RNNì´ ì§ˆë¬¸ì„ Vectorë¡œ ìš”ì•½í•˜ê³ , CNNì´ ì´ë¯¸ì§€ ìš”ì•½ì„ í•œë‹¤
- RNN/CNNì—ì„œ ë‚˜ì˜¨ Vectorë¥¼ ì¡°í•©í•˜ë©´ ì§ˆë¬¸ì— ëŒ€í•œ ë¶„í¬ë¥¼ ì˜ˆì¸¡í•  ìˆ˜ ìˆë‹¤


#### Multilayer RNNs


![26](/assets/img/2023-05-09-CS231n---Lecture-10.md/26.png)

- Multilayer RNNì€ ì²« ë²ˆì§¸ RNNìœ¼ë¡œ Inputì´ ë“¤ì–´ê°€ì„œ ì²« ë²ˆì§¸ Hidden Stateë¥¼ ë§Œë“¤ì–´ ë‚¸ë‹¤. ì´ë ‡ê²Œ ë§Œë“¤ì–´ì§„ Hidden State Sequenceë¥¼ ë‹¤ë¥¸ RNNì˜ Inputìœ¼ë¡œ ë„£ì–´ì¤„ ìˆ˜ ìˆë‹¤
- RNNì—ì„œë„ ëª¨ë¸ì´ ê¹Šì–´ì§ˆìˆ˜ë¡ ë‹¤ì–‘í•œ ë¬¸ì œë“¤ì—ì„œ ì„±ëŠ¥ì´ ë” ì¢‹ì•„ì§€ê³ , ë§ì€ ê²½ìš° 3~4 layer RNNì„ ì‚¬ìš©í•œë‹¤
- ë³´í†µ ì—„ì²­ë‚˜ê²Œ ê¹Šì€ RNN ëª¨ë¸ì„ ì‚¬ìš©í•˜ì§€ëŠ” ì•Šê³ , ì¼ë°˜ì ìœ¼ë¡œ 2~4 layer RNNì„ ì‚¬ìš©í•œë‹¤


#### Vanilla RNN Gradient Flow


![27](/assets/img/2023-05-09-CS231n---Lecture-10.md/27.png)

- RNNì€ Backward Passì‹œ $h_t$ì— ëŒ€í•œ Lossì˜ ë¯¸ë¶„ê°’ì„ ì–»ê³ , Lossì— ëŒ€í•œ $h_{t-1}$ì˜ ë¯¸ë¶„ê°’ì„ ê³„ì‚°í•˜ê²Œ ëœë‹¤.
- Gradientê°€ $tanh$ `Gate`ë¥¼ íƒ€ê³  í˜ëŸ¬ê°€ê³ , `Mat Mul Gate`ë¥¼ í†µê³¼í•˜ê³  Back Propagationì€ ê²°êµ­ ì´ Transpose(ê°€ì¤‘ì¹˜ í–‰ë ¬)ì„ ê³±í•˜ê²Œ ëœë‹¤
- ì´ëŠ” ë§¤ë²ˆ Vanilla RNN Cellsë¥¼ í•˜ë‚˜ í†µê³¼í•  ë•Œë§ˆë‹¤ ê°€ì¤‘ì¹˜ í–‰ë ¬ì˜ ì¼ë¶€ë¥¼ ê³±í•˜ê²Œ ëœë‹¤ëŠ” ì˜ë¯¸ì´ë‹¤
- RNNì´ ì—¬ëŸ¬ Sequenceì˜ Cellì„ ìŒ“ì•„ ì˜¬ë¦¬ëŠ” ì‚¬ì‹¤ì„ ê³ ë ¤í•˜ë©´ Gradientê°€ RNN ëª¨ë¸ì˜ Layers Sequenceë¥¼ í†µí•´ ì–´ë–¤ ë°©ì‹ìœ¼ë¡œ ì „ë‹¬ë˜ëŠ”ì§€ ìƒê°í•´ ë³¼ ìˆ˜ ìˆë‹¤

![28](/assets/img/2023-05-09-CS231n---Lecture-10.md/28.png)

- ë§Œì•½ ìš°ë¦¬ê°€ $h_0$ì— ëŒ€í•œ Gradientë¥¼ êµ¬í•˜ê³ ì í•œë‹¤ë©´ ê²°êµ­ ëª¨ë“  RNN Cellsë¥¼ ê±°ì³ì•¼ í•˜ëŠ”ë°, Cellì´ í•˜ë‚˜ë¥¼ í†µê³¼í•  ë•Œë§ˆë‹¤ ê° Cellì˜ í–‰ë ¬ W transpose factorsê°€ ê´€ì—¬í•˜ê³ , $h_0$ì˜ Gradientë¥¼ ê³„ì‚°í•˜ëŠ” ì‹ì„ ì¨ë³´ë©´ ì•„ì£¼ ë§ì€ ê°€ì¤‘ì¹˜ í–‰ë ¬ì´ ê°œì…í•˜ê²Œ ë˜ë©° ì•ˆ ì¢‹ë‹¤
- ë§Œì•½ ê³±í•´ì§€ëŠ” ê°’ì´ 1ë³´ë‹¤ í° ê²½ìš°ë¼ë©´ ì ì  ê°’ì´ ì»¤ì§ˆ ê²ƒì´ê³ , 1ë³´ë‹¤ ì‘ì€ ê²½ìš°ë¼ë©´ ì ì  ì‘ì•„ì ¸ì„œ 0ì´ ëœë‹¤
	- `í–‰ë ¬ì˜ íŠ¹ì´ê°’ì´ 1ë³´ë‹¤ í¬ë‹¤ë©´` $h_0$ì˜ GradientëŠ” ì•„ì£¼ ì»¤ì§€ê²Œ ë˜ê³ , ì´ë¥¼ Exploding Gradientsë¼ê³  í•œë‹¤. Back Propagationì‹œ ë ˆì´ì–´ê°€ ê¹Šì–´ì§ˆ ìˆ˜ë¡ Gradientê°€ ê¸°í•˜ê¸‰ìˆ˜ì ìœ¼ë¡œ ì¦ê°€í•˜ëŠ” í˜„ìƒì´ë‹¤
	- `í–‰ë ¬ì˜ íŠ¹ì´ê°’ì´ 1ë³´ë‹¤ ì‘ë‹¤ë©´` $h_0$ì˜ Gradeintê°€ ì•„ì£¼ ì‘ì•„ì§€ê²Œ ë˜ê³ , ì´ë¥¼ Vanishing Gradientsë¼ê³  í•œë‹¤

	**â†’ ë§Œì•½ ë‘ ìƒí™©ì´ ì¼ì–´ë‚˜ì§€ ì•Šìœ¼ë ¤ë©´ ê³±í•´ì§€ëŠ” í–‰ë ¬ì˜ íŠ¹ì´ê°’ì´** **`1`****ì´ì—¬ì•¼ í•œë‹¤**


**Gradient Clipping**



{% raw %}
```python
grad_norm = np.sum(grad * grad)
if grad_norm > threshold:
	grad *= (threshold / grad_norm)
```
{% endraw %}


- Gradient Clippingì€ Gradientë¥¼ ê³„ì‚°í•˜ê³  L2-Normì´ ì„ê³„ê°’ë³´ë‹¤ í° ê²½ìš° ê·¸ë ˆë””ì–¸íŠ¸ê°€ ìµœëŒ€ ì„ê³„ê°’ì„ ë„˜ì§€ ëª»í•˜ë„ë¡ ì¡°ì •í•œë‹¤
- ë°˜ëŒ€ë¡œ Vanishing Gradientsë¥¼ ë‹¤ë£¨ë ¤ë©´ ë” ë³µì¡í•œ RNN ì•„ì¼€í…ì²˜ê°€ í•„ìš”í•˜ê³  ì´ëŠ” LSTMì— ê´€í•œ ê²ƒì´ë‹¤


### Long Short Term Memory (LSTM)


![29](/assets/img/2023-05-09-CS231n---Lecture-10.md/29.png)

- LSTMì€ Exploding Gradients, Vanishing Gradients ë¬¸ì œë¥¼ ì™„í™”ì‹œí‚¤ê¸° ìœ„í•´ ë””ìì¸ëœ ëª¨ë¸ì´ë‹¤
- LSTMì—ëŠ” í•œ Cell ë‹¹ Hidden Stateì¸ $h_t$ì™€ Cell Stateì¸ $c_t$ê°€ ìˆë‹¤
- Cell StateëŠ” LSTM ë‚´ë¶€ì—ë§Œ ì¡´ì¬í•˜ë©° ë°–ì— ë…¸ì¶œë˜ì§€ ì•ŠëŠ” ë³€ìˆ˜ì´ë‹¤. LSTMë„ $h_{t-1}, x_t$ 2ê°œë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ê³  4ê°œì˜ Gatesë¥¼ ê³„ì‚°í•œë‹¤
- ì´ Gatesë¥¼ $c_t$ë¥¼ ì—…ë°ì´íŠ¸í•˜ëŠ”ë° ì´ìš©í•˜ê³  $c_t$ë¡œ ë‹¤ìŒ ìŠ¤í…ì˜ Hidden Stateë¥¼ ì—…ë°ì´íŠ¸í•œë‹¤

![30](/assets/img/2023-05-09-CS231n---Lecture-10.md/30.png)

1. $g$ **- Gate Gate** : Cellì„ ì–¼ë§ˆë‚˜ í¬í•¨ì‹œí‚¬ ê²ƒì¸ê°€?
	- Input Cellì„ ì–¼ë§ˆë‚˜ í¬í•¨ì‹œí‚¬ì§€ ê²°ì •í•˜ëŠ” ê°€ì¤‘ì¹˜ì´ë‹¤
	- `i, f, o Gate`ëŠ” sigmoidë¥¼ ì‚¬ìš©í•´ì„œ ê°’ì´ [0, 1] ì´ë‹¤. í•˜ì§€ë§Œ `g Gate`ëŠ” tanhë¥¼ ì‚¬ìš©í•´ì„œ ê°’ì´ [-1, 1] ì´ë‹¤
2. $f$ **- Forget Gate** : ì´ì „ Cellì— ëŒ€í•œ ì •ë³´ë¥¼ ì–¼ë§ˆë‚˜ ì§€ìš¸ ê²ƒì¸ê°€?
	- Previous Cell State$(c_{t-1})$ì€ Forget Gateì™€ Element-Wise Multiplicationí•˜ê³  ê²°ê³¼ ë²¡í„°$(c_{t-1})$ëŠ” 0 ë˜ëŠ” 1 ì¼ ê²ƒì´ë‹¤
	- ë”°ë¼ì„œ Forget Gate=0ì¸ ElementëŠ” Previous Cell Stateë¥¼ ìŠê³ , Forget Gate=1ì¸ ElementëŠ” Previous Cell Stateë¥¼ ê¸°ì–µí•œë‹¤
3. $i$ **- Input Gate** : Cellì— ëŒ€í•œ ì…ë ¥ ($x_t$ì— ëŒ€í•œ ê°€ì¤‘ì¹˜)
	- ê° Elementì— ëŒ€í•´ ì´ Cell Stateë¥¼ ì‚¬ìš©í•˜ê³  ì‹¶ìœ¼ë©´ 1ì´ ë˜ê³ , ì“°ê³  ì‹¶ì§€ ì•Šìœ¼ë©´ 0ì´ ëœë‹¤
4. $o$ **- Output Gate** : $c_t$ë¥¼ ì–¼ë§ˆë‚˜ ë°–ìœ¼ë¡œ ë“œëŸ¬ë‚¼ ê²ƒì¸ê°€?
	- Hidden Gateë¥¼ ê³„ì‚°í•  ë•Œ Cell State$(c_t)$ë¥¼ ì–¼ë§ˆë‚˜ ë…¸ì¶œì‹œí‚¬ì§€ë¥¼ ê²°ì •í•œë‹¤

![31](/assets/img/2023-05-09-CS231n---Lecture-10.md/31.png)

- ëª¨ë¸ì˜ Inputìœ¼ë¡œ $c_{t-1}, h_{t-1}, x_t$ê°€ ë“¤ì–´ì˜¨ë‹¤
- $h_{t-1}$ì™€ $x_t$ë¥¼ ìŒ“ê³  ì—¬ê¸°ì— ê°€ì¤‘ì¹˜ í–‰ë ¬ 4ê°œë¥¼ ê³±í•´ Gateë¥¼ ë§Œë“ ë‹¤
- $f$ëŠ” $c_{t-1}$ì™€ Element Wise Multiplicationí•˜ê³  $i, g$ì˜ ê³±ì„ ë”í•´ Cell Stateë¥¼ ë§Œë“ ë‹¤. $c_t$ëŠ” $tanh$ë¥¼ ê±°ì¹œ í›„ $o$ì™€ ê³±í•´ì ¸ì„œ ë‹¤ìŒ Hidden stateì¸ $ h_t$ë¥¼ ë§Œë“ ë‹¤


#### LSTM Gradient Flow

- GradientëŠ” Upstream Gradientì™€ Forget Gateì˜ Element Wise Multiplicationì´ë‹¤.

	â†’ $f$ì™€ ê³±í•´ì§€ëŠ” ì—°ì‚°ì´ Matrix Multiplicationì´ ì•„ë‹Œ Element-Wise Multiplicationì´ë¼ì„œ ì¢‹ë‹¤


	â†’ Element-Wise Multiplicationì„ í†µí•´ ë§¤ Step $f$ê°’ì´ ë³€í•´ì„œ Exploding / Vanishing Gradient ë¬¸ì œë¥¼ ì‰½ê²Œ í•´ê²°í•œë‹¤


	â†’ ìµœì¢… Hidden Stateì¸ $h_t$ë¥¼ ê°€ì¥ ì²« Cell State$(c_0)$ê¹Œì§€ Back Propagationí•˜ë©´, RNNì²˜ëŸ¼ ë§¤ Stepë§ˆë‹¤ $tanh$ë¥¼ ê±°ì¹  í•„ìš”ì—†ì´ ë‹¨ í•œ ë²ˆë§Œ $tanh$ë¥¼ ê±°ì¹˜ë©´ ëœë‹¤


![32](/assets/img/2023-05-09-CS231n---Lecture-10.md/32.png)

- Gradientê°€ ëª¨ë¸ì˜ ì¢…ë‹¨ì´ Lossì—ì„œ ê°€ì¥ ì²˜ìŒ Cell State$(c_0)$ê¹Œì§€ í˜ëŸ¬ê°€ë©° ë°©í•´ë¥¼ ëœ ë°›ëŠ”ë‹¤
