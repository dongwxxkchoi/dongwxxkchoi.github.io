---
layout: single
date: 2023-04-12
title: "CS231n - Lecture 6"
use_math: true
author_profile: false
tags: [강의/책 정리, ]
categories: [AI, ]
---


## **Activation Functions**


![0](/assets/img/2023-04-12-CS231n---Lecture-6.md/0.png)



#### Sigmoid


![1](/assets/img/2023-04-12-CS231n---Lecture-6.md/1.png)

1. Saturated neurons ‘kill’ the gradients

	sigmoid의 양 극단의 기울기는 거의 0에 가까움 이 경우에 backpropagation의 과정 중에 gradient를 kill, 즉 거리가 먼 뉴런까지 학습되지 않을 가능성이 높다. 


	만약, sigmoid 뉴런의 가중치가 너무 큰 상황 → saturated, 학습 저하 가능성 있음

2. Sigmoid outputs are not zero-centered

	gradient가 항상 positive하거나 negative함. 이렇게 되면, zig-zag 형태의 gradient updates가 일어남. 하지만 결과에 큰 영향은 X

3. exp() is a bit compute expensive

	말 그대로 큰 computing이 필요함



#### tanh


![2](/assets/img/2023-04-12-CS231n---Lecture-6.md/2.png)

- real value를 [-1, 1]로 축소하기 때문에, zero centered되어 zia-zag 현상 해결
- saturation 현상 여전
- sigmoid보단 조금 나은 정도

$$
tanh(x) = 2\sigma(2x) - 1
$$



#### ReLU (Rectified Linear Unit)


![3](/assets/img/2023-04-12-CS231n---Lecture-6.md/3.png)


$$
f(x) = max(0, x)
$$


n other words, the activation is simply thresholded at zero (see image above on the left). There are several pros and cons to using the ReLUs:

- sgd 등 시에, 빠르게 수렴함
- 계산이 간편함 (exponential 등 x)
- 일부 ReLU unit이 죽어버리는 Dead ReLU 현상이 일어날 수 있음
	- active ReLU는 ReLU함수가 반응하는 구간
	- dead ReLU는 ReLU함수가 active하지 않고 죽어버리는 구간 (입력값 음수)

	DADA CLOUD에서 ReLU함수를 적용할 시 절반만 activate 된다는 것을 알 수 있습니다


![4](/assets/img/2023-04-12-CS231n---Lecture-6.md/4.png)


그림에서와 같이 **가중치 평면이 DATA CLOUD에서 멀리 떨어져 있는 경우**(빨간선), 어떤 입력 데이터에서도 activate 되는 경우가 존재하지 않을 것입니다. 따라서 activation이 일어나지 않기 때문에 backprop(오차 역전파)도 일어나지 않을 것입니다.


다음으로 '더 흔한 경우'에 대해 살펴보겠습니다.(강의자 Serena Yeung의 말에 의하면 두 번째 경우가 더 흔한 경우라고 합니다.) 바로 **learning rate가 높은 경우**입니다.


처음에 적절한 ReLU로 학습을 할 수 있어도 learning rate가 높아 weight update가 지나치게 커지면 가중치가 날뛰게 되어 ReLU가 데이터의 manifold를 벗어나게 된다고 합니다. 그래서 처음에 학습이 잘 되다가도 갑자기 죽어버리게 되는 경우가 생긴다고 합니다.



#### **Leaky ReLU**


![5](/assets/img/2023-04-12-CS231n---Lecture-6.md/5.png)

- 음수에도 0.01의 gradient를 부여해, dying ReLU현상을 해결

	![6](/assets/img/2023-04-12-CS231n---Lecture-6.md/6.png)


	![7](/assets/img/2023-04-12-CS231n---Lecture-6.md/7.png)


	→ 이처럼, $\alpha$값을 parametric해서 사용하기도 함

- 하지만, 일관성이 x


#### ELU


![8](/assets/img/2023-04-12-CS231n---Lecture-6.md/8.png)

- 장점이 많지만, 연산량이 크다는 것이 함정


#### **Maxout**


![9](/assets/img/2023-04-12-CS231n---Lecture-6.md/9.png)

- dot product → non-linearity
- ReLU와 Leaky ReLU 역시 이 형태의 special case of form
- parameter 수가 많음

This concludes our discussion of the most common types of neurons and their activation functions. As a last comment, it is very rare to mix and match different types of neurons in the same network, even though there is no fundamental problem with doing so.



#### **TLDR**


![10](/assets/img/2023-04-12-CS231n---Lecture-6.md/10.png)



## Data Preprocessing


![11](/assets/img/2023-04-12-CS231n---Lecture-6.md/11.png)

- zero-centered 이유?

	→ 위에서 언급했듯이, 모든 input이 positive라면(ex. sigmoid의 결과), zig-zag 현상


	→ pre-processing 과정에서 zero-centered 필요

- normalized data

	→ data를 normalizing해서, data의 dimension이 대략적으로 같은 scale에 포함되도록 함

	- standardscaler → mean:0, std:1로 normalization
	- minmaxscaler → min, max 사이로 normalization (대체로 -1, 1)

![12](/assets/img/2023-04-12-CS231n---Lecture-6.md/12.png)


PCA → 차원 축소 **원 데이터의 분포를 최대한 보존하면서 고차원 공간의 데이터들을 저차원 공간으로 변환**


Whitening → input의 feature들을 uncorrelated하게 해주고, 각각의 variance를 1로 만들어주는 작업


(계산량이 너무 많고, 일부 parameter들의 영향이 무시되는 현상이 있어 많이 쓰진 않는 듯)


![13](/assets/img/2023-04-12-CS231n---Lecture-6.md/13.png)


**일반적으로 이미지의 경우에는 픽셀들을 너무 많이 정규화할 필요가 없다**. 왜냐하면 이미지는 각 위치에서 이미 **비교적 비슷한 규모와 분포(0~256)**를 가지고 있기 때문이다. 


따라서 zero-centering 정도만 해주면 충분하다. CNN에서 원본 이미지 자체의 spatial 정보를 이용해서 이미지의 spatial structure을 얻도록 함



## Weight Initialization



#### all zero


![14](/assets/img/2023-04-12-CS231n---Lecture-6.md/14.png)


가충치가 0이기 때문에 입력이 주어지면 모든 뉴런은 기본적으로 동일한 작업을 하게 된다. 즉, 같은 값을 출력할 것이기 때문에 동일한 gradient를 갖게 되고, 동일하게 학습된다. 복잡한 뉴런층이 각기 다른 것을 학습해서 non-linearity를 학습할 수 있도록 해야 하는데, 같은 것을 학습하면 불가능



#### small random numbers


![15](/assets/img/2023-04-12-CS231n---Lecture-6.md/15.png)


모든 가중치를 아주 작은 number로 초기화


학습이 안 되는 것은 아니지만, 애초에 작은 수로 시작해서 가중치들이 업데이트 되기 때문에, 간단한 네트워크에서는 괜찮겠지만 뉴런들이 복잡해질수록 모든 값이 0에 가까워지는 문제(gradient vanishing)가 발생한다. 계속해서 작은 수들이 곱해진다면, 뒤로 갈수록 기울기가 거의 소실되기 때문


입력값이 매우 작다. weight에 대한 gradient는 upstream gradient * local gradient이다. 이 dot product는 W x X를 수행하는데, 기본적으로 X가 된다. 바로 입력값이다. 따라서 X가 작기 때문에 가충치는 매우 작은 기울기를 얻게 되고, 업데이트 되지 않는다.


![16](/assets/img/2023-04-12-CS231n---Lecture-6.md/16.png)



#### Xavier


standard gaussian으로 뽑은 값을 입력 수로 스케일링, 입/출력의 분산을 맞춰준다는 것


![17](/assets/img/2023-04-12-CS231n---Lecture-6.md/17.png)


_Xavier Initialization_을 사용하게 된다면, 뉴런의 개수가 많을수록 초깃값으로 설정하는 weight이 더 좁게 퍼짐을 알 수 있습니다. (분산이 작아짐)


![18](/assets/img/2023-04-12-CS231n---Lecture-6.md/18.png)


_Xavier Initialization_을 사용하면 위의 그래프처럼 됩니다.


이 결과는 layer가 더 깊어질수록 앞에 본 방식보다 더 넓게 분포됨을 알 수 있습니다.


데이터가 더 적당히 넓게 펴지므로 sigmoid 함수를 사용할 때도 표현을 제한 받지 않고 학습이 가능합니다.


또한, neuron의 개수에 따라 weight이 초기화되기 때문에 고정된 표준편차를 사용할 때보다 더 robust한 성질을 가집니다.


_Xavier Initialization_는 tanh 또는 sigmoid로 활성화되는 경우라 가정(선형성) 하고 초기화된 값


_tanh_ 또는 _sigmoid_와 잘 맞는 이유도 두 함수는 좌우 대칭이라 중앙 부분이 선형인 함수로 볼 수 있기 때문인데요.


이처럼 선형적인 case에는 적절하다. 이전 노드의 개수가 많아지면 weight 값이 작아지고, 반대면 커진다. 입력의 수에 따라 가중치의 크기가 달라지기 때문에 saturation이 발생하지 않고 고르게 분포하게 된다. 그러나 ReLU와 같은 비선형 case에는 적절하지 않은데, ReLU의 특성을 생각해보면 음수인 경우 모두 가중치가 0이었기 때문에 기울기도 반토막


![19](/assets/img/2023-04-12-CS231n---Lecture-6.md/19.png)


![20](/assets/img/2023-04-12-CS231n---Lecture-6.md/20.png)


![21](/assets/img/2023-04-12-CS231n---Lecture-6.md/21.png)



#### He


따라서 비선형 case에 대해서는 he 초기값을 사용한다.


![22](/assets/img/2023-04-12-CS231n---Lecture-6.md/22.png)


_ReLU_를 위해 만들어진 초기화 방법입니다. _He Initialization_는 **앞 layer의 neuron이 n개**일 때, **표준편차**가 **1/(sqrt(**_**n/**_**2))인 정규분포**를 사용합니다. 


ReLU는 음의 영역이 0이라서 활성화되는 영역을 더 넓게 분포시키기 위해 2배의 계수가 필요하다고 해석. 


_He Initialization_은 **모든 layer에서 균일하게 분포되어 있음**을 알 수 있습니다. 그래서 backpropagation 때도 적절한 값이 나올 것이라 기대할 수 있습니다.


---



## Batch Normalization


이제 batch normalization으로 넘어갑니다.


우리가 원하는 가우시안의 범위 내에서 activation값들을 뽑고 싶은 idea에서 기인한 방식입니다.


현재 batch에서의 평균과 분산을 이용해서 훈련 처음부터 batch normalization을 취해주어 모든 층에서 정규 분포를 따르게끔 하는 것이죠.


각 뉴런에서 평균과 분산을 구하는 function을 이용해서 batch마다 normalization을 해주는 것인데, 이는 미분가능한 함수라서 back prop에도 문제가 없습니다.


![23](/assets/img/2023-04-12-CS231n---Lecture-6.md/23.png)


BN Layer는 일반적으로 FC 혹은 Conv layer 뒤와, non-linear func 전에 삽입된다. Mini Batch 단위로 정규화를 수행한다. 초기값을 설정하는데 많은 공을 들이지 않고도 활성화값 분포를 적당히 퍼트릴 수 있도록 강제할 수 있다는데 이점이 있다. 모델이 학습됨에 따라 감마와 베타도 학습될 수 있다. dropout의 필요성을 약간 감소시켜준다.


![24](/assets/img/2023-04-12-CS231n---Lecture-6.md/24.png)


![25](/assets/img/2023-04-12-CS231n---Lecture-6.md/25.png)


![26](/assets/img/2023-04-12-CS231n---Lecture-6.md/26.png)


![27](/assets/img/2023-04-12-CS231n---Lecture-6.md/27.png)



## Babysitting the Learning Process



## Hyperparameter Optimization


grid search


그렇다면 이러한 learning rate와 같은 hyperparmeter 중 어떻게 가장 좋은 값을 선택하는가? hyperparameter를 정하는 대표적인 방법 중 하나는, 예전에 다뤘던 교차 검증(cross-validation)이 있다. 교차 검증은 다시 말해서 훈련 세트에 대한 훈련이다.


![28](/assets/img/2023-04-12-CS231n---Lecture-6.md/28.png)


![29](/assets/img/2023-04-12-CS231n---Lecture-6.md/29.png)

