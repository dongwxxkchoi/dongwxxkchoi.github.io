---
layout: single
date: 2023-06-09
title: "CS224n - Lecture 1 (Intro & Word Vectors)"
use_math: true
author_profile: false
tags: [ê°•ì˜/ì±… ì •ë¦¬, ]
categories: [AI, ]
---

> â˜‘ï¸ **Lecture 1 : Introduction and Word Vectors**  
> 1. Human language and word meaning  
> 2. Word2vec introduction  
> 3. Word2vec objective function gradients  
> 4. Optimization basics  
> 5. Looking at word vectors



### ë§ì˜ ì˜ë¯¸ë¥¼ ì–´ë–»ê²Œ í‘œí˜„í• ê¹Œ?


**ì˜ë¯¸**ë€ ë¬´ì—‡ì¼ê¹Œ?

- the idea that is represented by a word, phrase, etc.

	word, phraseì— ì˜í•´ í‘œí˜„ë˜ëŠ” ìƒê°

- the idea that a person wants to express by using words, signs, etc.

	words, signsì„ ì‚¬ìš©í•´ í‘œí˜„í•˜ê³ ì í•˜ëŠ” ì‚¬ëŒì˜ ìƒê°

- the idea that is expressed in a work of writing, art, etc.

	writing, art ì‘í’ˆ ë“±ì— ì˜í•´ í‘œí˜„ë˜ëŠ” ìƒê°


> ğŸ’¡ **signifier** (ê¸°í‘œ; symbol) â†”Â **signified** (ê¸°ì˜; idea or thing)  
> **â‡’ denotational semantics (í‘œì‹œì  ì˜ë¯¸ë¡ )**


**ê°€ì¥ ê°„ë‹¨í•œ NLP solution?**


â†’ ê°œë³„ì ì¸ wordì— ëŒ€í•´ dictionary, thesaurus ë“±ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ë‹¤.


(synonym(ë™ì˜ì–´ ê´€ê³„) sets, hypernyms(ìƒí•˜ìœ„ ê´€ê³„)ë¥¼ í¬í•¨)


â†’ ë‹¤ì‹œ ë§í•´ â€œISAâ€ relationshipì´ë¼ê³  í•  ìˆ˜ ìˆë‹¤.


![0](/assets/img/2023-06-09-CS224n---Lecture-1-(Intro-&-Word-Vectors).md/0.png)



#### ê°„ë‹¨í•œ ë§Œí¼ ë‹¨ì ë„ ì¡´ì¬

- **nuance**ë¥¼ ì‚´ë¦¬ì§€ ëª»í•©ë‹ˆë‹¤.

	â†’ **contextì˜ ê³ ë ¤**ê°€ í•„ìš”í•¨

- wordì˜ ìƒˆ ì˜ë¯¸ë¥¼ ë†“ì¹¨

	â†’ ex. wicked ë“±ë“±


	â†’ í•­ìƒ, up-to-dateí•˜ê¸° í˜ë“¦


ì´ì²˜ëŸ¼, ì „í†µì ì¸ NLPëŠ” ë‹¨ì–´ë¥¼ ê·¸ì € discrete symbolë¡œ ì—¬ê²¨ ì™”ìŠµë‹ˆë‹¤


â†’ one-hot vectorë¡œ ë³€í™˜í•´ matrixì—ì„œ í•œ ê°’ë§Œ 1ì´ê³  ë‚˜ë¨¸ì§€ê°€ 0ì¸ ë°ì´í„°ë¡œ ì·¨ê¸‰


â†’ one-hot vectorë¡œ ë³€í™˜ëœ **ë‘ vectorëŠ” ìœ ì‚¬í•œ ì˜ë¯¸**ë¥¼ ê°–ë‚˜, **orthogonal**í•˜ê¸° ë•Œë¬¸ì—, **similarityê°€ ì—†ìŒ**


![1](/assets/img/2023-06-09-CS224n---Lecture-1-(Intro-&-Word-Vectors).md/1.png)


![2](/assets/img/2023-06-09-CS224n---Lecture-1-(Intro-&-Word-Vectors).md/2.png)



#### ë¬¸ë§¥ì„ í†µí•´ words ë‚˜íƒ€ë‚´ê¸°


> ğŸ’¡ **Distributional semantics (ë¶„í¬ ì˜ë¯¸ë¡ )**  
> â†’ wordì˜ meaningì€ ê·¼ì²˜ì— ìì£¼ ë‚˜íƒ€ë‚˜ëŠ” wordsë“¤ì— ì˜í•´ ì£¼ì–´ì§


ì´ë¥¼ í†µí•´, ì—¬ëŸ¬ê°€ì§€ì˜ context ì†ì—ì„œì˜ wë¥¼ ì‚¬ìš©í•´, wë¥¼ ë‚˜íƒ€ë‚´ëŠ” ê²ƒì´ë‹¤.


![3](/assets/img/2023-06-09-CS224n---Lecture-1-(Intro-&-Word-Vectors).md/3.png)


â†’ ì´ì²˜ëŸ¼ **ê° wordì— ëŒ€í•œ vector**ë¥¼ **ìš°ë¦¬ê°€ ì„¤ì •**í•´, **contextì™€ ì—°ê´€**ë˜ê²Œ í•¨


â†’ ì´ word vectorsë¥¼ **word embeddings** ë˜ëŠ” **word representations**ë¼ê³  í•œë‹¤.


â†’ **ì‹¤ì œ**ë¡œëŠ” í›¨ì”¬ ë” **ê³ ì°¨ì›**ì¼ ê²ƒ


ëŒ€í‘œì  ì˜ˆì‹œ **Word2vec**


---



## Word2vec


2013ë…„ ë°œí‘œëœ word vector í•™ìŠµ **framework**


(neural network ì´ìš©)


![4](/assets/img/2023-06-09-CS224n---Lecture-1-(Intro-&-Word-Vectors).md/4.png)


* corpus: ë§ë­‰ì¹˜


* textì˜ të¼ëŠ” ìœ„ì¹˜, c: centor word, o: context words


â†’ cì™€ oì˜ word vectorë“¤ì˜ similarityë¥¼ ì‚¬ìš©í•´, ì£¼ì–´ì§„ c(centor)ê°€ ì£¼ì–´ì¡Œì„ ë•Œì˜ o(í•´ë‹¹ context)ë¥¼ ì˜ˆì¸¡/ê³„ì‚° í•˜ëŠ” ê²ƒ


â‡’  $P(w_{t+j}|w_t)$ë¥¼ êµ¬í•˜ëŠ” ê²ƒ (tê°€ ì£¼ì–´ì¡Œì„ ë•Œ(të¥¼ ì¤‘ì‹¬ìœ¼ë¡œ) ì•ë’¤ì˜ wì˜ í™•ë¥ ì„ ì˜ˆì¸¡)


![5](/assets/img/2023-06-09-CS224n---Lecture-1-(Intro-&-Word-Vectors).md/5.png)


**(skip-gram ë°©ì‹)**


![6](/assets/img/2023-06-09-CS224n---Lecture-1-(Intro-&-Word-Vectors).md/6.png)

- ê° ìœ„ì¹˜ **t = 1, â€¦, T  /**  ì¦‰, ì´ ë‹¨ì–´ì˜ ìˆ˜ê°€ T
- context words **o**ë¥¼ ì˜ˆì¸¡
- window of fixed size **m** (window ì•ˆ 2m+1ì˜ words)
- given center word $w_j$
- data likelihood: $L(\theta) = \Pi^T_{t=1}\Pi_{-m\leq j \leq m, j\ne0}P(w_{t+j}|w_t;\theta)$
- objective function(cost, loss): $J(\theta) = -\frac{1}{T}logL(\theta)$

	**Minimize** $J(\theta)$ **â†”Â Maximize** $L(\theta)$


â‡’ ì¦‰, centor word ê¸°ì¤€ 2mê°œì˜ í™•ë¥ ì´ ë†’ì•„ì§€ê²Œ


ê·¸ë ‡ë‹¤ë©´ ì–´ë–»ê²Œ í•˜ë©´, $P(w_{t+j}|w_t;\theta)$ë¥¼ ê³„ì‚°í•  ìˆ˜ ìˆì„ê¹Œ?


â‡’ word **w**ë¡œ **ë‘ ë²¡í„°ê°’ì„ ì´ìš©**

- $v_w$ : wê°€ centor word(c)ì¼ ë•Œ
- $u_w$ : wê°€ context word(o)ì¼ ë•Œ

ì´ë ‡ê²Œ ë‘ê°œë¡œ ë‚˜ëˆ  **center wordê°€ ì£¼ì–´ì¡Œì„ ë•Œì˜ ê·¸ context wordë¥¼ ì˜ˆì¸¡**í•  ìˆ˜ ìˆë‹¤. (outputs)


$$
P(o|c) = \frac{exp(u^T_ov_c)}{âˆ‘_{wâˆˆV}exp(u^T_wv_c)}
$$


centorì¼ ë•Œ output(context)ì˜ í™•ë¥ 


í•˜ë‚˜ì”© ì‚´í´ë³´ì

- $exp(u^T_ov_c)$ì—ì„œ $u^T_ov_c$

	dot productë¥¼ í†µí•´ oì™€ cì˜ ìœ ì‚¬ë„ë¥¼ ë¹„êµí•¨ ( $âˆ‘^n_{i=1}u_iv_i$ )


	ë§Œì•½ ì´ **dot productì˜ ê²°ê³¼ê°’ì´ í¬ë‹¤ë©´ ë” ìœ ì‚¬ë„ê°€ ë†’ì€ ê²ƒ**ì´ë‹¤.


	(ë²¡í„° ê³µê°„ì—ì„œ ê°™ì€ ë°©í–¥ì— ìˆëŠ” ë²¡í„°ì¼ìˆ˜ë¡ ë‚´ì ì˜ ê°’ì´ ì»¤ì§€ê¸° ë•Œë¬¸)

- $âˆ‘_{wâˆˆV}exp(u^T_wv_c)$

	entire vocabularyì— ëŒ€í•´ normalizeë¥¼ ìˆ˜í–‰í•´ probability distributionì„ ì œê³µ


ì‚¬ì‹¤ ì´ ìˆ˜ì‹ì˜ ê¼´ì€ softmax í•¨ìˆ˜ë¥¼ ì ìš©í•œ í˜•íƒœì´ë‹¤.


softmaxì˜ íŠ¹ì§•ì´ë¼ í•˜ë©´, classificationì—ì„œ ì£¼ë¡œ ì‚¬ìš©í•˜ê³ ,


ì—¬ëŸ¬ê°€ì§€ ê°€ëŠ¥í•œ ê²½ìš°ì— ëŒ€í•œ í™•ë¥ ì„ ë‚˜íƒ€ë‚¸ë‹¤


ê° í™•ë¥ ì˜ ì´ í•©ì€ 1ì´ ëœë‹¤


ê·¸ë ‡ê¸° ë•Œë¬¸ì— ë‹¤ì‹œ ì ëŠ”ë‹¤ë©´,


$$
softmax(x_i) = \frac{exp(x_i)}{âˆ‘^n_{j=1}exp(x_j)} =p_i
$$


ì´ë ‡ê²Œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤.


ì¦‰, arbitrary values $x_i$ë¥¼ probability distribution $p_i$ë¡œ ë§¤í•‘

- soft â†’ ë” **ì‘ì€** $x_i$**ì—ë„ ê°’ì„ í• ë‹¹**í•˜ê¸° ë•Œë¬¸
- max â†’ **ê°€ì¥ ë¹„ìŠ·í•œ** $x_i$**ì˜ í™•ë¥ ì„ ì¦í­**ì‹œí‚¤ê¸° ë•Œë¬¸

ì´ ì‹ì— logë¥¼ ì”Œìš°ëŠ”ë°, ì´ëŠ” ê³„ì‚°ì„ í¸ë¦¬í•˜ê²Œ í•˜ê¸° ìœ„í•¨ì„


(Logì™€ expëŠ” ìƒì‡„ë˜ê¸° ë•Œë¬¸)


ì´ ëª¨ë¸ì—ì„  gradientë¥¼ ì¤„ì´ëŠ” ë°©ì‹ìœ¼ë¡œ í•™ìŠµí•˜ê¸° ë•Œë¬¸ì—,


ë¯¸ë¶„ì„ í†µí•´ ë¯¸ë¶„ê³„ìˆ˜ë¥¼ êµ¬í•¨


â†’ ë¯¸ë¶„ì‹ì€ ì•„ë˜


word2vec ìƒì„¸ ì„¤ëª…:


[https://ratsgo.github.io/from frequency to semantics/2017/03/30/word2vec/](https://ratsgo.github.io/from%20frequency%20to%20semantics/2017/03/30/word2vec/)


ì›Œë“œ ì„ë² ë”© ê³¼ì • ê·¸ë˜í”½:


[bookmark](https://ronxin.github.io/wevi/)


![7](/assets/img/2023-06-09-CS224n---Lecture-1-(Intro-&-Word-Vectors).md/7.png)



#### ì™œ loss functionì„ ( predicted value - observation value )ë¼ê³  í‘œí˜„í• ê¹Œ?


word2vecì˜ skip-gram ëª¨ë¸ì˜ í•™ìŠµì€ context window ë‹¨ìœ„ë¡œ ì´ë¤„ì§


â†’ context windowì˜ centor wordë¥¼ inputìœ¼ë¡œ ë°›ì•˜ì„ ë•Œ, ëª¨ë“  vocabë“¤ì— ëŒ€í•œ ë°œìƒ í™•ë¥ ì„ output


(softmax)


â†’ ê·¸ í™•ë¥ ë“¤ì„ $L(\theta) = \Pi^T_{t=1}\Pi_{-m\leq j \leq m, j\ne0}P(w_{t+j}|w_t;\theta)$


$$
\mathcal{L} = -\log \sigma(\mathbf{v}{w_o}^T \mathbf{v}{w_i}) - \sum_{j=1}^{k} \log \sigma(-\mathbf{v}_{\tilde{w}j}^T \mathbf{v}{w_i})
$$

